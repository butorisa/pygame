{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\user\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "WARNING:tensorflow:From C:\\Users\\user\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 151, 120, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 51, 40, 8)         4064      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 17, 14, 8)         10824     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 5, 8)           10824     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 240)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               30848     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 90,100\n",
      "Trainable params: 90,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Weights are loaded.\n",
      "Training for 20000 steps ...\n",
      "WARNING:tensorflow:From C:\\Users\\user\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "   111/20000: episode: 1, duration: 20.957s, episode steps: 111, steps per second: 5, episode reward: 6.450, mean reward: 0.058 [0.000, 6.450], mean action: 1.730 [0.000, 3.000], mean observation: 166.087 [0.000, 255.000], loss: 0.056507, mae: 3.116144, mean_q: 4.195264\n",
      "   150/20000: episode: 2, duration: 21.376s, episode steps: 39, steps per second: 2, episode reward: 0.300, mean reward: 0.008 [0.000, 0.300], mean action: 2.846 [0.000, 3.000], mean observation: 166.052 [0.000, 255.000], loss: 0.050972, mae: 2.907697, mean_q: 4.064200\n",
      "   189/20000: episode: 3, duration: 21.168s, episode steps: 39, steps per second: 2, episode reward: 0.100, mean reward: 0.003 [0.000, 0.100], mean action: 2.718 [0.000, 3.000], mean observation: 168.485 [0.000, 255.000], loss: 0.047924, mae: 2.696382, mean_q: 3.812121\n",
      "   227/20000: episode: 4, duration: 21.004s, episode steps: 38, steps per second: 2, episode reward: 2.350, mean reward: 0.062 [0.000, 2.350], mean action: 2.684 [0.000, 3.000], mean observation: 163.787 [0.000, 255.000], loss: 0.061305, mae: 2.537526, mean_q: 3.690296\n",
      "   265/20000: episode: 5, duration: 20.928s, episode steps: 38, steps per second: 2, episode reward: 1.150, mean reward: 0.030 [0.000, 1.150], mean action: 2.026 [0.000, 3.000], mean observation: 163.561 [0.000, 255.000], loss: 0.049340, mae: 2.514939, mean_q: 3.641411\n",
      "   303/20000: episode: 6, duration: 21.032s, episode steps: 38, steps per second: 2, episode reward: 1.150, mean reward: 0.030 [0.000, 1.150], mean action: 2.842 [1.000, 3.000], mean observation: 164.590 [0.000, 255.000], loss: 0.035176, mae: 2.572246, mean_q: 3.634607\n",
      "   340/20000: episode: 7, duration: 21.155s, episode steps: 37, steps per second: 2, episode reward: 1.250, mean reward: 0.034 [0.000, 1.250], mean action: 2.135 [0.000, 3.000], mean observation: 163.736 [0.000, 255.000], loss: 0.046113, mae: 2.463437, mean_q: 3.449338\n",
      "   375/20000: episode: 8, duration: 21.259s, episode steps: 35, steps per second: 2, episode reward: 1.150, mean reward: 0.033 [0.000, 1.150], mean action: 2.229 [1.000, 3.000], mean observation: 163.820 [0.000, 255.000], loss: 0.049536, mae: 2.474918, mean_q: 3.488916\n",
      "   412/20000: episode: 9, duration: 21.260s, episode steps: 37, steps per second: 2, episode reward: 1.250, mean reward: 0.034 [0.000, 1.250], mean action: 2.135 [0.000, 3.000], mean observation: 161.931 [0.000, 255.000], loss: 0.033692, mae: 2.481978, mean_q: 3.445301\n",
      "   447/20000: episode: 10, duration: 21.722s, episode steps: 35, steps per second: 2, episode reward: 2.550, mean reward: 0.073 [0.000, 2.550], mean action: 1.429 [0.000, 3.000], mean observation: 162.232 [0.000, 255.000], loss: 0.036965, mae: 2.448609, mean_q: 3.386335\n",
      "   481/20000: episode: 11, duration: 21.906s, episode steps: 34, steps per second: 2, episode reward: 2.750, mean reward: 0.081 [0.000, 2.750], mean action: 2.206 [0.000, 3.000], mean observation: 165.453 [0.000, 255.000], loss: 0.028617, mae: 2.452741, mean_q: 3.400186\n",
      "   516/20000: episode: 12, duration: 21.286s, episode steps: 35, steps per second: 2, episode reward: 2.750, mean reward: 0.079 [0.000, 2.750], mean action: 1.400 [0.000, 3.000], mean observation: 164.354 [0.000, 255.000], loss: 0.042373, mae: 2.466220, mean_q: 3.410069\n",
      "   550/20000: episode: 13, duration: 21.690s, episode steps: 34, steps per second: 2, episode reward: 3.850, mean reward: 0.113 [0.000, 3.850], mean action: 1.765 [0.000, 3.000], mean observation: 163.025 [0.000, 255.000], loss: 0.042058, mae: 2.486493, mean_q: 3.434389\n",
      "   590/20000: episode: 14, duration: 21.265s, episode steps: 40, steps per second: 2, episode reward: 4.300, mean reward: 0.107 [0.000, 4.300], mean action: 1.425 [0.000, 3.000], mean observation: 163.057 [0.000, 255.000], loss: 0.047196, mae: 2.570636, mean_q: 3.543396\n",
      "   632/20000: episode: 15, duration: 21.117s, episode steps: 42, steps per second: 2, episode reward: 1.750, mean reward: 0.042 [0.000, 1.750], mean action: 0.333 [0.000, 3.000], mean observation: 163.192 [0.000, 255.000], loss: 0.049641, mae: 2.607708, mean_q: 3.603781\n",
      "   677/20000: episode: 16, duration: 21.148s, episode steps: 45, steps per second: 2, episode reward: -10.000, mean reward: -0.222 [-10.000, 0.000], mean action: 0.222 [0.000, 3.000], mean observation: 169.875 [0.000, 255.000], loss: 0.039175, mae: 2.613486, mean_q: 3.594874\n",
      "   723/20000: episode: 17, duration: 20.879s, episode steps: 46, steps per second: 2, episode reward: 3.000, mean reward: 0.065 [0.000, 3.000], mean action: 1.500 [0.000, 3.000], mean observation: 161.269 [0.000, 255.000], loss: 0.156867, mae: 2.599510, mean_q: 3.561573\n",
      "   769/20000: episode: 18, duration: 21.088s, episode steps: 46, steps per second: 2, episode reward: 2.700, mean reward: 0.059 [0.000, 2.700], mean action: 1.587 [0.000, 2.000], mean observation: 158.765 [0.000, 255.000], loss: 0.155616, mae: 2.599673, mean_q: 3.571340\n",
      "   815/20000: episode: 19, duration: 21.097s, episode steps: 46, steps per second: 2, episode reward: 2.800, mean reward: 0.061 [0.000, 2.800], mean action: 1.174 [0.000, 2.000], mean observation: 163.852 [0.000, 255.000], loss: 0.097928, mae: 2.621044, mean_q: 3.617662\n",
      "   861/20000: episode: 20, duration: 21.233s, episode steps: 46, steps per second: 2, episode reward: 4.950, mean reward: 0.108 [0.000, 4.950], mean action: 1.283 [0.000, 3.000], mean observation: 160.351 [0.000, 255.000], loss: 0.101052, mae: 2.616969, mean_q: 3.609117\n",
      "   907/20000: episode: 21, duration: 21.234s, episode steps: 46, steps per second: 2, episode reward: 3.950, mean reward: 0.086 [0.000, 3.950], mean action: 1.478 [0.000, 3.000], mean observation: 161.164 [0.000, 255.000], loss: 0.090195, mae: 2.718797, mean_q: 3.733625\n",
      "   952/20000: episode: 22, duration: 21.015s, episode steps: 45, steps per second: 2, episode reward: 2.600, mean reward: 0.058 [0.000, 2.600], mean action: 1.867 [0.000, 2.000], mean observation: 159.858 [0.000, 255.000], loss: 0.165750, mae: 2.730377, mean_q: 3.753044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   997/20000: episode: 23, duration: 20.838s, episode steps: 45, steps per second: 2, episode reward: 2.500, mean reward: 0.056 [0.000, 2.500], mean action: 1.889 [0.000, 2.000], mean observation: 158.758 [0.000, 255.000], loss: 0.046139, mae: 2.780423, mean_q: 3.825707\n",
      "  1041/20000: episode: 24, duration: 21.114s, episode steps: 44, steps per second: 2, episode reward: 4.100, mean reward: 0.093 [0.000, 4.100], mean action: 1.409 [0.000, 3.000], mean observation: 161.493 [0.000, 255.000], loss: 0.039330, mae: 2.739099, mean_q: 3.731833\n",
      "  1084/20000: episode: 25, duration: 21.122s, episode steps: 43, steps per second: 2, episode reward: 3.000, mean reward: 0.070 [0.000, 3.000], mean action: 0.791 [0.000, 2.000], mean observation: 160.849 [0.000, 255.000], loss: 0.042308, mae: 2.797349, mean_q: 3.809203\n",
      "  1126/20000: episode: 26, duration: 20.965s, episode steps: 42, steps per second: 2, episode reward: 2.700, mean reward: 0.064 [0.000, 2.700], mean action: 1.000 [0.000, 2.000], mean observation: 159.013 [0.000, 255.000], loss: 0.153485, mae: 2.750348, mean_q: 3.750874\n",
      "  1160/20000: episode: 27, duration: 21.110s, episode steps: 34, steps per second: 2, episode reward: 3.750, mean reward: 0.110 [0.000, 3.750], mean action: 1.676 [0.000, 3.000], mean observation: 160.806 [0.000, 255.000], loss: 0.104700, mae: 2.775504, mean_q: 3.774203\n",
      "  1201/20000: episode: 28, duration: 21.373s, episode steps: 41, steps per second: 2, episode reward: -10.000, mean reward: -0.244 [-10.000, 0.000], mean action: 1.098 [0.000, 2.000], mean observation: 166.934 [0.000, 255.000], loss: 0.037802, mae: 2.768540, mean_q: 3.784122\n",
      "  1240/20000: episode: 29, duration: 21.091s, episode steps: 39, steps per second: 2, episode reward: 3.900, mean reward: 0.100 [0.000, 3.900], mean action: 1.256 [0.000, 3.000], mean observation: 163.285 [0.000, 255.000], loss: 0.106310, mae: 2.734214, mean_q: 3.722775\n",
      "  1276/20000: episode: 30, duration: 21.189s, episode steps: 36, steps per second: 2, episode reward: 2.800, mean reward: 0.078 [0.000, 2.800], mean action: 1.528 [0.000, 3.000], mean observation: 161.891 [0.000, 255.000], loss: 0.180247, mae: 2.723772, mean_q: 3.712957\n",
      "  1316/20000: episode: 31, duration: 21.333s, episode steps: 40, steps per second: 2, episode reward: 4.900, mean reward: 0.123 [0.000, 4.900], mean action: 1.400 [0.000, 3.000], mean observation: 166.792 [0.000, 255.000], loss: 0.149759, mae: 2.733428, mean_q: 3.748800\n",
      "  1357/20000: episode: 32, duration: 21.139s, episode steps: 41, steps per second: 2, episode reward: 5.250, mean reward: 0.128 [0.000, 5.250], mean action: 1.634 [0.000, 3.000], mean observation: 163.137 [0.000, 255.000], loss: 0.148590, mae: 2.757089, mean_q: 3.782098\n",
      "  1397/20000: episode: 33, duration: 20.934s, episode steps: 40, steps per second: 2, episode reward: 3.750, mean reward: 0.094 [0.000, 3.750], mean action: 0.775 [0.000, 3.000], mean observation: 160.894 [0.000, 255.000], loss: 0.031951, mae: 2.781123, mean_q: 3.811974\n",
      "  1438/20000: episode: 34, duration: 21.271s, episode steps: 41, steps per second: 2, episode reward: 4.950, mean reward: 0.121 [0.000, 4.950], mean action: 1.098 [0.000, 3.000], mean observation: 163.959 [0.000, 255.000], loss: 0.210163, mae: 2.765184, mean_q: 3.797732\n",
      "  1478/20000: episode: 35, duration: 20.948s, episode steps: 40, steps per second: 2, episode reward: 2.700, mean reward: 0.068 [0.000, 2.700], mean action: 1.100 [0.000, 3.000], mean observation: 160.037 [0.000, 255.000], loss: 0.155353, mae: 2.746167, mean_q: 3.747587\n",
      "  1518/20000: episode: 36, duration: 21.038s, episode steps: 40, steps per second: 2, episode reward: 1.950, mean reward: 0.049 [0.000, 1.950], mean action: 0.650 [0.000, 3.000], mean observation: 162.890 [0.000, 255.000], loss: 0.280711, mae: 2.757047, mean_q: 3.735337\n",
      "  1558/20000: episode: 37, duration: 21.069s, episode steps: 40, steps per second: 2, episode reward: 3.250, mean reward: 0.081 [0.000, 3.250], mean action: 1.525 [0.000, 3.000], mean observation: 161.723 [0.000, 255.000], loss: 0.164036, mae: 2.724539, mean_q: 3.692660\n",
      "  1599/20000: episode: 38, duration: 21.411s, episode steps: 41, steps per second: 2, episode reward: 2.250, mean reward: 0.055 [0.000, 2.250], mean action: 0.780 [0.000, 3.000], mean observation: 162.581 [0.000, 255.000], loss: 0.156941, mae: 2.711036, mean_q: 3.692497\n",
      "  1639/20000: episode: 39, duration: 21.032s, episode steps: 40, steps per second: 2, episode reward: 1.500, mean reward: 0.037 [0.000, 1.500], mean action: 1.000 [0.000, 3.000], mean observation: 168.071 [0.000, 255.000], loss: 0.158352, mae: 2.695231, mean_q: 3.666537\n",
      "  1680/20000: episode: 40, duration: 21.418s, episode steps: 41, steps per second: 2, episode reward: 2.500, mean reward: 0.061 [0.000, 2.500], mean action: 1.341 [0.000, 3.000], mean observation: 163.156 [0.000, 255.000], loss: 0.098740, mae: 2.677145, mean_q: 3.622331\n",
      "  1721/20000: episode: 41, duration: 21.073s, episode steps: 41, steps per second: 2, episode reward: 3.150, mean reward: 0.077 [0.000, 3.150], mean action: 0.415 [0.000, 3.000], mean observation: 162.658 [0.000, 255.000], loss: 0.212971, mae: 2.684893, mean_q: 3.628516\n",
      "  1762/20000: episode: 42, duration: 21.234s, episode steps: 41, steps per second: 2, episode reward: 4.300, mean reward: 0.105 [0.000, 4.300], mean action: 1.000 [0.000, 3.000], mean observation: 163.448 [0.000, 255.000], loss: 0.032696, mae: 2.660356, mean_q: 3.630771\n",
      "  1803/20000: episode: 43, duration: 21.159s, episode steps: 41, steps per second: 2, episode reward: 3.850, mean reward: 0.094 [0.000, 3.850], mean action: 1.927 [0.000, 3.000], mean observation: 162.205 [0.000, 255.000], loss: 0.026433, mae: 2.683733, mean_q: 3.642060\n",
      "  1844/20000: episode: 44, duration: 21.248s, episode steps: 41, steps per second: 2, episode reward: 2.650, mean reward: 0.065 [0.000, 2.650], mean action: 0.537 [0.000, 3.000], mean observation: 161.653 [0.000, 255.000], loss: 0.100454, mae: 2.700632, mean_q: 3.671998\n",
      "  1885/20000: episode: 45, duration: 21.419s, episode steps: 41, steps per second: 2, episode reward: 3.350, mean reward: 0.082 [0.000, 3.350], mean action: 1.854 [0.000, 3.000], mean observation: 161.468 [0.000, 255.000], loss: 0.213853, mae: 2.682730, mean_q: 3.633126\n",
      "  1926/20000: episode: 46, duration: 21.365s, episode steps: 41, steps per second: 2, episode reward: 2.750, mean reward: 0.067 [0.000, 2.750], mean action: 1.585 [0.000, 3.000], mean observation: 161.252 [0.000, 255.000], loss: 0.033143, mae: 2.677752, mean_q: 3.625244\n",
      "  1967/20000: episode: 47, duration: 21.437s, episode steps: 41, steps per second: 2, episode reward: 1.650, mean reward: 0.040 [0.000, 1.650], mean action: 0.829 [0.000, 3.000], mean observation: 163.631 [0.000, 255.000], loss: 0.026937, mae: 2.637097, mean_q: 3.567686\n",
      "  2008/20000: episode: 48, duration: 21.420s, episode steps: 41, steps per second: 2, episode reward: 1.250, mean reward: 0.030 [0.000, 1.250], mean action: 0.780 [0.000, 3.000], mean observation: 163.950 [0.000, 255.000], loss: 0.084163, mae: 2.612519, mean_q: 3.555229\n",
      "  2049/20000: episode: 49, duration: 21.356s, episode steps: 41, steps per second: 2, episode reward: 1.350, mean reward: 0.033 [0.000, 1.350], mean action: 0.756 [0.000, 3.000], mean observation: 163.159 [0.000, 255.000], loss: 0.089970, mae: 2.634579, mean_q: 3.561233\n",
      "  2089/20000: episode: 50, duration: 21.104s, episode steps: 40, steps per second: 2, episode reward: 1.150, mean reward: 0.029 [0.000, 1.150], mean action: 1.600 [0.000, 3.000], mean observation: 164.187 [0.000, 255.000], loss: 0.022413, mae: 2.639252, mean_q: 3.574030\n",
      "  2130/20000: episode: 51, duration: 21.384s, episode steps: 41, steps per second: 2, episode reward: 1.350, mean reward: 0.033 [0.000, 1.350], mean action: 0.244 [0.000, 1.000], mean observation: 163.857 [0.000, 255.000], loss: 0.086618, mae: 2.636893, mean_q: 3.574368\n",
      "  2171/20000: episode: 52, duration: 21.348s, episode steps: 41, steps per second: 2, episode reward: 1.300, mean reward: 0.032 [0.000, 1.300], mean action: 2.585 [0.000, 3.000], mean observation: 167.046 [0.000, 255.000], loss: 0.144368, mae: 2.597115, mean_q: 3.492716\n",
      "  2210/20000: episode: 53, duration: 21.411s, episode steps: 39, steps per second: 2, episode reward: 1.500, mean reward: 0.038 [0.000, 1.500], mean action: 2.462 [0.000, 3.000], mean observation: 168.492 [0.000, 255.000], loss: 0.034360, mae: 2.580562, mean_q: 3.475683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2249/20000: episode: 54, duration: 20.995s, episode steps: 39, steps per second: 2, episode reward: 1.250, mean reward: 0.032 [0.000, 1.250], mean action: 0.333 [0.000, 3.000], mean observation: 163.271 [0.000, 255.000], loss: 0.024901, mae: 2.574172, mean_q: 3.480442\n",
      "  2289/20000: episode: 55, duration: 21.126s, episode steps: 40, steps per second: 2, episode reward: 2.550, mean reward: 0.064 [0.000, 2.550], mean action: 1.550 [0.000, 3.000], mean observation: 163.510 [0.000, 255.000], loss: 0.022722, mae: 2.579547, mean_q: 3.485327\n",
      "  2329/20000: episode: 56, duration: 21.351s, episode steps: 40, steps per second: 2, episode reward: 1.450, mean reward: 0.036 [0.000, 1.450], mean action: 0.725 [0.000, 3.000], mean observation: 163.582 [0.000, 255.000], loss: 0.087097, mae: 2.573868, mean_q: 3.466062\n",
      "  2368/20000: episode: 57, duration: 20.986s, episode steps: 39, steps per second: 2, episode reward: 1.250, mean reward: 0.032 [0.000, 1.250], mean action: 0.487 [0.000, 3.000], mean observation: 164.129 [0.000, 255.000], loss: 0.022064, mae: 2.522486, mean_q: 3.409903\n",
      "  2407/20000: episode: 58, duration: 21.134s, episode steps: 39, steps per second: 2, episode reward: 2.650, mean reward: 0.068 [0.000, 2.650], mean action: 0.308 [0.000, 3.000], mean observation: 164.492 [0.000, 255.000], loss: 0.092690, mae: 2.518653, mean_q: 3.395900\n",
      "  2447/20000: episode: 59, duration: 21.198s, episode steps: 40, steps per second: 2, episode reward: 1.250, mean reward: 0.031 [0.000, 1.250], mean action: 1.175 [0.000, 3.000], mean observation: 164.250 [0.000, 255.000], loss: 0.086979, mae: 2.524373, mean_q: 3.412780\n",
      "  2487/20000: episode: 60, duration: 21.291s, episode steps: 40, steps per second: 2, episode reward: 2.750, mean reward: 0.069 [0.000, 2.750], mean action: 1.650 [0.000, 3.000], mean observation: 160.419 [0.000, 255.000], loss: 0.021935, mae: 2.465147, mean_q: 3.327896\n",
      "  2527/20000: episode: 61, duration: 21.172s, episode steps: 40, steps per second: 2, episode reward: -10.000, mean reward: -0.250 [-10.000, 0.000], mean action: 2.675 [0.000, 3.000], mean observation: 171.555 [0.000, 255.000], loss: 0.085174, mae: 2.498559, mean_q: 3.367732\n",
      "  2567/20000: episode: 62, duration: 21.080s, episode steps: 40, steps per second: 2, episode reward: 2.650, mean reward: 0.066 [0.000, 2.650], mean action: 2.400 [0.000, 3.000], mean observation: 164.562 [0.000, 255.000], loss: 0.027533, mae: 2.457711, mean_q: 3.321035\n",
      "  2607/20000: episode: 63, duration: 21.307s, episode steps: 40, steps per second: 2, episode reward: 2.350, mean reward: 0.059 [0.000, 2.350], mean action: 0.575 [0.000, 3.000], mean observation: 161.403 [0.000, 255.000], loss: 0.028497, mae: 2.471215, mean_q: 3.329052\n",
      "  2647/20000: episode: 64, duration: 21.216s, episode steps: 40, steps per second: 2, episode reward: 2.450, mean reward: 0.061 [0.000, 2.450], mean action: 1.850 [0.000, 3.000], mean observation: 161.452 [0.000, 255.000], loss: 0.021437, mae: 2.459193, mean_q: 3.309914\n",
      "  2687/20000: episode: 65, duration: 21.468s, episode steps: 40, steps per second: 2, episode reward: 2.650, mean reward: 0.066 [0.000, 2.650], mean action: 0.700 [0.000, 3.000], mean observation: 163.861 [0.000, 255.000], loss: 0.085442, mae: 2.424152, mean_q: 3.260762\n",
      "  2727/20000: episode: 66, duration: 21.224s, episode steps: 40, steps per second: 2, episode reward: 2.650, mean reward: 0.066 [0.000, 2.650], mean action: 0.750 [0.000, 3.000], mean observation: 161.730 [0.000, 255.000], loss: 0.148470, mae: 2.397185, mean_q: 3.237174\n",
      "  2767/20000: episode: 67, duration: 21.243s, episode steps: 40, steps per second: 2, episode reward: 1.250, mean reward: 0.031 [0.000, 1.250], mean action: 0.450 [0.000, 3.000], mean observation: 163.719 [0.000, 255.000], loss: 0.147505, mae: 2.404328, mean_q: 3.243758\n",
      "  2806/20000: episode: 68, duration: 21.527s, episode steps: 39, steps per second: 2, episode reward: 1.350, mean reward: 0.035 [0.000, 1.350], mean action: 1.077 [0.000, 3.000], mean observation: 164.545 [0.000, 255.000], loss: 0.270755, mae: 2.420750, mean_q: 3.272722\n",
      "  2843/20000: episode: 69, duration: 21.233s, episode steps: 37, steps per second: 2, episode reward: -10.000, mean reward: -0.270 [-10.000, 0.000], mean action: 2.676 [0.000, 3.000], mean observation: 170.320 [0.000, 255.000], loss: 0.021683, mae: 2.365814, mean_q: 3.207625\n",
      "  2881/20000: episode: 70, duration: 21.702s, episode steps: 38, steps per second: 2, episode reward: -10.000, mean reward: -0.263 [-10.000, 0.000], mean action: 2.737 [0.000, 3.000], mean observation: 171.614 [0.000, 255.000], loss: 0.083732, mae: 2.330671, mean_q: 3.142940\n",
      "  2914/20000: episode: 71, duration: 21.709s, episode steps: 33, steps per second: 2, episode reward: 1.250, mean reward: 0.038 [0.000, 1.250], mean action: 1.242 [0.000, 3.000], mean observation: 163.636 [0.000, 255.000], loss: 0.171316, mae: 2.342487, mean_q: 3.158483\n",
      "  2944/20000: episode: 72, duration: 21.898s, episode steps: 30, steps per second: 1, episode reward: 1.150, mean reward: 0.038 [0.000, 1.150], mean action: 1.933 [1.000, 3.000], mean observation: 159.785 [0.000, 255.000], loss: 0.182829, mae: 2.321182, mean_q: 3.125869\n",
      "  2973/20000: episode: 73, duration: 21.646s, episode steps: 29, steps per second: 1, episode reward: 1.200, mean reward: 0.041 [0.000, 1.200], mean action: 2.759 [0.000, 3.000], mean observation: 168.727 [0.000, 255.000], loss: 0.098773, mae: 2.297743, mean_q: 3.093392\n",
      "  2998/20000: episode: 74, duration: 21.885s, episode steps: 25, steps per second: 1, episode reward: 2.500, mean reward: 0.100 [0.000, 2.500], mean action: 1.320 [0.000, 3.000], mean observation: 162.578 [0.000, 255.000], loss: 0.219102, mae: 2.289642, mean_q: 3.082548\n",
      "  3024/20000: episode: 75, duration: 22.155s, episode steps: 26, steps per second: 1, episode reward: -10.000, mean reward: -0.385 [-10.000, 0.000], mean action: 1.692 [0.000, 3.000], mean observation: 168.648 [0.000, 255.000], loss: 0.387639, mae: 2.271256, mean_q: 3.065809\n",
      "  3049/20000: episode: 76, duration: 21.850s, episode steps: 25, steps per second: 1, episode reward: -10.000, mean reward: -0.400 [-10.000, 0.000], mean action: 1.960 [0.000, 3.000], mean observation: 168.851 [0.000, 255.000], loss: 0.116701, mae: 2.263193, mean_q: 3.053340\n",
      "  3073/20000: episode: 77, duration: 22.168s, episode steps: 24, steps per second: 1, episode reward: -10.000, mean reward: -0.417 [-10.000, 0.000], mean action: 2.000 [1.000, 3.000], mean observation: 169.204 [0.000, 255.000], loss: 0.025428, mae: 2.283258, mean_q: 3.062878\n",
      "  3095/20000: episode: 78, duration: 23.009s, episode steps: 22, steps per second: 1, episode reward: 0.100, mean reward: 0.005 [0.000, 0.100], mean action: 1.864 [0.000, 2.000], mean observation: 169.348 [0.000, 255.000], loss: 0.338634, mae: 2.266810, mean_q: 3.050189\n",
      "  3118/20000: episode: 79, duration: 21.641s, episode steps: 23, steps per second: 1, episode reward: 1.250, mean reward: 0.054 [0.000, 1.250], mean action: 1.739 [0.000, 3.000], mean observation: 163.313 [0.000, 255.000], loss: 0.018298, mae: 2.210973, mean_q: 2.990842\n",
      "  3140/20000: episode: 80, duration: 22.578s, episode steps: 22, steps per second: 1, episode reward: 1.200, mean reward: 0.055 [0.000, 1.200], mean action: 2.045 [0.000, 3.000], mean observation: 164.924 [0.000, 255.000], loss: 0.446187, mae: 2.229339, mean_q: 2.986320\n",
      "  3161/20000: episode: 81, duration: 21.965s, episode steps: 21, steps per second: 1, episode reward: 2.350, mean reward: 0.112 [0.000, 2.350], mean action: 0.333 [0.000, 3.000], mean observation: 164.451 [0.000, 255.000], loss: 0.020561, mae: 2.176184, mean_q: 2.962542\n",
      "  3186/20000: episode: 82, duration: 21.779s, episode steps: 25, steps per second: 1, episode reward: 0.200, mean reward: 0.008 [0.000, 0.200], mean action: 2.040 [0.000, 3.000], mean observation: 168.451 [0.000, 255.000], loss: 0.110419, mae: 2.235829, mean_q: 3.028670\n",
      "  3210/20000: episode: 83, duration: 22.476s, episode steps: 24, steps per second: 1, episode reward: 1.150, mean reward: 0.048 [0.000, 1.150], mean action: 1.167 [1.000, 2.000], mean observation: 162.768 [0.000, 255.000], loss: 0.308902, mae: 2.207783, mean_q: 2.993536\n",
      "  3232/20000: episode: 84, duration: 22.279s, episode steps: 22, steps per second: 1, episode reward: 1.150, mean reward: 0.052 [0.000, 1.150], mean action: 1.045 [1.000, 2.000], mean observation: 165.112 [0.000, 255.000], loss: 0.125532, mae: 2.144789, mean_q: 2.921981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3255/20000: episode: 85, duration: 21.628s, episode steps: 23, steps per second: 1, episode reward: 4.800, mean reward: 0.209 [0.000, 4.800], mean action: 1.304 [0.000, 2.000], mean observation: 164.255 [0.000, 255.000], loss: 0.119612, mae: 2.169650, mean_q: 2.928690\n",
      "  3277/20000: episode: 86, duration: 22.750s, episode steps: 22, steps per second: 1, episode reward: 0.200, mean reward: 0.009 [0.000, 0.200], mean action: 2.727 [0.000, 3.000], mean observation: 167.558 [0.000, 255.000], loss: 0.136926, mae: 2.211826, mean_q: 2.990405\n",
      "  3297/20000: episode: 87, duration: 22.779s, episode steps: 20, steps per second: 1, episode reward: 0.100, mean reward: 0.005 [0.000, 0.100], mean action: 2.750 [0.000, 3.000], mean observation: 167.269 [0.000, 255.000], loss: 0.255952, mae: 2.208281, mean_q: 2.978340\n",
      "  3320/20000: episode: 88, duration: 22.528s, episode steps: 23, steps per second: 1, episode reward: 2.450, mean reward: 0.107 [0.000, 2.450], mean action: 1.000 [0.000, 3.000], mean observation: 164.067 [0.000, 255.000], loss: 0.224393, mae: 2.190162, mean_q: 2.959477\n",
      "  3344/20000: episode: 89, duration: 22.456s, episode steps: 24, steps per second: 1, episode reward: 1.400, mean reward: 0.058 [0.000, 1.400], mean action: 2.167 [0.000, 3.000], mean observation: 166.652 [0.000, 255.000], loss: 0.117784, mae: 2.173525, mean_q: 2.958184\n",
      "  3365/20000: episode: 90, duration: 22.776s, episode steps: 21, steps per second: 1, episode reward: 1.200, mean reward: 0.057 [0.000, 1.200], mean action: 2.286 [0.000, 3.000], mean observation: 166.409 [0.000, 255.000], loss: 0.139978, mae: 2.168208, mean_q: 2.932307\n",
      "  3386/20000: episode: 91, duration: 22.412s, episode steps: 21, steps per second: 1, episode reward: 1.450, mean reward: 0.069 [0.000, 1.450], mean action: 0.286 [0.000, 2.000], mean observation: 165.057 [0.000, 255.000], loss: 0.139571, mae: 2.202317, mean_q: 2.974748\n",
      "  3408/20000: episode: 92, duration: 22.407s, episode steps: 22, steps per second: 1, episode reward: 1.250, mean reward: 0.057 [0.000, 1.250], mean action: 0.727 [0.000, 1.000], mean observation: 165.052 [0.000, 255.000], loss: 0.144586, mae: 2.190133, mean_q: 2.972603\n",
      "  3431/20000: episode: 93, duration: 22.190s, episode steps: 23, steps per second: 1, episode reward: 2.450, mean reward: 0.107 [0.000, 2.450], mean action: 0.652 [0.000, 2.000], mean observation: 161.958 [0.000, 255.000], loss: 0.131888, mae: 2.200464, mean_q: 2.978955\n",
      "  3451/20000: episode: 94, duration: 23.332s, episode steps: 20, steps per second: 1, episode reward: 0.200, mean reward: 0.010 [0.000, 0.200], mean action: 2.700 [0.000, 3.000], mean observation: 167.583 [0.000, 255.000], loss: 0.020394, mae: 2.141234, mean_q: 2.892533\n",
      "  3467/20000: episode: 95, duration: 23.334s, episode steps: 16, steps per second: 1, episode reward: 1.300, mean reward: 0.081 [0.000, 1.300], mean action: 2.000 [0.000, 3.000], mean observation: 168.314 [0.000, 255.000], loss: 0.622124, mae: 2.120675, mean_q: 2.810576\n",
      "  3482/20000: episode: 96, duration: 24.570s, episode steps: 15, steps per second: 1, episode reward: 0.100, mean reward: 0.007 [0.000, 0.100], mean action: 2.667 [0.000, 3.000], mean observation: 168.348 [0.000, 255.000], loss: 0.178986, mae: 2.108865, mean_q: 2.881280\n",
      "  3497/20000: episode: 97, duration: 23.742s, episode steps: 15, steps per second: 1, episode reward: 0.100, mean reward: 0.007 [0.000, 0.100], mean action: 2.667 [0.000, 3.000], mean observation: 168.330 [0.000, 255.000], loss: 0.185706, mae: 2.144462, mean_q: 2.953494\n",
      "  3514/20000: episode: 98, duration: 23.849s, episode steps: 17, steps per second: 1, episode reward: 0.100, mean reward: 0.006 [0.000, 0.100], mean action: 2.706 [0.000, 3.000], mean observation: 167.831 [0.000, 255.000], loss: 0.030356, mae: 2.124559, mean_q: 2.903902\n",
      "  3532/20000: episode: 99, duration: 23.152s, episode steps: 18, steps per second: 1, episode reward: 1.300, mean reward: 0.072 [0.000, 1.300], mean action: 1.611 [0.000, 3.000], mean observation: 166.724 [0.000, 255.000], loss: 0.457068, mae: 2.141949, mean_q: 2.859981\n",
      "  3551/20000: episode: 100, duration: 22.059s, episode steps: 19, steps per second: 1, episode reward: 1.250, mean reward: 0.066 [0.000, 1.250], mean action: 0.211 [0.000, 3.000], mean observation: 165.611 [0.000, 255.000], loss: 0.045399, mae: 2.156262, mean_q: 2.940012\n",
      "  3571/20000: episode: 101, duration: 23.110s, episode steps: 20, steps per second: 1, episode reward: 1.250, mean reward: 0.062 [0.000, 1.250], mean action: 0.350 [0.000, 3.000], mean observation: 165.462 [0.000, 255.000], loss: 0.257622, mae: 2.159457, mean_q: 2.946656\n",
      "  3595/20000: episode: 102, duration: 22.827s, episode steps: 24, steps per second: 1, episode reward: 1.450, mean reward: 0.060 [0.000, 1.450], mean action: 0.125 [0.000, 1.000], mean observation: 164.501 [0.000, 255.000], loss: 0.222389, mae: 2.224076, mean_q: 3.019932\n",
      "  3618/20000: episode: 103, duration: 22.402s, episode steps: 23, steps per second: 1, episode reward: 2.550, mean reward: 0.111 [0.000, 2.550], mean action: 0.783 [0.000, 3.000], mean observation: 164.120 [0.000, 255.000], loss: 0.029169, mae: 2.185668, mean_q: 2.972253\n",
      "  3640/20000: episode: 104, duration: 22.052s, episode steps: 22, steps per second: 1, episode reward: 1.250, mean reward: 0.057 [0.000, 1.250], mean action: 1.682 [0.000, 3.000], mean observation: 164.200 [0.000, 255.000], loss: 0.352085, mae: 2.176467, mean_q: 2.909866\n",
      "  3663/20000: episode: 105, duration: 22.695s, episode steps: 23, steps per second: 1, episode reward: 2.600, mean reward: 0.113 [0.000, 2.600], mean action: 1.087 [0.000, 2.000], mean observation: 161.110 [0.000, 255.000], loss: 0.131904, mae: 2.180482, mean_q: 2.933104\n",
      "  3686/20000: episode: 106, duration: 22.671s, episode steps: 23, steps per second: 1, episode reward: 1.150, mean reward: 0.050 [0.000, 1.150], mean action: 1.870 [1.000, 2.000], mean observation: 159.922 [0.000, 255.000], loss: 0.128889, mae: 2.108826, mean_q: 2.867446\n",
      "  3708/20000: episode: 107, duration: 22.195s, episode steps: 22, steps per second: 1, episode reward: 1.150, mean reward: 0.052 [0.000, 1.150], mean action: 1.909 [0.000, 3.000], mean observation: 161.843 [0.000, 255.000], loss: 0.151170, mae: 2.156478, mean_q: 2.905564\n",
      "  3730/20000: episode: 108, duration: 22.959s, episode steps: 22, steps per second: 1, episode reward: 1.450, mean reward: 0.066 [0.000, 1.450], mean action: 0.773 [0.000, 3.000], mean observation: 162.743 [0.000, 255.000], loss: 0.126041, mae: 2.124864, mean_q: 2.849019\n",
      "  3750/20000: episode: 109, duration: 22.356s, episode steps: 20, steps per second: 1, episode reward: 2.700, mean reward: 0.135 [0.000, 2.700], mean action: 1.150 [0.000, 3.000], mean observation: 164.097 [0.000, 255.000], loss: 0.031792, mae: 2.129753, mean_q: 2.859321\n",
      "  3770/20000: episode: 110, duration: 22.189s, episode steps: 20, steps per second: 1, episode reward: 1.150, mean reward: 0.058 [0.000, 1.150], mean action: 1.850 [0.000, 2.000], mean observation: 163.341 [0.000, 255.000], loss: 0.026783, mae: 2.102338, mean_q: 2.848432\n",
      "  3792/20000: episode: 111, duration: 21.865s, episode steps: 22, steps per second: 1, episode reward: -10.000, mean reward: -0.455 [-10.000, 0.000], mean action: 1.091 [0.000, 2.000], mean observation: 169.388 [0.000, 255.000], loss: 0.023754, mae: 2.133296, mean_q: 2.875971\n",
      "  3814/20000: episode: 112, duration: 21.879s, episode steps: 22, steps per second: 1, episode reward: 0.100, mean reward: 0.005 [0.000, 0.100], mean action: 1.045 [1.000, 2.000], mean observation: 166.995 [0.000, 255.000], loss: 0.544693, mae: 2.112631, mean_q: 2.821689\n",
      "  3837/20000: episode: 113, duration: 21.968s, episode steps: 23, steps per second: 1, episode reward: 1.250, mean reward: 0.054 [0.000, 1.250], mean action: 1.870 [0.000, 3.000], mean observation: 161.169 [0.000, 255.000], loss: 0.028313, mae: 2.115111, mean_q: 2.861371\n",
      "  3861/20000: episode: 114, duration: 22.534s, episode steps: 24, steps per second: 1, episode reward: 1.250, mean reward: 0.052 [0.000, 1.250], mean action: 0.875 [0.000, 3.000], mean observation: 161.504 [0.000, 255.000], loss: 0.409354, mae: 2.136214, mean_q: 2.850224\n",
      "  3881/20000: episode: 115, duration: 22.240s, episode steps: 20, steps per second: 1, episode reward: 1.350, mean reward: 0.068 [0.000, 1.350], mean action: 0.650 [0.000, 2.000], mean observation: 164.601 [0.000, 255.000], loss: 0.029368, mae: 2.115889, mean_q: 2.849103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3903/20000: episode: 116, duration: 22.815s, episode steps: 22, steps per second: 1, episode reward: 1.250, mean reward: 0.057 [0.000, 1.250], mean action: 1.773 [0.000, 2.000], mean observation: 164.777 [0.000, 255.000], loss: 0.137144, mae: 2.116305, mean_q: 2.841987\n",
      "  3925/20000: episode: 117, duration: 22.387s, episode steps: 22, steps per second: 1, episode reward: 1.150, mean reward: 0.052 [0.000, 1.150], mean action: 1.909 [0.000, 3.000], mean observation: 160.372 [0.000, 255.000], loss: 0.027837, mae: 2.101790, mean_q: 2.829130\n",
      "  3948/20000: episode: 118, duration: 22.251s, episode steps: 23, steps per second: 1, episode reward: 4.950, mean reward: 0.215 [0.000, 4.950], mean action: 1.130 [0.000, 2.000], mean observation: 163.992 [0.000, 255.000], loss: 0.229149, mae: 2.115800, mean_q: 2.837968\n",
      "  3971/20000: episode: 119, duration: 22.522s, episode steps: 23, steps per second: 1, episode reward: 1.150, mean reward: 0.050 [0.000, 1.150], mean action: 1.913 [0.000, 3.000], mean observation: 163.018 [0.000, 255.000], loss: 0.322445, mae: 2.118000, mean_q: 2.843172\n",
      "  3994/20000: episode: 120, duration: 21.835s, episode steps: 23, steps per second: 1, episode reward: 1.150, mean reward: 0.050 [0.000, 1.150], mean action: 1.913 [1.000, 2.000], mean observation: 159.922 [0.000, 255.000], loss: 0.025736, mae: 2.030823, mean_q: 2.748371\n",
      "  4017/20000: episode: 121, duration: 21.945s, episode steps: 23, steps per second: 1, episode reward: 1.150, mean reward: 0.050 [0.000, 1.150], mean action: 1.870 [1.000, 2.000], mean observation: 159.922 [0.000, 255.000], loss: 0.122322, mae: 2.101158, mean_q: 2.835538\n",
      "  4041/20000: episode: 122, duration: 22.099s, episode steps: 24, steps per second: 1, episode reward: 1.250, mean reward: 0.052 [0.000, 1.250], mean action: 1.000 [0.000, 3.000], mean observation: 161.486 [0.000, 255.000], loss: 0.305499, mae: 2.072049, mean_q: 2.782722\n",
      "  4065/20000: episode: 123, duration: 22.551s, episode steps: 24, steps per second: 1, episode reward: 1.250, mean reward: 0.052 [0.000, 1.250], mean action: 1.333 [0.000, 3.000], mean observation: 163.840 [0.000, 255.000], loss: 0.219381, mae: 2.066782, mean_q: 2.786395\n",
      "  4089/20000: episode: 124, duration: 22.560s, episode steps: 24, steps per second: 1, episode reward: 0.100, mean reward: 0.004 [0.000, 0.100], mean action: 1.042 [1.000, 2.000], mean observation: 166.753 [0.000, 255.000], loss: 0.116740, mae: 2.043641, mean_q: 2.748361\n",
      "  4113/20000: episode: 125, duration: 22.452s, episode steps: 24, steps per second: 1, episode reward: 1.150, mean reward: 0.048 [0.000, 1.150], mean action: 2.042 [1.000, 3.000], mean observation: 160.325 [0.000, 255.000], loss: 0.027532, mae: 2.062869, mean_q: 2.763773\n",
      "  4137/20000: episode: 126, duration: 22.374s, episode steps: 24, steps per second: 1, episode reward: 1.250, mean reward: 0.052 [0.000, 1.250], mean action: 1.667 [0.000, 3.000], mean observation: 161.745 [0.000, 255.000], loss: 0.394731, mae: 2.058742, mean_q: 2.749287\n",
      "  4161/20000: episode: 127, duration: 22.177s, episode steps: 24, steps per second: 1, episode reward: 1.150, mean reward: 0.048 [0.000, 1.150], mean action: 2.042 [1.000, 3.000], mean observation: 162.225 [0.000, 255.000], loss: 0.117547, mae: 2.037344, mean_q: 2.762053\n",
      "  4184/20000: episode: 128, duration: 22.538s, episode steps: 23, steps per second: 1, episode reward: 1.150, mean reward: 0.050 [0.000, 1.150], mean action: 1.913 [0.000, 3.000], mean observation: 162.176 [0.000, 255.000], loss: 0.322185, mae: 2.035290, mean_q: 2.739029\n",
      "  4222/20000: episode: 129, duration: 21.103s, episode steps: 38, steps per second: 2, episode reward: 1.150, mean reward: 0.030 [0.000, 1.150], mean action: 1.921 [0.000, 3.000], mean observation: 162.164 [0.000, 255.000], loss: 0.144757, mae: 2.055365, mean_q: 2.772316\n",
      "  4262/20000: episode: 130, duration: 21.275s, episode steps: 40, steps per second: 2, episode reward: 2.550, mean reward: 0.064 [0.000, 2.550], mean action: 1.200 [0.000, 3.000], mean observation: 161.910 [0.000, 255.000], loss: 0.259797, mae: 2.003356, mean_q: 2.701594\n",
      "  4302/20000: episode: 131, duration: 21.426s, episode steps: 40, steps per second: 2, episode reward: 1.150, mean reward: 0.029 [0.000, 1.150], mean action: 1.825 [0.000, 3.000], mean observation: 160.433 [0.000, 255.000], loss: 0.023162, mae: 2.029405, mean_q: 2.755702\n",
      "  4342/20000: episode: 132, duration: 21.428s, episode steps: 40, steps per second: 2, episode reward: 1.150, mean reward: 0.029 [0.000, 1.150], mean action: 1.700 [1.000, 3.000], mean observation: 160.114 [0.000, 255.000], loss: 0.077982, mae: 2.015542, mean_q: 2.724720\n",
      "  4382/20000: episode: 133, duration: 21.384s, episode steps: 40, steps per second: 2, episode reward: 3.700, mean reward: 0.092 [0.000, 3.700], mean action: 0.675 [0.000, 2.000], mean observation: 163.474 [0.000, 255.000], loss: 0.136335, mae: 1.997974, mean_q: 2.694180\n",
      "  4421/20000: episode: 134, duration: 20.945s, episode steps: 39, steps per second: 2, episode reward: 0.200, mean reward: 0.005 [0.000, 0.200], mean action: 1.051 [0.000, 2.000], mean observation: 168.390 [0.000, 255.000], loss: 0.190094, mae: 1.987892, mean_q: 2.682852\n",
      "  4461/20000: episode: 135, duration: 21.434s, episode steps: 40, steps per second: 2, episode reward: 2.700, mean reward: 0.068 [0.000, 2.700], mean action: 1.300 [0.000, 3.000], mean observation: 160.265 [0.000, 255.000], loss: 0.131708, mae: 1.986392, mean_q: 2.681656\n",
      "  4500/20000: episode: 136, duration: 21.014s, episode steps: 39, steps per second: 2, episode reward: 2.350, mean reward: 0.060 [0.000, 2.350], mean action: 1.179 [0.000, 3.000], mean observation: 159.891 [0.000, 255.000], loss: 0.181899, mae: 1.989476, mean_q: 2.704330\n",
      "  4539/20000: episode: 137, duration: 21.318s, episode steps: 39, steps per second: 2, episode reward: 2.500, mean reward: 0.064 [0.000, 2.500], mean action: 1.513 [0.000, 2.000], mean observation: 159.944 [0.000, 255.000], loss: 0.133789, mae: 1.979373, mean_q: 2.681469\n",
      "  4578/20000: episode: 138, duration: 21.464s, episode steps: 39, steps per second: 2, episode reward: 2.050, mean reward: 0.053 [0.000, 2.050], mean action: 2.051 [0.000, 3.000], mean observation: 160.689 [0.000, 255.000], loss: 0.076487, mae: 1.978639, mean_q: 2.687210\n",
      "  4616/20000: episode: 139, duration: 21.049s, episode steps: 38, steps per second: 2, episode reward: 2.450, mean reward: 0.064 [0.000, 2.450], mean action: 0.342 [0.000, 2.000], mean observation: 162.510 [0.000, 255.000], loss: 0.257891, mae: 1.991986, mean_q: 2.663640\n",
      "  4654/20000: episode: 140, duration: 21.293s, episode steps: 38, steps per second: 2, episode reward: 2.450, mean reward: 0.064 [0.000, 2.450], mean action: 0.211 [0.000, 3.000], mean observation: 162.430 [0.000, 255.000], loss: 0.133105, mae: 1.940072, mean_q: 2.612318\n",
      "  4691/20000: episode: 141, duration: 21.085s, episode steps: 37, steps per second: 2, episode reward: 1.250, mean reward: 0.034 [0.000, 1.250], mean action: 2.027 [0.000, 3.000], mean observation: 159.521 [0.000, 255.000], loss: 0.261815, mae: 1.953491, mean_q: 2.617837\n",
      "  4728/20000: episode: 142, duration: 21.440s, episode steps: 37, steps per second: 2, episode reward: 2.850, mean reward: 0.077 [0.000, 2.850], mean action: 0.351 [0.000, 2.000], mean observation: 162.613 [0.000, 255.000], loss: 0.139918, mae: 1.968285, mean_q: 2.646973\n",
      "  4765/20000: episode: 143, duration: 21.310s, episode steps: 37, steps per second: 2, episode reward: 1.150, mean reward: 0.031 [0.000, 1.150], mean action: 2.027 [0.000, 3.000], mean observation: 160.047 [0.000, 255.000], loss: 0.255665, mae: 1.937403, mean_q: 2.599075\n",
      "  4802/20000: episode: 144, duration: 21.632s, episode steps: 37, steps per second: 2, episode reward: 1.450, mean reward: 0.039 [0.000, 1.450], mean action: 1.973 [0.000, 3.000], mean observation: 162.331 [0.000, 255.000], loss: 0.083013, mae: 1.933961, mean_q: 2.608035\n",
      "  4836/20000: episode: 145, duration: 21.551s, episode steps: 34, steps per second: 2, episode reward: 3.650, mean reward: 0.107 [0.000, 3.650], mean action: 2.206 [0.000, 3.000], mean observation: 161.717 [0.000, 255.000], loss: 0.202919, mae: 1.898431, mean_q: 2.569836\n",
      "  4871/20000: episode: 146, duration: 21.164s, episode steps: 35, steps per second: 2, episode reward: 1.250, mean reward: 0.036 [0.000, 1.250], mean action: 2.486 [0.000, 3.000], mean observation: 162.484 [0.000, 255.000], loss: 0.147231, mae: 1.926326, mean_q: 2.608910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4906/20000: episode: 147, duration: 21.444s, episode steps: 35, steps per second: 2, episode reward: 2.650, mean reward: 0.076 [0.000, 2.650], mean action: 1.457 [0.000, 3.000], mean observation: 161.008 [0.000, 255.000], loss: 0.090628, mae: 1.929389, mean_q: 2.598830\n",
      "  4939/20000: episode: 148, duration: 21.708s, episode steps: 33, steps per second: 2, episode reward: 2.650, mean reward: 0.080 [0.000, 2.650], mean action: 1.576 [0.000, 3.000], mean observation: 162.466 [0.000, 255.000], loss: 0.219096, mae: 1.930538, mean_q: 2.602988\n",
      "  4973/20000: episode: 149, duration: 21.374s, episode steps: 34, steps per second: 2, episode reward: 2.700, mean reward: 0.079 [0.000, 2.700], mean action: 1.088 [0.000, 3.000], mean observation: 160.661 [0.000, 255.000], loss: 0.152499, mae: 1.924819, mean_q: 2.594511\n",
      "  5007/20000: episode: 150, duration: 21.520s, episode steps: 34, steps per second: 2, episode reward: 2.450, mean reward: 0.072 [0.000, 2.450], mean action: 1.971 [0.000, 3.000], mean observation: 162.095 [0.000, 255.000], loss: 0.209662, mae: 1.920714, mean_q: 2.589214\n",
      "  5040/20000: episode: 151, duration: 21.529s, episode steps: 33, steps per second: 2, episode reward: 2.550, mean reward: 0.077 [0.000, 2.550], mean action: 1.515 [0.000, 3.000], mean observation: 159.921 [0.000, 255.000], loss: 0.150826, mae: 1.921803, mean_q: 2.584561\n",
      "  5074/20000: episode: 152, duration: 21.697s, episode steps: 34, steps per second: 2, episode reward: 2.700, mean reward: 0.079 [0.000, 2.700], mean action: 1.618 [0.000, 3.000], mean observation: 160.756 [0.000, 255.000], loss: 0.272859, mae: 1.908040, mean_q: 2.563490\n",
      "  5107/20000: episode: 153, duration: 21.797s, episode steps: 33, steps per second: 2, episode reward: 2.400, mean reward: 0.073 [0.000, 2.400], mean action: 0.788 [0.000, 3.000], mean observation: 163.471 [0.000, 255.000], loss: 0.144581, mae: 1.868080, mean_q: 2.509333\n",
      "  5140/20000: episode: 154, duration: 21.257s, episode steps: 33, steps per second: 2, episode reward: 2.350, mean reward: 0.071 [0.000, 2.350], mean action: 0.091 [0.000, 2.000], mean observation: 161.228 [0.000, 255.000], loss: 0.152421, mae: 1.903750, mean_q: 2.562102\n",
      "  5173/20000: episode: 155, duration: 21.303s, episode steps: 33, steps per second: 2, episode reward: 2.650, mean reward: 0.080 [0.000, 2.650], mean action: 2.333 [0.000, 3.000], mean observation: 161.060 [0.000, 255.000], loss: 0.022272, mae: 1.869347, mean_q: 2.517277\n",
      "  5206/20000: episode: 156, duration: 21.594s, episode steps: 33, steps per second: 2, episode reward: 2.500, mean reward: 0.076 [0.000, 2.500], mean action: 1.061 [0.000, 2.000], mean observation: 159.673 [0.000, 255.000], loss: 0.146111, mae: 1.871189, mean_q: 2.524851\n",
      "  5239/20000: episode: 157, duration: 21.474s, episode steps: 33, steps per second: 2, episode reward: 2.500, mean reward: 0.076 [0.000, 2.500], mean action: 2.061 [0.000, 3.000], mean observation: 162.556 [0.000, 255.000], loss: 0.081961, mae: 1.897313, mean_q: 2.560692\n",
      "  5272/20000: episode: 158, duration: 21.281s, episode steps: 33, steps per second: 2, episode reward: 2.700, mean reward: 0.082 [0.000, 2.700], mean action: 1.606 [0.000, 3.000], mean observation: 160.181 [0.000, 255.000], loss: 0.020982, mae: 1.897559, mean_q: 2.564281\n",
      "  5305/20000: episode: 159, duration: 21.383s, episode steps: 33, steps per second: 2, episode reward: 4.950, mean reward: 0.150 [0.000, 4.950], mean action: 1.333 [0.000, 3.000], mean observation: 162.378 [0.000, 255.000], loss: 0.017439, mae: 1.867720, mean_q: 2.528909\n",
      "  5339/20000: episode: 160, duration: 21.784s, episode steps: 34, steps per second: 2, episode reward: 3.700, mean reward: 0.109 [0.000, 3.700], mean action: 0.765 [0.000, 2.000], mean observation: 164.106 [0.000, 255.000], loss: 0.143954, mae: 1.899138, mean_q: 2.560813\n",
      "  5373/20000: episode: 161, duration: 21.708s, episode steps: 34, steps per second: 2, episode reward: 1.450, mean reward: 0.043 [0.000, 1.450], mean action: 1.324 [0.000, 3.000], mean observation: 161.256 [0.000, 255.000], loss: 0.211325, mae: 1.869119, mean_q: 2.523481\n",
      "  5406/20000: episode: 162, duration: 21.544s, episode steps: 33, steps per second: 2, episode reward: 1.250, mean reward: 0.038 [0.000, 1.250], mean action: 2.515 [0.000, 3.000], mean observation: 164.374 [0.000, 255.000], loss: 0.084378, mae: 1.874105, mean_q: 2.535904\n",
      "  5439/20000: episode: 163, duration: 21.299s, episode steps: 33, steps per second: 2, episode reward: 3.650, mean reward: 0.111 [0.000, 3.650], mean action: 0.727 [0.000, 3.000], mean observation: 162.875 [0.000, 255.000], loss: 0.085309, mae: 1.857573, mean_q: 2.521213\n",
      "  5471/20000: episode: 164, duration: 21.202s, episode steps: 32, steps per second: 2, episode reward: 1.150, mean reward: 0.036 [0.000, 1.150], mean action: 0.906 [0.000, 3.000], mean observation: 160.091 [0.000, 255.000], loss: 0.215177, mae: 1.868849, mean_q: 2.534086\n",
      "  5504/20000: episode: 165, duration: 21.178s, episode steps: 33, steps per second: 2, episode reward: 2.550, mean reward: 0.077 [0.000, 2.550], mean action: 1.000 [0.000, 3.000], mean observation: 162.403 [0.000, 255.000], loss: 0.075949, mae: 1.857789, mean_q: 2.525504\n",
      "  5537/20000: episode: 166, duration: 21.431s, episode steps: 33, steps per second: 2, episode reward: 2.750, mean reward: 0.083 [0.000, 2.750], mean action: 1.364 [0.000, 3.000], mean observation: 163.039 [0.000, 255.000], loss: 0.148428, mae: 1.835645, mean_q: 2.480520\n",
      "  5570/20000: episode: 167, duration: 21.486s, episode steps: 33, steps per second: 2, episode reward: 2.750, mean reward: 0.083 [0.000, 2.750], mean action: 0.485 [0.000, 3.000], mean observation: 163.129 [0.000, 255.000], loss: 0.080904, mae: 1.795050, mean_q: 2.439059\n",
      "  5603/20000: episode: 168, duration: 21.309s, episode steps: 33, steps per second: 2, episode reward: 2.750, mean reward: 0.083 [0.000, 2.750], mean action: 0.727 [0.000, 3.000], mean observation: 162.423 [0.000, 255.000], loss: 0.150489, mae: 1.877399, mean_q: 2.541045\n",
      "  5636/20000: episode: 169, duration: 21.426s, episode steps: 33, steps per second: 2, episode reward: 2.550, mean reward: 0.077 [0.000, 2.550], mean action: 1.242 [0.000, 3.000], mean observation: 160.814 [0.000, 255.000], loss: 0.017787, mae: 1.869108, mean_q: 2.532826\n",
      "  5669/20000: episode: 170, duration: 21.290s, episode steps: 33, steps per second: 2, episode reward: 3.600, mean reward: 0.109 [0.000, 3.600], mean action: 0.848 [0.000, 3.000], mean observation: 159.590 [0.000, 255.000], loss: 0.265831, mae: 1.857607, mean_q: 2.510227\n",
      "  5703/20000: episode: 171, duration: 21.785s, episode steps: 34, steps per second: 2, episode reward: 2.550, mean reward: 0.075 [0.000, 2.550], mean action: 1.471 [0.000, 3.000], mean observation: 163.546 [0.000, 255.000], loss: 0.208754, mae: 1.861158, mean_q: 2.503032\n",
      "  5736/20000: episode: 172, duration: 21.251s, episode steps: 33, steps per second: 2, episode reward: 2.550, mean reward: 0.077 [0.000, 2.550], mean action: 1.242 [0.000, 3.000], mean observation: 161.732 [0.000, 255.000], loss: 0.141335, mae: 1.814299, mean_q: 2.454851\n",
      "  5769/20000: episode: 173, duration: 21.479s, episode steps: 33, steps per second: 2, episode reward: 1.550, mean reward: 0.047 [0.000, 1.550], mean action: 0.364 [0.000, 2.000], mean observation: 163.966 [0.000, 255.000], loss: 0.144994, mae: 1.805319, mean_q: 2.431763\n",
      "  5801/20000: episode: 174, duration: 21.530s, episode steps: 32, steps per second: 1, episode reward: 1.450, mean reward: 0.045 [0.000, 1.450], mean action: 0.844 [0.000, 3.000], mean observation: 163.705 [0.000, 255.000], loss: 0.205659, mae: 1.865752, mean_q: 2.524061\n",
      "  5834/20000: episode: 175, duration: 21.517s, episode steps: 33, steps per second: 2, episode reward: 2.750, mean reward: 0.083 [0.000, 2.750], mean action: 0.667 [0.000, 3.000], mean observation: 162.591 [0.000, 255.000], loss: 0.145032, mae: 1.833268, mean_q: 2.483530\n",
      "  5866/20000: episode: 176, duration: 21.408s, episode steps: 32, steps per second: 1, episode reward: 2.800, mean reward: 0.088 [0.000, 2.800], mean action: 1.344 [0.000, 3.000], mean observation: 160.377 [0.000, 255.000], loss: 0.077012, mae: 1.827563, mean_q: 2.463449\n",
      "  5899/20000: episode: 177, duration: 21.442s, episode steps: 33, steps per second: 2, episode reward: 1.450, mean reward: 0.044 [0.000, 1.450], mean action: 1.667 [0.000, 3.000], mean observation: 162.396 [0.000, 255.000], loss: 0.019620, mae: 1.847206, mean_q: 2.492772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5933/20000: episode: 178, duration: 21.836s, episode steps: 34, steps per second: 2, episode reward: 1.250, mean reward: 0.037 [0.000, 1.250], mean action: 2.471 [0.000, 3.000], mean observation: 163.560 [0.000, 255.000], loss: 0.138983, mae: 1.833264, mean_q: 2.485834\n",
      "  5966/20000: episode: 179, duration: 21.393s, episode steps: 33, steps per second: 2, episode reward: 2.550, mean reward: 0.077 [0.000, 2.550], mean action: 1.242 [0.000, 3.000], mean observation: 163.891 [0.000, 255.000], loss: 0.203095, mae: 1.838422, mean_q: 2.494488\n",
      "  5999/20000: episode: 180, duration: 21.358s, episode steps: 33, steps per second: 2, episode reward: 2.550, mean reward: 0.077 [0.000, 2.550], mean action: 2.152 [0.000, 3.000], mean observation: 161.433 [0.000, 255.000], loss: 0.139666, mae: 1.844954, mean_q: 2.500036\n",
      "  6032/20000: episode: 181, duration: 21.492s, episode steps: 33, steps per second: 2, episode reward: 2.800, mean reward: 0.085 [0.000, 2.800], mean action: 1.182 [0.000, 3.000], mean observation: 160.152 [0.000, 255.000], loss: 0.143555, mae: 1.817312, mean_q: 2.444636\n",
      "  6065/20000: episode: 182, duration: 21.531s, episode steps: 33, steps per second: 2, episode reward: 3.900, mean reward: 0.118 [0.000, 3.900], mean action: 1.485 [0.000, 3.000], mean observation: 161.633 [0.000, 255.000], loss: 0.018969, mae: 1.783636, mean_q: 2.424161\n",
      "  6098/20000: episode: 183, duration: 21.400s, episode steps: 33, steps per second: 2, episode reward: 1.250, mean reward: 0.038 [0.000, 1.250], mean action: 0.212 [0.000, 3.000], mean observation: 164.433 [0.000, 255.000], loss: 0.077729, mae: 1.821867, mean_q: 2.466885\n",
      "  6131/20000: episode: 184, duration: 21.453s, episode steps: 33, steps per second: 2, episode reward: 2.750, mean reward: 0.083 [0.000, 2.750], mean action: 0.394 [0.000, 2.000], mean observation: 162.788 [0.000, 255.000], loss: 0.081280, mae: 1.794116, mean_q: 2.424669\n",
      "  6164/20000: episode: 185, duration: 21.232s, episode steps: 33, steps per second: 2, episode reward: 2.700, mean reward: 0.082 [0.000, 2.700], mean action: 1.333 [0.000, 3.000], mean observation: 163.524 [0.000, 255.000], loss: 0.139911, mae: 1.853641, mean_q: 2.492963\n",
      "  6197/20000: episode: 186, duration: 21.300s, episode steps: 33, steps per second: 2, episode reward: 1.650, mean reward: 0.050 [0.000, 1.650], mean action: 0.576 [0.000, 3.000], mean observation: 161.982 [0.000, 255.000], loss: 0.201728, mae: 1.803944, mean_q: 2.440003\n",
      "  6230/20000: episode: 187, duration: 21.384s, episode steps: 33, steps per second: 2, episode reward: 2.600, mean reward: 0.079 [0.000, 2.600], mean action: 1.364 [0.000, 2.000], mean observation: 159.397 [0.000, 255.000], loss: 0.080098, mae: 1.793107, mean_q: 2.439280\n",
      "  6262/20000: episode: 188, duration: 21.680s, episode steps: 32, steps per second: 1, episode reward: 3.700, mean reward: 0.116 [0.000, 3.700], mean action: 0.750 [0.000, 3.000], mean observation: 163.451 [0.000, 255.000], loss: 0.017282, mae: 1.792256, mean_q: 2.427744\n",
      "  6295/20000: episode: 189, duration: 21.339s, episode steps: 33, steps per second: 2, episode reward: 1.850, mean reward: 0.056 [0.000, 1.850], mean action: 2.515 [0.000, 3.000], mean observation: 160.056 [0.000, 255.000], loss: 0.078112, mae: 1.785077, mean_q: 2.401367\n",
      "  6328/20000: episode: 190, duration: 21.457s, episode steps: 33, steps per second: 2, episode reward: 2.700, mean reward: 0.082 [0.000, 2.700], mean action: 1.515 [0.000, 3.000], mean observation: 160.571 [0.000, 255.000], loss: 0.085026, mae: 1.785488, mean_q: 2.404624\n",
      "  6360/20000: episode: 191, duration: 21.396s, episode steps: 32, steps per second: 1, episode reward: 1.350, mean reward: 0.042 [0.000, 1.350], mean action: 0.781 [0.000, 3.000], mean observation: 164.342 [0.000, 255.000], loss: 0.080440, mae: 1.785513, mean_q: 2.399780\n",
      "  6392/20000: episode: 192, duration: 21.399s, episode steps: 32, steps per second: 1, episode reward: 2.350, mean reward: 0.073 [0.000, 2.350], mean action: 2.125 [0.000, 3.000], mean observation: 162.650 [0.000, 255.000], loss: 0.014739, mae: 1.798578, mean_q: 2.421836\n",
      "  6425/20000: episode: 193, duration: 21.280s, episode steps: 33, steps per second: 2, episode reward: 1.150, mean reward: 0.035 [0.000, 1.150], mean action: 1.879 [0.000, 3.000], mean observation: 161.272 [0.000, 255.000], loss: 0.393472, mae: 1.764492, mean_q: 2.377112\n",
      "  6458/20000: episode: 194, duration: 21.597s, episode steps: 33, steps per second: 2, episode reward: 1.350, mean reward: 0.041 [0.000, 1.350], mean action: 1.818 [0.000, 3.000], mean observation: 163.504 [0.000, 255.000], loss: 0.140587, mae: 1.777384, mean_q: 2.396242\n",
      "  6490/20000: episode: 195, duration: 21.570s, episode steps: 32, steps per second: 1, episode reward: 1.350, mean reward: 0.042 [0.000, 1.350], mean action: 0.438 [0.000, 3.000], mean observation: 160.318 [0.000, 255.000], loss: 0.023738, mae: 1.759364, mean_q: 2.402160\n",
      "  6523/20000: episode: 196, duration: 21.809s, episode steps: 33, steps per second: 2, episode reward: 2.900, mean reward: 0.088 [0.000, 2.900], mean action: 1.182 [0.000, 3.000], mean observation: 160.817 [0.000, 255.000], loss: 0.078087, mae: 1.785153, mean_q: 2.421498\n",
      "  6556/20000: episode: 197, duration: 21.377s, episode steps: 33, steps per second: 2, episode reward: 1.150, mean reward: 0.035 [0.000, 1.150], mean action: 1.697 [0.000, 3.000], mean observation: 162.868 [0.000, 255.000], loss: 0.136427, mae: 1.744635, mean_q: 2.363502\n",
      "  6588/20000: episode: 198, duration: 21.603s, episode steps: 32, steps per second: 1, episode reward: 3.800, mean reward: 0.119 [0.000, 3.800], mean action: 1.656 [0.000, 3.000], mean observation: 161.688 [0.000, 255.000], loss: 0.201924, mae: 1.775452, mean_q: 2.385849\n",
      "  6620/20000: episode: 199, duration: 21.498s, episode steps: 32, steps per second: 1, episode reward: 1.350, mean reward: 0.042 [0.000, 1.350], mean action: 1.094 [0.000, 3.000], mean observation: 162.751 [0.000, 255.000], loss: 0.202475, mae: 1.770784, mean_q: 2.405541\n",
      "  6652/20000: episode: 200, duration: 21.559s, episode steps: 32, steps per second: 1, episode reward: 2.350, mean reward: 0.073 [0.000, 2.350], mean action: 1.250 [0.000, 3.000], mean observation: 162.029 [0.000, 255.000], loss: 0.144992, mae: 1.751652, mean_q: 2.364316\n",
      "  6685/20000: episode: 201, duration: 21.568s, episode steps: 33, steps per second: 2, episode reward: 1.150, mean reward: 0.035 [0.000, 1.150], mean action: 2.606 [0.000, 3.000], mean observation: 164.031 [0.000, 255.000], loss: 0.079898, mae: 1.742124, mean_q: 2.351614\n",
      "  6718/20000: episode: 202, duration: 21.344s, episode steps: 33, steps per second: 2, episode reward: 2.350, mean reward: 0.071 [0.000, 2.350], mean action: 0.667 [0.000, 3.000], mean observation: 163.608 [0.000, 255.000], loss: 0.018468, mae: 1.753345, mean_q: 2.374957\n",
      "  6751/20000: episode: 203, duration: 21.899s, episode steps: 33, steps per second: 2, episode reward: 1.650, mean reward: 0.050 [0.000, 1.650], mean action: 0.788 [0.000, 3.000], mean observation: 163.108 [0.000, 255.000], loss: 0.071851, mae: 1.749891, mean_q: 2.369693\n",
      "  6783/20000: episode: 204, duration: 21.403s, episode steps: 32, steps per second: 1, episode reward: 2.700, mean reward: 0.084 [0.000, 2.700], mean action: 1.875 [0.000, 3.000], mean observation: 159.862 [0.000, 255.000], loss: 0.079680, mae: 1.736391, mean_q: 2.355766\n",
      "  6816/20000: episode: 205, duration: 21.522s, episode steps: 33, steps per second: 2, episode reward: 3.900, mean reward: 0.118 [0.000, 3.900], mean action: 0.303 [0.000, 2.000], mean observation: 163.877 [0.000, 255.000], loss: 0.134363, mae: 1.735140, mean_q: 2.354116\n",
      "  6849/20000: episode: 206, duration: 21.364s, episode steps: 33, steps per second: 2, episode reward: 2.750, mean reward: 0.083 [0.000, 2.750], mean action: 1.485 [0.000, 3.000], mean observation: 160.943 [0.000, 255.000], loss: 0.075660, mae: 1.743477, mean_q: 2.353669\n",
      "  6882/20000: episode: 207, duration: 21.294s, episode steps: 33, steps per second: 2, episode reward: 4.000, mean reward: 0.121 [0.000, 4.000], mean action: 2.303 [0.000, 3.000], mean observation: 160.231 [0.000, 255.000], loss: 0.074955, mae: 1.735175, mean_q: 2.352848\n",
      "  6915/20000: episode: 208, duration: 21.397s, episode steps: 33, steps per second: 2, episode reward: 2.550, mean reward: 0.077 [0.000, 2.550], mean action: 0.970 [0.000, 3.000], mean observation: 161.655 [0.000, 255.000], loss: 0.077618, mae: 1.743031, mean_q: 2.357888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6947/20000: episode: 209, duration: 21.547s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 0.688 [0.000, 3.000], mean observation: 164.483 [0.000, 255.000], loss: 0.262026, mae: 1.739233, mean_q: 2.331042\n",
      "  6980/20000: episode: 210, duration: 21.630s, episode steps: 33, steps per second: 2, episode reward: 4.050, mean reward: 0.123 [0.000, 4.050], mean action: 0.545 [0.000, 2.000], mean observation: 162.634 [0.000, 255.000], loss: 0.133502, mae: 1.758830, mean_q: 2.375882\n",
      "  7013/20000: episode: 211, duration: 21.555s, episode steps: 33, steps per second: 2, episode reward: 2.450, mean reward: 0.074 [0.000, 2.450], mean action: 0.697 [0.000, 3.000], mean observation: 163.889 [0.000, 255.000], loss: 0.016134, mae: 1.766261, mean_q: 2.401660\n",
      "  7046/20000: episode: 212, duration: 21.578s, episode steps: 33, steps per second: 2, episode reward: 1.250, mean reward: 0.038 [0.000, 1.250], mean action: 1.000 [0.000, 3.000], mean observation: 164.546 [0.000, 255.000], loss: 0.021260, mae: 1.739647, mean_q: 2.355812\n",
      "  7079/20000: episode: 213, duration: 21.539s, episode steps: 33, steps per second: 2, episode reward: 1.450, mean reward: 0.044 [0.000, 1.450], mean action: 0.091 [0.000, 1.000], mean observation: 164.050 [0.000, 255.000], loss: 0.249645, mae: 1.737760, mean_q: 2.348892\n",
      "  7112/20000: episode: 214, duration: 21.415s, episode steps: 33, steps per second: 2, episode reward: 2.800, mean reward: 0.085 [0.000, 2.800], mean action: 1.364 [0.000, 3.000], mean observation: 160.336 [0.000, 255.000], loss: 0.019804, mae: 1.729365, mean_q: 2.338577\n",
      "  7145/20000: episode: 215, duration: 21.802s, episode steps: 33, steps per second: 2, episode reward: 1.250, mean reward: 0.038 [0.000, 1.250], mean action: 0.333 [0.000, 3.000], mean observation: 164.446 [0.000, 255.000], loss: 0.075845, mae: 1.776386, mean_q: 2.420476\n",
      "  7177/20000: episode: 216, duration: 21.740s, episode steps: 32, steps per second: 1, episode reward: 1.200, mean reward: 0.037 [0.000, 1.200], mean action: 2.719 [0.000, 3.000], mean observation: 165.988 [0.000, 255.000], loss: 0.079792, mae: 1.732329, mean_q: 2.350755\n",
      "  7207/20000: episode: 217, duration: 21.840s, episode steps: 30, steps per second: 1, episode reward: 2.450, mean reward: 0.082 [0.000, 2.450], mean action: 1.000 [0.000, 3.000], mean observation: 162.203 [0.000, 255.000], loss: 0.146740, mae: 1.717654, mean_q: 2.321089\n",
      "  7232/20000: episode: 218, duration: 21.882s, episode steps: 25, steps per second: 1, episode reward: 1.250, mean reward: 0.050 [0.000, 1.250], mean action: 0.840 [0.000, 3.000], mean observation: 163.963 [0.000, 255.000], loss: 0.173506, mae: 1.727098, mean_q: 2.335451\n",
      "  7255/20000: episode: 219, duration: 21.691s, episode steps: 23, steps per second: 1, episode reward: 2.400, mean reward: 0.104 [0.000, 2.400], mean action: 0.913 [0.000, 3.000], mean observation: 162.175 [0.000, 255.000], loss: 0.098432, mae: 1.739416, mean_q: 2.351663\n",
      "  7281/20000: episode: 220, duration: 22.373s, episode steps: 26, steps per second: 1, episode reward: 4.950, mean reward: 0.190 [0.000, 4.950], mean action: 1.808 [0.000, 3.000], mean observation: 164.126 [0.000, 255.000], loss: 0.242552, mae: 1.729035, mean_q: 2.333471\n",
      "  7310/20000: episode: 221, duration: 21.874s, episode steps: 29, steps per second: 1, episode reward: 2.900, mean reward: 0.100 [0.000, 2.900], mean action: 0.345 [0.000, 2.000], mean observation: 160.430 [0.000, 255.000], loss: 0.080448, mae: 1.753169, mean_q: 2.371940\n",
      "  7341/20000: episode: 222, duration: 21.442s, episode steps: 31, steps per second: 1, episode reward: 2.450, mean reward: 0.079 [0.000, 2.450], mean action: 1.484 [0.000, 3.000], mean observation: 162.751 [0.000, 255.000], loss: 0.016851, mae: 1.713417, mean_q: 2.335090\n",
      "  7373/20000: episode: 223, duration: 21.856s, episode steps: 32, steps per second: 1, episode reward: 2.700, mean reward: 0.084 [0.000, 2.700], mean action: 1.062 [0.000, 3.000], mean observation: 159.921 [0.000, 255.000], loss: 0.074372, mae: 1.744903, mean_q: 2.369528\n",
      "  7404/20000: episode: 224, duration: 21.404s, episode steps: 31, steps per second: 1, episode reward: 1.250, mean reward: 0.040 [0.000, 1.250], mean action: 1.452 [0.000, 3.000], mean observation: 162.545 [0.000, 255.000], loss: 0.079637, mae: 1.752165, mean_q: 2.362480\n",
      "  7436/20000: episode: 225, duration: 21.648s, episode steps: 32, steps per second: 1, episode reward: 6.100, mean reward: 0.191 [0.000, 6.100], mean action: 1.125 [0.000, 3.000], mean observation: 164.368 [0.000, 255.000], loss: 0.069815, mae: 1.730034, mean_q: 2.344514\n",
      "  7468/20000: episode: 226, duration: 21.800s, episode steps: 32, steps per second: 1, episode reward: 1.350, mean reward: 0.042 [0.000, 1.350], mean action: 0.188 [0.000, 3.000], mean observation: 164.094 [0.000, 255.000], loss: 0.135505, mae: 1.682927, mean_q: 2.295598\n",
      "  7500/20000: episode: 227, duration: 21.830s, episode steps: 32, steps per second: 1, episode reward: 1.350, mean reward: 0.042 [0.000, 1.350], mean action: 1.781 [0.000, 3.000], mean observation: 162.995 [0.000, 255.000], loss: 0.075799, mae: 1.696931, mean_q: 2.300707\n",
      "  7532/20000: episode: 228, duration: 21.753s, episode steps: 32, steps per second: 1, episode reward: 1.750, mean reward: 0.055 [0.000, 1.750], mean action: 1.812 [0.000, 3.000], mean observation: 162.616 [0.000, 255.000], loss: 0.083306, mae: 1.731670, mean_q: 2.352067\n",
      "  7561/20000: episode: 229, duration: 21.364s, episode steps: 29, steps per second: 1, episode reward: 2.500, mean reward: 0.086 [0.000, 2.500], mean action: 0.379 [0.000, 2.000], mean observation: 160.396 [0.000, 255.000], loss: 0.145906, mae: 1.730728, mean_q: 2.345272\n",
      "  7593/20000: episode: 230, duration: 21.887s, episode steps: 32, steps per second: 1, episode reward: 2.550, mean reward: 0.080 [0.000, 2.550], mean action: 0.438 [0.000, 2.000], mean observation: 162.026 [0.000, 255.000], loss: 0.309793, mae: 1.747889, mean_q: 2.358848\n",
      "  7624/20000: episode: 231, duration: 21.274s, episode steps: 31, steps per second: 1, episode reward: 2.750, mean reward: 0.089 [0.000, 2.750], mean action: 0.935 [0.000, 3.000], mean observation: 162.807 [0.000, 255.000], loss: 0.206405, mae: 1.726843, mean_q: 2.330415\n",
      "  7655/20000: episode: 232, duration: 21.877s, episode steps: 31, steps per second: 1, episode reward: 1.350, mean reward: 0.044 [0.000, 1.350], mean action: 1.484 [0.000, 3.000], mean observation: 162.755 [0.000, 255.000], loss: 0.192986, mae: 1.717028, mean_q: 2.319266\n",
      "  7686/20000: episode: 233, duration: 21.582s, episode steps: 31, steps per second: 1, episode reward: 2.600, mean reward: 0.084 [0.000, 2.600], mean action: 1.548 [0.000, 2.000], mean observation: 160.239 [0.000, 255.000], loss: 0.129685, mae: 1.718734, mean_q: 2.334424\n",
      "  7718/20000: episode: 234, duration: 21.574s, episode steps: 32, steps per second: 1, episode reward: 3.700, mean reward: 0.116 [0.000, 3.700], mean action: 1.031 [0.000, 3.000], mean observation: 162.628 [0.000, 255.000], loss: 0.072493, mae: 1.702441, mean_q: 2.311661\n",
      "  7749/20000: episode: 235, duration: 21.291s, episode steps: 31, steps per second: 1, episode reward: 2.550, mean reward: 0.082 [0.000, 2.550], mean action: 0.581 [0.000, 2.000], mean observation: 163.159 [0.000, 255.000], loss: 0.013934, mae: 1.705889, mean_q: 2.328006\n",
      "  7781/20000: episode: 236, duration: 21.596s, episode steps: 32, steps per second: 1, episode reward: 2.850, mean reward: 0.089 [0.000, 2.850], mean action: 1.656 [0.000, 3.000], mean observation: 161.609 [0.000, 255.000], loss: 0.020391, mae: 1.675464, mean_q: 2.275936\n",
      "  7813/20000: episode: 237, duration: 21.756s, episode steps: 32, steps per second: 1, episode reward: 2.700, mean reward: 0.084 [0.000, 2.700], mean action: 1.219 [0.000, 3.000], mean observation: 161.722 [0.000, 255.000], loss: 0.193335, mae: 1.738973, mean_q: 2.347675\n",
      "  7845/20000: episode: 238, duration: 21.543s, episode steps: 32, steps per second: 1, episode reward: 2.550, mean reward: 0.080 [0.000, 2.550], mean action: 1.656 [0.000, 3.000], mean observation: 163.034 [0.000, 255.000], loss: 0.065836, mae: 1.714063, mean_q: 2.333335\n",
      "  7877/20000: episode: 239, duration: 21.304s, episode steps: 32, steps per second: 2, episode reward: 3.900, mean reward: 0.122 [0.000, 3.900], mean action: 1.969 [0.000, 3.000], mean observation: 161.597 [0.000, 255.000], loss: 0.244108, mae: 1.674353, mean_q: 2.261893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7909/20000: episode: 240, duration: 21.440s, episode steps: 32, steps per second: 1, episode reward: 4.850, mean reward: 0.152 [0.000, 4.850], mean action: 1.031 [0.000, 2.000], mean observation: 164.260 [0.000, 255.000], loss: 0.249252, mae: 1.701394, mean_q: 2.304226\n",
      "  7941/20000: episode: 241, duration: 21.854s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 1.062 [0.000, 2.000], mean observation: 162.577 [0.000, 255.000], loss: 0.080163, mae: 1.715425, mean_q: 2.334441\n",
      "  7972/20000: episode: 242, duration: 21.567s, episode steps: 31, steps per second: 1, episode reward: 2.350, mean reward: 0.076 [0.000, 2.350], mean action: 1.097 [0.000, 3.000], mean observation: 162.585 [0.000, 255.000], loss: 0.016761, mae: 1.711706, mean_q: 2.339354\n",
      "  8003/20000: episode: 243, duration: 21.256s, episode steps: 31, steps per second: 1, episode reward: 2.650, mean reward: 0.085 [0.000, 2.650], mean action: 1.161 [0.000, 3.000], mean observation: 161.188 [0.000, 255.000], loss: 0.197838, mae: 1.695133, mean_q: 2.310664\n",
      "  8034/20000: episode: 244, duration: 21.597s, episode steps: 31, steps per second: 1, episode reward: 1.200, mean reward: 0.039 [0.000, 1.200], mean action: 2.161 [1.000, 3.000], mean observation: 165.107 [0.000, 255.000], loss: 0.137230, mae: 1.706913, mean_q: 2.331330\n",
      "  8066/20000: episode: 245, duration: 21.876s, episode steps: 32, steps per second: 1, episode reward: 2.350, mean reward: 0.073 [0.000, 2.350], mean action: 1.250 [0.000, 3.000], mean observation: 161.615 [0.000, 255.000], loss: 0.017538, mae: 1.680896, mean_q: 2.289546\n",
      "  8098/20000: episode: 246, duration: 21.526s, episode steps: 32, steps per second: 1, episode reward: 3.750, mean reward: 0.117 [0.000, 3.750], mean action: 2.188 [0.000, 3.000], mean observation: 162.973 [0.000, 255.000], loss: 0.125625, mae: 1.731048, mean_q: 2.372786\n",
      "  8130/20000: episode: 247, duration: 21.853s, episode steps: 32, steps per second: 1, episode reward: 2.800, mean reward: 0.088 [0.000, 2.800], mean action: 1.625 [0.000, 3.000], mean observation: 161.514 [0.000, 255.000], loss: 0.018336, mae: 1.703403, mean_q: 2.316270\n",
      "  8161/20000: episode: 248, duration: 21.309s, episode steps: 31, steps per second: 1, episode reward: 1.250, mean reward: 0.040 [0.000, 1.250], mean action: 1.516 [0.000, 3.000], mean observation: 161.340 [0.000, 255.000], loss: 0.072460, mae: 1.704830, mean_q: 2.317369\n",
      "  8193/20000: episode: 249, duration: 21.508s, episode steps: 32, steps per second: 1, episode reward: 2.350, mean reward: 0.073 [0.000, 2.350], mean action: 2.000 [0.000, 3.000], mean observation: 164.204 [0.000, 255.000], loss: 0.022160, mae: 1.711700, mean_q: 2.327169\n",
      "  8225/20000: episode: 250, duration: 21.704s, episode steps: 32, steps per second: 1, episode reward: 2.550, mean reward: 0.080 [0.000, 2.550], mean action: 0.469 [0.000, 3.000], mean observation: 163.914 [0.000, 255.000], loss: 0.011399, mae: 1.709778, mean_q: 2.329348\n",
      "  8257/20000: episode: 251, duration: 21.511s, episode steps: 32, steps per second: 1, episode reward: 1.550, mean reward: 0.048 [0.000, 1.550], mean action: 1.438 [0.000, 3.000], mean observation: 162.759 [0.000, 255.000], loss: 0.022193, mae: 1.732670, mean_q: 2.357854\n",
      "  8290/20000: episode: 252, duration: 21.770s, episode steps: 33, steps per second: 2, episode reward: 2.800, mean reward: 0.085 [0.000, 2.800], mean action: 1.818 [0.000, 3.000], mean observation: 160.768 [0.000, 255.000], loss: 0.078020, mae: 1.692407, mean_q: 2.303367\n",
      "  8322/20000: episode: 253, duration: 21.406s, episode steps: 32, steps per second: 1, episode reward: 1.350, mean reward: 0.042 [0.000, 1.350], mean action: 0.969 [0.000, 3.000], mean observation: 164.179 [0.000, 255.000], loss: 0.017165, mae: 1.726667, mean_q: 2.358818\n",
      "  8354/20000: episode: 254, duration: 21.890s, episode steps: 32, steps per second: 1, episode reward: 1.350, mean reward: 0.042 [0.000, 1.350], mean action: 0.875 [0.000, 3.000], mean observation: 164.106 [0.000, 255.000], loss: 0.017712, mae: 1.730288, mean_q: 2.352558\n",
      "  8385/20000: episode: 255, duration: 21.640s, episode steps: 31, steps per second: 1, episode reward: 4.900, mean reward: 0.158 [0.000, 4.900], mean action: 1.516 [0.000, 3.000], mean observation: 165.474 [0.000, 255.000], loss: 0.018992, mae: 1.743575, mean_q: 2.359068\n",
      "  8413/20000: episode: 256, duration: 22.261s, episode steps: 28, steps per second: 1, episode reward: 1.250, mean reward: 0.045 [0.000, 1.250], mean action: 2.036 [0.000, 3.000], mean observation: 162.105 [0.000, 255.000], loss: 0.016838, mae: 1.718687, mean_q: 2.328192\n",
      "  8440/20000: episode: 257, duration: 21.774s, episode steps: 27, steps per second: 1, episode reward: 3.600, mean reward: 0.133 [0.000, 3.600], mean action: 1.333 [0.000, 3.000], mean observation: 162.592 [0.000, 255.000], loss: 0.083764, mae: 1.734640, mean_q: 2.341490\n",
      "  8469/20000: episode: 258, duration: 21.863s, episode steps: 29, steps per second: 1, episode reward: 2.550, mean reward: 0.088 [0.000, 2.550], mean action: 0.345 [0.000, 3.000], mean observation: 163.581 [0.000, 255.000], loss: 0.082223, mae: 1.751740, mean_q: 2.359408\n",
      "  8498/20000: episode: 259, duration: 21.479s, episode steps: 29, steps per second: 1, episode reward: 1.150, mean reward: 0.040 [0.000, 1.150], mean action: 0.793 [0.000, 3.000], mean observation: 160.021 [0.000, 255.000], loss: 0.157600, mae: 1.710477, mean_q: 2.330163\n",
      "  8527/20000: episode: 260, duration: 22.420s, episode steps: 29, steps per second: 1, episode reward: 1.450, mean reward: 0.050 [0.000, 1.450], mean action: 1.000 [0.000, 3.000], mean observation: 162.740 [0.000, 255.000], loss: 0.078891, mae: 1.744480, mean_q: 2.363284\n",
      "  8547/20000: episode: 261, duration: 23.236s, episode steps: 20, steps per second: 1, episode reward: 3.800, mean reward: 0.190 [0.000, 3.800], mean action: 0.600 [0.000, 2.000], mean observation: 164.620 [0.000, 255.000], loss: 0.019440, mae: 1.734184, mean_q: 2.346800\n",
      "  8573/20000: episode: 262, duration: 21.727s, episode steps: 26, steps per second: 1, episode reward: 2.550, mean reward: 0.098 [0.000, 2.550], mean action: 0.615 [0.000, 3.000], mean observation: 162.724 [0.000, 255.000], loss: 0.018505, mae: 1.747532, mean_q: 2.362765\n",
      "  8603/20000: episode: 263, duration: 21.603s, episode steps: 30, steps per second: 1, episode reward: 1.650, mean reward: 0.055 [0.000, 1.650], mean action: 0.867 [0.000, 3.000], mean observation: 163.372 [0.000, 255.000], loss: 0.087241, mae: 1.771826, mean_q: 2.389265\n",
      "  8633/20000: episode: 264, duration: 21.754s, episode steps: 30, steps per second: 1, episode reward: 1.350, mean reward: 0.045 [0.000, 1.350], mean action: 2.400 [0.000, 3.000], mean observation: 161.073 [0.000, 255.000], loss: 0.081590, mae: 1.768603, mean_q: 2.382695\n",
      "  8664/20000: episode: 265, duration: 21.797s, episode steps: 31, steps per second: 1, episode reward: 1.400, mean reward: 0.045 [0.000, 1.400], mean action: 0.419 [0.000, 3.000], mean observation: 165.354 [0.000, 255.000], loss: 0.080587, mae: 1.724801, mean_q: 2.345359\n",
      "  8696/20000: episode: 266, duration: 21.646s, episode steps: 32, steps per second: 1, episode reward: 1.350, mean reward: 0.042 [0.000, 1.350], mean action: 1.188 [0.000, 3.000], mean observation: 163.957 [0.000, 255.000], loss: 0.131754, mae: 1.764640, mean_q: 2.375742\n",
      "  8728/20000: episode: 267, duration: 21.891s, episode steps: 32, steps per second: 1, episode reward: 2.550, mean reward: 0.080 [0.000, 2.550], mean action: 1.688 [0.000, 3.000], mean observation: 162.362 [0.000, 255.000], loss: 0.014950, mae: 1.723739, mean_q: 2.345458\n",
      "  8760/20000: episode: 268, duration: 21.821s, episode steps: 32, steps per second: 1, episode reward: 2.850, mean reward: 0.089 [0.000, 2.850], mean action: 1.281 [0.000, 3.000], mean observation: 160.921 [0.000, 255.000], loss: 0.077890, mae: 1.739313, mean_q: 2.351647\n",
      "  8791/20000: episode: 269, duration: 21.963s, episode steps: 31, steps per second: 1, episode reward: 2.450, mean reward: 0.079 [0.000, 2.450], mean action: 1.129 [0.000, 3.000], mean observation: 164.200 [0.000, 255.000], loss: 0.017315, mae: 1.746484, mean_q: 2.383983\n",
      "  8822/20000: episode: 270, duration: 21.604s, episode steps: 31, steps per second: 1, episode reward: 2.650, mean reward: 0.085 [0.000, 2.650], mean action: 0.903 [0.000, 3.000], mean observation: 160.852 [0.000, 255.000], loss: 0.018391, mae: 1.728270, mean_q: 2.330354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8854/20000: episode: 271, duration: 21.693s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 1.531 [0.000, 3.000], mean observation: 164.580 [0.000, 255.000], loss: 0.011210, mae: 1.736886, mean_q: 2.366236\n",
      "  8886/20000: episode: 272, duration: 21.702s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 2.781 [0.000, 3.000], mean observation: 162.633 [0.000, 255.000], loss: 0.201873, mae: 1.750175, mean_q: 2.398872\n",
      "  8917/20000: episode: 273, duration: 21.712s, episode steps: 31, steps per second: 1, episode reward: 1.150, mean reward: 0.037 [0.000, 1.150], mean action: 0.935 [0.000, 3.000], mean observation: 160.007 [0.000, 255.000], loss: 0.086627, mae: 1.699071, mean_q: 2.314070\n",
      "  8948/20000: episode: 274, duration: 21.924s, episode steps: 31, steps per second: 1, episode reward: 1.250, mean reward: 0.040 [0.000, 1.250], mean action: 1.581 [0.000, 3.000], mean observation: 163.421 [0.000, 255.000], loss: 0.014057, mae: 1.666706, mean_q: 2.273025\n",
      "  8978/20000: episode: 275, duration: 21.401s, episode steps: 30, steps per second: 1, episode reward: 2.850, mean reward: 0.095 [0.000, 2.850], mean action: 2.400 [0.000, 3.000], mean observation: 163.451 [0.000, 255.000], loss: 0.020848, mae: 1.708672, mean_q: 2.313188\n",
      "  9008/20000: episode: 276, duration: 21.397s, episode steps: 30, steps per second: 1, episode reward: 3.700, mean reward: 0.123 [0.000, 3.700], mean action: 1.833 [0.000, 3.000], mean observation: 163.437 [0.000, 255.000], loss: 0.017760, mae: 1.715265, mean_q: 2.325670\n",
      "  9039/20000: episode: 277, duration: 21.598s, episode steps: 31, steps per second: 1, episode reward: 2.650, mean reward: 0.085 [0.000, 2.650], mean action: 1.581 [0.000, 3.000], mean observation: 163.666 [0.000, 255.000], loss: 0.018434, mae: 1.727847, mean_q: 2.347698\n",
      "  9071/20000: episode: 278, duration: 21.791s, episode steps: 32, steps per second: 1, episode reward: 3.750, mean reward: 0.117 [0.000, 3.750], mean action: 0.719 [0.000, 3.000], mean observation: 162.533 [0.000, 255.000], loss: 0.070587, mae: 1.718544, mean_q: 2.345101\n",
      "  9101/20000: episode: 279, duration: 21.869s, episode steps: 30, steps per second: 1, episode reward: 3.700, mean reward: 0.123 [0.000, 3.700], mean action: 1.633 [0.000, 3.000], mean observation: 163.091 [0.000, 255.000], loss: 0.072085, mae: 1.693539, mean_q: 2.305318\n",
      "  9132/20000: episode: 280, duration: 21.409s, episode steps: 31, steps per second: 1, episode reward: 2.450, mean reward: 0.079 [0.000, 2.450], mean action: 2.484 [0.000, 3.000], mean observation: 164.255 [0.000, 255.000], loss: 0.077529, mae: 1.701833, mean_q: 2.320617\n",
      "  9160/20000: episode: 281, duration: 21.467s, episode steps: 28, steps per second: 1, episode reward: 2.550, mean reward: 0.091 [0.000, 2.550], mean action: 1.107 [0.000, 3.000], mean observation: 161.984 [0.000, 255.000], loss: 0.165862, mae: 1.725438, mean_q: 2.343534\n",
      "  9191/20000: episode: 282, duration: 21.539s, episode steps: 31, steps per second: 1, episode reward: 1.350, mean reward: 0.044 [0.000, 1.350], mean action: 1.645 [0.000, 3.000], mean observation: 161.196 [0.000, 255.000], loss: 0.082021, mae: 1.695993, mean_q: 2.295930\n",
      "  9224/20000: episode: 283, duration: 21.203s, episode steps: 33, steps per second: 2, episode reward: 3.650, mean reward: 0.111 [0.000, 3.650], mean action: 1.303 [0.000, 3.000], mean observation: 160.572 [0.000, 255.000], loss: 0.136729, mae: 1.687304, mean_q: 2.276533\n",
      "  9254/20000: episode: 284, duration: 21.684s, episode steps: 30, steps per second: 1, episode reward: 1.150, mean reward: 0.038 [0.000, 1.150], mean action: 0.967 [0.000, 2.000], mean observation: 159.456 [0.000, 255.000], loss: 0.083585, mae: 1.737794, mean_q: 2.358549\n",
      "  9285/20000: episode: 285, duration: 21.390s, episode steps: 31, steps per second: 1, episode reward: 2.350, mean reward: 0.076 [0.000, 2.350], mean action: 2.065 [0.000, 3.000], mean observation: 163.434 [0.000, 255.000], loss: 0.026400, mae: 1.737299, mean_q: 2.345660\n",
      "  9317/20000: episode: 286, duration: 21.881s, episode steps: 32, steps per second: 1, episode reward: 2.850, mean reward: 0.089 [0.000, 2.850], mean action: 0.875 [0.000, 3.000], mean observation: 161.245 [0.000, 255.000], loss: 0.196923, mae: 1.732469, mean_q: 2.345996\n",
      "  9346/20000: episode: 287, duration: 21.841s, episode steps: 29, steps per second: 1, episode reward: 1.350, mean reward: 0.047 [0.000, 1.350], mean action: 0.276 [0.000, 2.000], mean observation: 162.234 [0.000, 255.000], loss: 0.019003, mae: 1.748726, mean_q: 2.375950\n",
      "  9377/20000: episode: 288, duration: 21.546s, episode steps: 31, steps per second: 1, episode reward: 1.650, mean reward: 0.053 [0.000, 1.650], mean action: 2.645 [0.000, 3.000], mean observation: 160.083 [0.000, 255.000], loss: 0.086158, mae: 1.760782, mean_q: 2.384057\n",
      "  9409/20000: episode: 289, duration: 21.738s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 1.281 [0.000, 3.000], mean observation: 164.567 [0.000, 255.000], loss: 0.073326, mae: 1.732085, mean_q: 2.368405\n",
      "  9440/20000: episode: 290, duration: 21.530s, episode steps: 31, steps per second: 1, episode reward: 2.600, mean reward: 0.084 [0.000, 2.600], mean action: 1.484 [0.000, 2.000], mean observation: 159.902 [0.000, 255.000], loss: 0.077839, mae: 1.740607, mean_q: 2.371900\n",
      "  9473/20000: episode: 291, duration: 21.726s, episode steps: 33, steps per second: 2, episode reward: 2.900, mean reward: 0.088 [0.000, 2.900], mean action: 1.091 [0.000, 2.000], mean observation: 160.010 [0.000, 255.000], loss: 0.249228, mae: 1.753487, mean_q: 2.366382\n",
      "  9506/20000: episode: 292, duration: 21.612s, episode steps: 33, steps per second: 2, episode reward: 1.200, mean reward: 0.036 [0.000, 1.200], mean action: 2.545 [0.000, 3.000], mean observation: 165.495 [0.000, 255.000], loss: 0.079298, mae: 1.702765, mean_q: 2.306295\n",
      "  9539/20000: episode: 293, duration: 21.676s, episode steps: 33, steps per second: 2, episode reward: 3.900, mean reward: 0.118 [0.000, 3.900], mean action: 1.576 [0.000, 3.000], mean observation: 164.147 [0.000, 255.000], loss: 0.185323, mae: 1.755212, mean_q: 2.369494\n",
      "  9572/20000: episode: 294, duration: 21.647s, episode steps: 33, steps per second: 2, episode reward: 2.850, mean reward: 0.086 [0.000, 2.850], mean action: 1.061 [0.000, 3.000], mean observation: 163.524 [0.000, 255.000], loss: 0.083046, mae: 1.740009, mean_q: 2.357514\n",
      "  9605/20000: episode: 295, duration: 21.433s, episode steps: 33, steps per second: 2, episode reward: 0.200, mean reward: 0.006 [0.000, 0.200], mean action: 2.939 [2.000, 3.000], mean observation: 166.228 [0.000, 255.000], loss: 0.069565, mae: 1.726574, mean_q: 2.342119\n",
      "  9638/20000: episode: 296, duration: 21.625s, episode steps: 33, steps per second: 2, episode reward: 3.750, mean reward: 0.114 [0.000, 3.750], mean action: 2.030 [0.000, 3.000], mean observation: 163.332 [0.000, 255.000], loss: 0.015856, mae: 1.744782, mean_q: 2.369321\n",
      "  9671/20000: episode: 297, duration: 21.309s, episode steps: 33, steps per second: 2, episode reward: 0.100, mean reward: 0.003 [0.000, 0.100], mean action: 2.970 [2.000, 3.000], mean observation: 166.021 [0.000, 255.000], loss: 0.017949, mae: 1.724534, mean_q: 2.342945\n",
      "  9704/20000: episode: 298, duration: 21.887s, episode steps: 33, steps per second: 2, episode reward: 1.350, mean reward: 0.041 [0.000, 1.350], mean action: 2.364 [0.000, 3.000], mean observation: 164.892 [0.000, 255.000], loss: 0.015331, mae: 1.738551, mean_q: 2.364307\n",
      "  9737/20000: episode: 299, duration: 21.791s, episode steps: 33, steps per second: 2, episode reward: 2.650, mean reward: 0.080 [0.000, 2.650], mean action: 1.242 [0.000, 3.000], mean observation: 163.686 [0.000, 255.000], loss: 0.018195, mae: 1.725273, mean_q: 2.342120\n",
      "  9770/20000: episode: 300, duration: 21.366s, episode steps: 33, steps per second: 2, episode reward: 2.450, mean reward: 0.074 [0.000, 2.450], mean action: 1.212 [0.000, 3.000], mean observation: 162.401 [0.000, 255.000], loss: 0.017000, mae: 1.720209, mean_q: 2.336266\n",
      "  9802/20000: episode: 301, duration: 21.428s, episode steps: 32, steps per second: 1, episode reward: 2.800, mean reward: 0.088 [0.000, 2.800], mean action: 1.688 [0.000, 3.000], mean observation: 160.291 [0.000, 255.000], loss: 0.138741, mae: 1.723817, mean_q: 2.334145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9835/20000: episode: 302, duration: 21.372s, episode steps: 33, steps per second: 2, episode reward: 1.350, mean reward: 0.041 [0.000, 1.350], mean action: 2.515 [0.000, 3.000], mean observation: 163.238 [0.000, 255.000], loss: 0.125741, mae: 1.709072, mean_q: 2.318754\n",
      "  9868/20000: episode: 303, duration: 21.629s, episode steps: 33, steps per second: 2, episode reward: 1.550, mean reward: 0.047 [0.000, 1.550], mean action: 2.303 [0.000, 3.000], mean observation: 162.245 [0.000, 255.000], loss: 0.021743, mae: 1.741783, mean_q: 2.364228\n",
      "  9901/20000: episode: 304, duration: 21.321s, episode steps: 33, steps per second: 2, episode reward: 2.550, mean reward: 0.077 [0.000, 2.550], mean action: 1.424 [0.000, 3.000], mean observation: 162.770 [0.000, 255.000], loss: 0.020682, mae: 1.710047, mean_q: 2.310116\n",
      "  9934/20000: episode: 305, duration: 21.491s, episode steps: 33, steps per second: 2, episode reward: 1.450, mean reward: 0.044 [0.000, 1.450], mean action: 1.303 [0.000, 3.000], mean observation: 162.975 [0.000, 255.000], loss: 0.130990, mae: 1.738293, mean_q: 2.339430\n",
      "  9967/20000: episode: 306, duration: 21.415s, episode steps: 33, steps per second: 2, episode reward: 3.800, mean reward: 0.115 [0.000, 3.800], mean action: 1.515 [0.000, 3.000], mean observation: 163.009 [0.000, 255.000], loss: 0.022968, mae: 1.714472, mean_q: 2.313668\n",
      " 10000/20000: episode: 307, duration: 21.367s, episode steps: 33, steps per second: 2, episode reward: 2.350, mean reward: 0.071 [0.000, 2.350], mean action: 1.636 [0.000, 3.000], mean observation: 163.323 [0.000, 255.000], loss: 0.015161, mae: 1.735282, mean_q: 2.354683\n",
      " 10033/20000: episode: 308, duration: 21.468s, episode steps: 33, steps per second: 2, episode reward: 1.350, mean reward: 0.041 [0.000, 1.350], mean action: 1.970 [0.000, 3.000], mean observation: 164.182 [0.000, 255.000], loss: 0.012999, mae: 1.745189, mean_q: 2.373634\n",
      " 10066/20000: episode: 309, duration: 21.450s, episode steps: 33, steps per second: 2, episode reward: 2.750, mean reward: 0.083 [0.000, 2.750], mean action: 0.273 [0.000, 2.000], mean observation: 161.245 [0.000, 255.000], loss: 0.063631, mae: 1.734648, mean_q: 2.348896\n",
      " 10099/20000: episode: 310, duration: 21.613s, episode steps: 33, steps per second: 2, episode reward: 2.450, mean reward: 0.074 [0.000, 2.450], mean action: 2.030 [0.000, 3.000], mean observation: 163.869 [0.000, 255.000], loss: 0.064875, mae: 1.737081, mean_q: 2.347118\n",
      " 10132/20000: episode: 311, duration: 21.591s, episode steps: 33, steps per second: 2, episode reward: 1.250, mean reward: 0.038 [0.000, 1.250], mean action: 2.576 [0.000, 3.000], mean observation: 162.098 [0.000, 255.000], loss: 0.018028, mae: 1.729985, mean_q: 2.339664\n",
      " 10165/20000: episode: 312, duration: 21.588s, episode steps: 33, steps per second: 2, episode reward: 4.000, mean reward: 0.121 [0.000, 4.000], mean action: 1.212 [0.000, 3.000], mean observation: 163.510 [0.000, 255.000], loss: 0.077937, mae: 1.732536, mean_q: 2.333619\n",
      " 10198/20000: episode: 313, duration: 21.588s, episode steps: 33, steps per second: 2, episode reward: 2.450, mean reward: 0.074 [0.000, 2.450], mean action: 1.121 [0.000, 3.000], mean observation: 163.374 [0.000, 255.000], loss: 0.139312, mae: 1.718330, mean_q: 2.323994\n",
      " 10231/20000: episode: 314, duration: 21.633s, episode steps: 33, steps per second: 2, episode reward: 1.950, mean reward: 0.059 [0.000, 1.950], mean action: 1.485 [0.000, 3.000], mean observation: 163.122 [0.000, 255.000], loss: 0.015603, mae: 1.747389, mean_q: 2.358728\n",
      " 10264/20000: episode: 315, duration: 21.450s, episode steps: 33, steps per second: 2, episode reward: 2.550, mean reward: 0.077 [0.000, 2.550], mean action: 2.182 [0.000, 3.000], mean observation: 162.307 [0.000, 255.000], loss: 0.024115, mae: 1.727541, mean_q: 2.333638\n",
      " 10297/20000: episode: 316, duration: 21.643s, episode steps: 33, steps per second: 2, episode reward: 1.150, mean reward: 0.035 [0.000, 1.150], mean action: 2.939 [1.000, 3.000], mean observation: 164.822 [0.000, 255.000], loss: 0.071490, mae: 1.736982, mean_q: 2.338312\n",
      " 10330/20000: episode: 317, duration: 21.603s, episode steps: 33, steps per second: 2, episode reward: 1.550, mean reward: 0.047 [0.000, 1.550], mean action: 2.364 [0.000, 3.000], mean observation: 161.648 [0.000, 255.000], loss: 0.017367, mae: 1.722008, mean_q: 2.325185\n",
      " 10363/20000: episode: 318, duration: 21.494s, episode steps: 33, steps per second: 2, episode reward: 2.550, mean reward: 0.077 [0.000, 2.550], mean action: 2.061 [0.000, 3.000], mean observation: 161.988 [0.000, 255.000], loss: 0.013665, mae: 1.743832, mean_q: 2.354665\n",
      " 10396/20000: episode: 319, duration: 21.893s, episode steps: 33, steps per second: 2, episode reward: 2.450, mean reward: 0.074 [0.000, 2.450], mean action: 0.939 [0.000, 3.000], mean observation: 163.701 [0.000, 255.000], loss: 0.128448, mae: 1.743845, mean_q: 2.347134\n",
      " 10428/20000: episode: 320, duration: 21.228s, episode steps: 32, steps per second: 2, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 0.625 [0.000, 3.000], mean observation: 164.323 [0.000, 255.000], loss: 0.086666, mae: 1.721177, mean_q: 2.322219\n",
      " 10461/20000: episode: 321, duration: 21.367s, episode steps: 33, steps per second: 2, episode reward: 2.650, mean reward: 0.080 [0.000, 2.650], mean action: 2.242 [0.000, 3.000], mean observation: 162.165 [0.000, 255.000], loss: 0.016095, mae: 1.725343, mean_q: 2.323085\n",
      " 10494/20000: episode: 322, duration: 21.486s, episode steps: 33, steps per second: 2, episode reward: 1.250, mean reward: 0.038 [0.000, 1.250], mean action: 1.152 [0.000, 3.000], mean observation: 164.374 [0.000, 255.000], loss: 0.247512, mae: 1.726851, mean_q: 2.334752\n",
      " 10527/20000: episode: 323, duration: 21.726s, episode steps: 33, steps per second: 2, episode reward: 1.350, mean reward: 0.041 [0.000, 1.350], mean action: 2.455 [0.000, 3.000], mean observation: 160.988 [0.000, 255.000], loss: 0.016313, mae: 1.723186, mean_q: 2.336563\n",
      " 10560/20000: episode: 324, duration: 21.503s, episode steps: 33, steps per second: 2, episode reward: 2.800, mean reward: 0.085 [0.000, 2.800], mean action: 2.091 [0.000, 3.000], mean observation: 161.895 [0.000, 255.000], loss: 0.144268, mae: 1.742947, mean_q: 2.340207\n",
      " 10593/20000: episode: 325, duration: 21.440s, episode steps: 33, steps per second: 2, episode reward: 1.150, mean reward: 0.035 [0.000, 1.150], mean action: 1.061 [0.000, 2.000], mean observation: 159.265 [0.000, 255.000], loss: 0.073105, mae: 1.801479, mean_q: 2.404255\n",
      " 10626/20000: episode: 326, duration: 21.529s, episode steps: 33, steps per second: 2, episode reward: 1.250, mean reward: 0.038 [0.000, 1.250], mean action: 1.485 [0.000, 3.000], mean observation: 162.336 [0.000, 255.000], loss: 0.076854, mae: 1.749145, mean_q: 2.375703\n",
      " 10657/20000: episode: 327, duration: 21.772s, episode steps: 31, steps per second: 1, episode reward: 0.200, mean reward: 0.006 [0.000, 0.200], mean action: 0.968 [0.000, 2.000], mean observation: 166.566 [0.000, 255.000], loss: 0.079543, mae: 1.777732, mean_q: 2.407464\n",
      " 10688/20000: episode: 328, duration: 21.454s, episode steps: 31, steps per second: 1, episode reward: 2.950, mean reward: 0.095 [0.000, 2.950], mean action: 1.097 [0.000, 3.000], mean observation: 162.192 [0.000, 255.000], loss: 0.076565, mae: 1.772238, mean_q: 2.395890\n",
      " 10717/20000: episode: 329, duration: 21.616s, episode steps: 29, steps per second: 1, episode reward: 4.900, mean reward: 0.169 [0.000, 4.900], mean action: 1.483 [0.000, 2.000], mean observation: 164.718 [0.000, 255.000], loss: 0.016258, mae: 1.752623, mean_q: 2.378237\n",
      " 10748/20000: episode: 330, duration: 21.341s, episode steps: 31, steps per second: 1, episode reward: 1.250, mean reward: 0.040 [0.000, 1.250], mean action: 1.548 [0.000, 3.000], mean observation: 164.450 [0.000, 255.000], loss: 0.150140, mae: 1.775145, mean_q: 2.409578\n",
      " 10779/20000: episode: 331, duration: 21.502s, episode steps: 31, steps per second: 1, episode reward: 2.450, mean reward: 0.079 [0.000, 2.450], mean action: 1.484 [0.000, 3.000], mean observation: 163.839 [0.000, 255.000], loss: 0.022484, mae: 1.790767, mean_q: 2.415695\n",
      " 10810/20000: episode: 332, duration: 21.600s, episode steps: 31, steps per second: 1, episode reward: 1.250, mean reward: 0.040 [0.000, 1.250], mean action: 1.871 [0.000, 3.000], mean observation: 161.331 [0.000, 255.000], loss: 0.013530, mae: 1.773674, mean_q: 2.393693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10841/20000: episode: 333, duration: 21.263s, episode steps: 31, steps per second: 1, episode reward: 2.800, mean reward: 0.090 [0.000, 2.800], mean action: 0.839 [0.000, 2.000], mean observation: 161.834 [0.000, 255.000], loss: 0.148145, mae: 1.775882, mean_q: 2.380402\n",
      " 10872/20000: episode: 334, duration: 21.377s, episode steps: 31, steps per second: 1, episode reward: 2.450, mean reward: 0.079 [0.000, 2.450], mean action: 1.323 [0.000, 3.000], mean observation: 160.801 [0.000, 255.000], loss: 0.138037, mae: 1.737061, mean_q: 2.336534\n",
      " 10904/20000: episode: 335, duration: 21.619s, episode steps: 32, steps per second: 1, episode reward: 1.450, mean reward: 0.045 [0.000, 1.450], mean action: 0.562 [0.000, 3.000], mean observation: 163.890 [0.000, 255.000], loss: 0.019109, mae: 1.777343, mean_q: 2.395601\n",
      " 10936/20000: episode: 336, duration: 21.680s, episode steps: 32, steps per second: 1, episode reward: 2.850, mean reward: 0.089 [0.000, 2.850], mean action: 2.062 [0.000, 3.000], mean observation: 161.490 [0.000, 255.000], loss: 0.085940, mae: 1.775244, mean_q: 2.393493\n",
      " 10968/20000: episode: 337, duration: 21.625s, episode steps: 32, steps per second: 1, episode reward: 2.350, mean reward: 0.073 [0.000, 2.350], mean action: 0.906 [0.000, 3.000], mean observation: 163.993 [0.000, 255.000], loss: 0.021326, mae: 1.756667, mean_q: 2.406538\n",
      " 11000/20000: episode: 338, duration: 21.683s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 0.031 [0.000, 1.000], mean observation: 164.483 [0.000, 255.000], loss: 0.132159, mae: 1.804455, mean_q: 2.440533\n",
      " 11032/20000: episode: 339, duration: 21.833s, episode steps: 32, steps per second: 1, episode reward: 1.650, mean reward: 0.052 [0.000, 1.650], mean action: 2.375 [0.000, 3.000], mean observation: 162.807 [0.000, 255.000], loss: 0.075116, mae: 1.809735, mean_q: 2.435182\n",
      " 11063/20000: episode: 340, duration: 21.758s, episode steps: 31, steps per second: 1, episode reward: 1.150, mean reward: 0.037 [0.000, 1.150], mean action: 2.000 [0.000, 3.000], mean observation: 162.812 [0.000, 255.000], loss: 0.152429, mae: 1.802352, mean_q: 2.433106\n",
      " 11093/20000: episode: 341, duration: 21.222s, episode steps: 30, steps per second: 1, episode reward: 2.550, mean reward: 0.085 [0.000, 2.550], mean action: 1.400 [0.000, 3.000], mean observation: 163.570 [0.000, 255.000], loss: 0.216124, mae: 1.823299, mean_q: 2.466731\n",
      " 11124/20000: episode: 342, duration: 21.487s, episode steps: 31, steps per second: 1, episode reward: 2.450, mean reward: 0.079 [0.000, 2.450], mean action: 1.613 [0.000, 3.000], mean observation: 161.533 [0.000, 255.000], loss: 0.025319, mae: 1.766131, mean_q: 2.392840\n",
      " 11155/20000: episode: 343, duration: 21.883s, episode steps: 31, steps per second: 1, episode reward: 2.450, mean reward: 0.079 [0.000, 2.450], mean action: 0.903 [0.000, 3.000], mean observation: 161.766 [0.000, 255.000], loss: 0.019702, mae: 1.803214, mean_q: 2.435041\n",
      " 11186/20000: episode: 344, duration: 21.371s, episode steps: 31, steps per second: 1, episode reward: 2.400, mean reward: 0.077 [0.000, 2.400], mean action: 0.742 [0.000, 3.000], mean observation: 163.923 [0.000, 255.000], loss: 0.089491, mae: 1.799986, mean_q: 2.416943\n",
      " 11217/20000: episode: 345, duration: 21.732s, episode steps: 31, steps per second: 1, episode reward: 1.250, mean reward: 0.040 [0.000, 1.250], mean action: 2.387 [0.000, 3.000], mean observation: 164.282 [0.000, 255.000], loss: 0.017553, mae: 1.802297, mean_q: 2.446862\n",
      " 11248/20000: episode: 346, duration: 21.783s, episode steps: 31, steps per second: 1, episode reward: 1.450, mean reward: 0.047 [0.000, 1.450], mean action: 0.548 [0.000, 3.000], mean observation: 163.736 [0.000, 255.000], loss: 0.018210, mae: 1.791316, mean_q: 2.410487\n",
      " 11279/20000: episode: 347, duration: 21.797s, episode steps: 31, steps per second: 1, episode reward: 1.150, mean reward: 0.037 [0.000, 1.150], mean action: 2.613 [1.000, 3.000], mean observation: 163.254 [0.000, 255.000], loss: 0.022607, mae: 1.820852, mean_q: 2.453741\n",
      " 11309/20000: episode: 348, duration: 21.687s, episode steps: 30, steps per second: 1, episode reward: 1.650, mean reward: 0.055 [0.000, 1.650], mean action: 2.367 [0.000, 3.000], mean observation: 161.927 [0.000, 255.000], loss: 0.089972, mae: 1.773146, mean_q: 2.412145\n",
      " 11339/20000: episode: 349, duration: 21.309s, episode steps: 30, steps per second: 1, episode reward: 1.250, mean reward: 0.042 [0.000, 1.250], mean action: 1.000 [0.000, 3.000], mean observation: 164.430 [0.000, 255.000], loss: 0.151910, mae: 1.810751, mean_q: 2.432113\n",
      " 11370/20000: episode: 350, duration: 21.737s, episode steps: 31, steps per second: 1, episode reward: 1.150, mean reward: 0.037 [0.000, 1.150], mean action: 2.806 [1.000, 3.000], mean observation: 164.936 [0.000, 255.000], loss: 0.131776, mae: 1.746643, mean_q: 2.393930\n",
      " 11402/20000: episode: 351, duration: 21.912s, episode steps: 32, steps per second: 1, episode reward: 2.350, mean reward: 0.073 [0.000, 2.350], mean action: 2.500 [0.000, 3.000], mean observation: 164.062 [0.000, 255.000], loss: 0.088151, mae: 1.780774, mean_q: 2.429944\n",
      " 11434/20000: episode: 352, duration: 21.846s, episode steps: 32, steps per second: 1, episode reward: 1.450, mean reward: 0.045 [0.000, 1.450], mean action: 0.469 [0.000, 3.000], mean observation: 164.560 [0.000, 255.000], loss: 0.020144, mae: 1.764796, mean_q: 2.400646\n",
      " 11466/20000: episode: 353, duration: 21.904s, episode steps: 32, steps per second: 1, episode reward: -10.000, mean reward: -0.312 [-10.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 165.903 [0.000, 255.000], loss: 0.214618, mae: 1.778688, mean_q: 2.370920\n",
      " 11498/20000: episode: 354, duration: 21.746s, episode steps: 32, steps per second: 1, episode reward: 2.800, mean reward: 0.088 [0.000, 2.800], mean action: 1.000 [0.000, 3.000], mean observation: 160.871 [0.000, 255.000], loss: 0.085607, mae: 1.778733, mean_q: 2.378364\n",
      " 11529/20000: episode: 355, duration: 21.413s, episode steps: 31, steps per second: 1, episode reward: 2.450, mean reward: 0.079 [0.000, 2.450], mean action: 1.290 [0.000, 3.000], mean observation: 164.087 [0.000, 255.000], loss: 0.211221, mae: 1.788782, mean_q: 2.395533\n",
      " 11561/20000: episode: 356, duration: 21.928s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 0.031 [0.000, 1.000], mean observation: 164.483 [0.000, 255.000], loss: 0.024427, mae: 1.765765, mean_q: 2.385278\n",
      " 11593/20000: episode: 357, duration: 21.935s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 2.719 [1.000, 3.000], mean observation: 163.910 [0.000, 255.000], loss: 0.090710, mae: 1.827489, mean_q: 2.460961\n",
      " 11623/20000: episode: 358, duration: 21.782s, episode steps: 30, steps per second: 1, episode reward: 1.350, mean reward: 0.045 [0.000, 1.350], mean action: 0.667 [0.000, 3.000], mean observation: 164.457 [0.000, 255.000], loss: 0.160269, mae: 1.797453, mean_q: 2.408302\n",
      " 11655/20000: episode: 359, duration: 21.736s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 2.750 [0.000, 3.000], mean observation: 159.806 [0.000, 255.000], loss: 0.089491, mae: 1.774578, mean_q: 2.373507\n",
      " 11687/20000: episode: 360, duration: 21.857s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 2.562 [1.000, 3.000], mean observation: 164.961 [0.000, 255.000], loss: 0.143227, mae: 1.774131, mean_q: 2.391263\n",
      " 11719/20000: episode: 361, duration: 21.617s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 0.656 [0.000, 3.000], mean observation: 164.357 [0.000, 255.000], loss: 0.221499, mae: 1.799611, mean_q: 2.403331\n",
      " 11751/20000: episode: 362, duration: 21.702s, episode steps: 32, steps per second: 1, episode reward: 1.450, mean reward: 0.045 [0.000, 1.450], mean action: 1.688 [0.000, 3.000], mean observation: 163.885 [0.000, 255.000], loss: 0.082327, mae: 1.768358, mean_q: 2.378972\n",
      " 11783/20000: episode: 363, duration: 21.703s, episode steps: 32, steps per second: 1, episode reward: 2.600, mean reward: 0.081 [0.000, 2.600], mean action: 1.469 [0.000, 3.000], mean observation: 160.103 [0.000, 255.000], loss: 0.077008, mae: 1.727592, mean_q: 2.321141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11815/20000: episode: 364, duration: 21.658s, episode steps: 32, steps per second: 1, episode reward: 2.900, mean reward: 0.091 [0.000, 2.900], mean action: 0.438 [0.000, 3.000], mean observation: 163.923 [0.000, 255.000], loss: 0.026889, mae: 1.817892, mean_q: 2.448548\n",
      " 11847/20000: episode: 365, duration: 21.667s, episode steps: 32, steps per second: 1, episode reward: 2.450, mean reward: 0.077 [0.000, 2.450], mean action: 0.469 [0.000, 3.000], mean observation: 164.243 [0.000, 255.000], loss: 0.016316, mae: 1.826207, mean_q: 2.456075\n",
      " 11879/20000: episode: 366, duration: 21.711s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 2.375 [0.000, 3.000], mean observation: 164.435 [0.000, 255.000], loss: 0.205346, mae: 1.847110, mean_q: 2.479177\n",
      " 11910/20000: episode: 367, duration: 21.375s, episode steps: 31, steps per second: 1, episode reward: 0.100, mean reward: 0.003 [0.000, 0.100], mean action: 2.032 [0.000, 3.000], mean observation: 171.055 [0.000, 255.000], loss: 0.036416, mae: 1.859954, mean_q: 2.483150\n",
      " 11941/20000: episode: 368, duration: 21.726s, episode steps: 31, steps per second: 1, episode reward: 1.250, mean reward: 0.040 [0.000, 1.250], mean action: 2.581 [0.000, 3.000], mean observation: 164.817 [0.000, 255.000], loss: 0.023767, mae: 1.833476, mean_q: 2.466345\n",
      " 11972/20000: episode: 369, duration: 21.486s, episode steps: 31, steps per second: 1, episode reward: 2.550, mean reward: 0.082 [0.000, 2.550], mean action: 1.000 [0.000, 3.000], mean observation: 163.458 [0.000, 255.000], loss: 0.025057, mae: 1.851494, mean_q: 2.487223\n",
      " 12003/20000: episode: 370, duration: 21.625s, episode steps: 31, steps per second: 1, episode reward: 1.150, mean reward: 0.037 [0.000, 1.150], mean action: 2.935 [1.000, 3.000], mean observation: 164.970 [0.000, 255.000], loss: 0.093148, mae: 1.856363, mean_q: 2.503582\n",
      " 12033/20000: episode: 371, duration: 21.475s, episode steps: 30, steps per second: 1, episode reward: 1.150, mean reward: 0.038 [0.000, 1.150], mean action: 2.833 [1.000, 3.000], mean observation: 164.816 [0.000, 255.000], loss: 0.019512, mae: 1.839442, mean_q: 2.494697\n",
      " 12063/20000: episode: 372, duration: 21.343s, episode steps: 30, steps per second: 1, episode reward: 2.800, mean reward: 0.093 [0.000, 2.800], mean action: 1.567 [0.000, 3.000], mean observation: 162.326 [0.000, 255.000], loss: 0.091000, mae: 1.868201, mean_q: 2.521279\n",
      " 12094/20000: episode: 373, duration: 21.689s, episode steps: 31, steps per second: 1, episode reward: 1.150, mean reward: 0.037 [0.000, 1.150], mean action: 2.742 [1.000, 3.000], mean observation: 164.129 [0.000, 255.000], loss: 0.092025, mae: 1.882989, mean_q: 2.543872\n",
      " 12125/20000: episode: 374, duration: 21.580s, episode steps: 31, steps per second: 1, episode reward: 1.150, mean reward: 0.037 [0.000, 1.150], mean action: 2.226 [1.000, 3.000], mean observation: 161.572 [0.000, 255.000], loss: 0.099065, mae: 1.874287, mean_q: 2.540713\n",
      " 12156/20000: episode: 375, duration: 21.336s, episode steps: 31, steps per second: 1, episode reward: 2.600, mean reward: 0.084 [0.000, 2.600], mean action: 1.000 [0.000, 3.000], mean observation: 161.977 [0.000, 255.000], loss: 0.150931, mae: 1.912304, mean_q: 2.595809\n",
      " 12186/20000: episode: 376, duration: 21.885s, episode steps: 30, steps per second: 1, episode reward: 2.550, mean reward: 0.085 [0.000, 2.550], mean action: 1.167 [0.000, 3.000], mean observation: 162.616 [0.000, 255.000], loss: 0.209870, mae: 1.835740, mean_q: 2.496717\n",
      " 12215/20000: episode: 377, duration: 21.925s, episode steps: 29, steps per second: 1, episode reward: 1.150, mean reward: 0.040 [0.000, 1.150], mean action: 2.414 [1.000, 3.000], mean observation: 161.919 [0.000, 255.000], loss: 0.034798, mae: 1.892522, mean_q: 2.578018\n",
      " 12245/20000: episode: 378, duration: 21.592s, episode steps: 30, steps per second: 1, episode reward: 1.350, mean reward: 0.045 [0.000, 1.350], mean action: 0.667 [0.000, 3.000], mean observation: 164.381 [0.000, 255.000], loss: 0.161708, mae: 1.899316, mean_q: 2.564044\n",
      " 12276/20000: episode: 379, duration: 21.354s, episode steps: 31, steps per second: 1, episode reward: 2.600, mean reward: 0.084 [0.000, 2.600], mean action: 0.581 [0.000, 3.000], mean observation: 160.168 [0.000, 255.000], loss: 0.034037, mae: 1.879754, mean_q: 2.575146\n",
      " 12307/20000: episode: 380, duration: 21.329s, episode steps: 31, steps per second: 1, episode reward: 2.400, mean reward: 0.077 [0.000, 2.400], mean action: 2.097 [0.000, 3.000], mean observation: 164.226 [0.000, 255.000], loss: 0.092112, mae: 1.877810, mean_q: 2.536407\n",
      " 12338/20000: episode: 381, duration: 21.854s, episode steps: 31, steps per second: 1, episode reward: 2.550, mean reward: 0.082 [0.000, 2.550], mean action: 1.903 [0.000, 3.000], mean observation: 163.091 [0.000, 255.000], loss: 0.081785, mae: 1.888114, mean_q: 2.569615\n",
      " 12369/20000: episode: 382, duration: 21.795s, episode steps: 31, steps per second: 1, episode reward: 1.350, mean reward: 0.044 [0.000, 1.350], mean action: 2.484 [0.000, 3.000], mean observation: 163.659 [0.000, 255.000], loss: 0.028535, mae: 1.864062, mean_q: 2.521811\n",
      " 12399/20000: episode: 383, duration: 21.392s, episode steps: 30, steps per second: 1, episode reward: 2.550, mean reward: 0.085 [0.000, 2.550], mean action: 0.833 [0.000, 3.000], mean observation: 162.636 [0.000, 255.000], loss: 0.024190, mae: 1.868479, mean_q: 2.541377\n",
      " 12430/20000: episode: 384, duration: 21.757s, episode steps: 31, steps per second: 1, episode reward: 1.250, mean reward: 0.040 [0.000, 1.250], mean action: 2.000 [0.000, 3.000], mean observation: 163.632 [0.000, 255.000], loss: 0.017703, mae: 1.883263, mean_q: 2.557361\n",
      " 12461/20000: episode: 385, duration: 21.667s, episode steps: 31, steps per second: 1, episode reward: 2.650, mean reward: 0.085 [0.000, 2.650], mean action: 0.452 [0.000, 3.000], mean observation: 163.756 [0.000, 255.000], loss: 0.085211, mae: 1.881781, mean_q: 2.555924\n",
      " 12493/20000: episode: 386, duration: 21.491s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 2.094 [0.000, 3.000], mean observation: 160.990 [0.000, 255.000], loss: 0.096865, mae: 1.928110, mean_q: 2.603592\n",
      " 12525/20000: episode: 387, duration: 21.714s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 1.562 [0.000, 3.000], mean observation: 161.767 [0.000, 255.000], loss: 0.023185, mae: 1.892574, mean_q: 2.552970\n",
      " 12556/20000: episode: 388, duration: 21.519s, episode steps: 31, steps per second: 1, episode reward: 2.400, mean reward: 0.077 [0.000, 2.400], mean action: 1.258 [0.000, 3.000], mean observation: 161.791 [0.000, 255.000], loss: 0.019887, mae: 1.889614, mean_q: 2.560724\n",
      " 12587/20000: episode: 389, duration: 21.268s, episode steps: 31, steps per second: 1, episode reward: 2.350, mean reward: 0.076 [0.000, 2.350], mean action: 1.677 [0.000, 3.000], mean observation: 162.635 [0.000, 255.000], loss: 0.093183, mae: 1.923365, mean_q: 2.594353\n",
      " 12618/20000: episode: 390, duration: 21.653s, episode steps: 31, steps per second: 1, episode reward: 1.250, mean reward: 0.040 [0.000, 1.250], mean action: 0.613 [0.000, 3.000], mean observation: 164.536 [0.000, 255.000], loss: 0.022453, mae: 1.919077, mean_q: 2.577568\n",
      " 12650/20000: episode: 391, duration: 21.668s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 2.469 [0.000, 3.000], mean observation: 162.816 [0.000, 255.000], loss: 0.080964, mae: 1.927005, mean_q: 2.600471\n",
      " 12681/20000: episode: 392, duration: 21.288s, episode steps: 31, steps per second: 1, episode reward: 1.150, mean reward: 0.037 [0.000, 1.150], mean action: 2.548 [1.000, 3.000], mean observation: 162.413 [0.000, 255.000], loss: 0.023699, mae: 1.904178, mean_q: 2.581606\n",
      " 12713/20000: episode: 393, duration: 21.503s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 2.062 [0.000, 3.000], mean observation: 164.422 [0.000, 255.000], loss: 0.020654, mae: 1.929225, mean_q: 2.612208\n",
      " 12745/20000: episode: 394, duration: 21.694s, episode steps: 32, steps per second: 1, episode reward: 4.950, mean reward: 0.155 [0.000, 4.950], mean action: 1.000 [0.000, 2.000], mean observation: 163.943 [0.000, 255.000], loss: 0.020614, mae: 1.920302, mean_q: 2.586576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12777/20000: episode: 395, duration: 21.842s, episode steps: 32, steps per second: 1, episode reward: 1.150, mean reward: 0.036 [0.000, 1.150], mean action: 2.219 [1.000, 3.000], mean observation: 160.601 [0.000, 255.000], loss: 0.286707, mae: 1.884131, mean_q: 2.515280\n",
      " 12808/20000: episode: 396, duration: 21.452s, episode steps: 31, steps per second: 1, episode reward: 1.500, mean reward: 0.048 [0.000, 1.500], mean action: 1.871 [0.000, 3.000], mean observation: 163.495 [0.000, 255.000], loss: 0.026886, mae: 1.916822, mean_q: 2.581319\n",
      " 12840/20000: episode: 397, duration: 21.909s, episode steps: 32, steps per second: 1, episode reward: 1.150, mean reward: 0.036 [0.000, 1.150], mean action: 2.250 [1.000, 3.000], mean observation: 163.452 [0.000, 255.000], loss: 0.024564, mae: 1.904473, mean_q: 2.564656\n",
      " 12871/20000: episode: 398, duration: 21.497s, episode steps: 31, steps per second: 1, episode reward: 1.550, mean reward: 0.050 [0.000, 1.550], mean action: 1.452 [0.000, 3.000], mean observation: 162.105 [0.000, 255.000], loss: 0.022789, mae: 1.879900, mean_q: 2.520986\n",
      " 12902/20000: episode: 399, duration: 21.446s, episode steps: 31, steps per second: 1, episode reward: 3.700, mean reward: 0.119 [0.000, 3.700], mean action: 1.355 [0.000, 3.000], mean observation: 164.079 [0.000, 255.000], loss: 0.087938, mae: 1.850371, mean_q: 2.490257\n",
      " 12934/20000: episode: 400, duration: 21.836s, episode steps: 32, steps per second: 1, episode reward: 1.150, mean reward: 0.036 [0.000, 1.150], mean action: 2.531 [1.000, 3.000], mean observation: 162.637 [0.000, 255.000], loss: 0.017767, mae: 1.871290, mean_q: 2.516927\n",
      " 12965/20000: episode: 401, duration: 21.350s, episode steps: 31, steps per second: 1, episode reward: 1.150, mean reward: 0.037 [0.000, 1.150], mean action: 2.000 [0.000, 3.000], mean observation: 164.726 [0.000, 255.000], loss: 0.089254, mae: 1.860163, mean_q: 2.503337\n",
      " 12996/20000: episode: 402, duration: 21.775s, episode steps: 31, steps per second: 1, episode reward: 1.250, mean reward: 0.040 [0.000, 1.250], mean action: 0.419 [0.000, 3.000], mean observation: 164.556 [0.000, 255.000], loss: 0.101019, mae: 1.870371, mean_q: 2.513188\n",
      " 13028/20000: episode: 403, duration: 21.471s, episode steps: 32, steps per second: 1, episode reward: 1.350, mean reward: 0.042 [0.000, 1.350], mean action: 1.844 [0.000, 3.000], mean observation: 162.586 [0.000, 255.000], loss: 0.083665, mae: 1.846206, mean_q: 2.486639\n",
      " 13060/20000: episode: 404, duration: 21.666s, episode steps: 32, steps per second: 1, episode reward: 1.650, mean reward: 0.052 [0.000, 1.650], mean action: 2.406 [0.000, 3.000], mean observation: 161.610 [0.000, 255.000], loss: 0.020496, mae: 1.856775, mean_q: 2.523298\n",
      " 13091/20000: episode: 405, duration: 21.537s, episode steps: 31, steps per second: 1, episode reward: 2.750, mean reward: 0.089 [0.000, 2.750], mean action: 2.387 [0.000, 3.000], mean observation: 163.484 [0.000, 255.000], loss: 0.016590, mae: 1.871472, mean_q: 2.523340\n",
      " 13121/20000: episode: 406, duration: 21.671s, episode steps: 30, steps per second: 1, episode reward: 2.600, mean reward: 0.087 [0.000, 2.600], mean action: 1.733 [0.000, 2.000], mean observation: 159.868 [0.000, 255.000], loss: 0.022724, mae: 1.853212, mean_q: 2.491718\n",
      " 13153/20000: episode: 407, duration: 21.285s, episode steps: 32, steps per second: 2, episode reward: 2.550, mean reward: 0.080 [0.000, 2.550], mean action: 1.500 [0.000, 3.000], mean observation: 163.485 [0.000, 255.000], loss: 0.080291, mae: 1.819851, mean_q: 2.454699\n",
      " 13185/20000: episode: 408, duration: 21.565s, episode steps: 32, steps per second: 1, episode reward: 3.650, mean reward: 0.114 [0.000, 3.650], mean action: 1.812 [0.000, 3.000], mean observation: 162.025 [0.000, 255.000], loss: 0.277826, mae: 1.847482, mean_q: 2.470630\n",
      " 13216/20000: episode: 409, duration: 21.348s, episode steps: 31, steps per second: 1, episode reward: 1.250, mean reward: 0.040 [0.000, 1.250], mean action: 0.548 [0.000, 3.000], mean observation: 160.030 [0.000, 255.000], loss: 0.089060, mae: 1.828788, mean_q: 2.464480\n",
      " 13247/20000: episode: 410, duration: 21.665s, episode steps: 31, steps per second: 1, episode reward: 2.350, mean reward: 0.076 [0.000, 2.350], mean action: 0.548 [0.000, 3.000], mean observation: 164.247 [0.000, 255.000], loss: 0.082043, mae: 1.811554, mean_q: 2.442608\n",
      " 13278/20000: episode: 411, duration: 21.303s, episode steps: 31, steps per second: 1, episode reward: 2.700, mean reward: 0.087 [0.000, 2.700], mean action: 1.323 [0.000, 2.000], mean observation: 160.946 [0.000, 255.000], loss: 0.083107, mae: 1.837319, mean_q: 2.479379\n",
      " 13310/20000: episode: 412, duration: 21.486s, episode steps: 32, steps per second: 1, episode reward: 1.350, mean reward: 0.042 [0.000, 1.350], mean action: 1.344 [0.000, 3.000], mean observation: 161.970 [0.000, 255.000], loss: 0.017411, mae: 1.798657, mean_q: 2.442686\n",
      " 13341/20000: episode: 413, duration: 21.250s, episode steps: 31, steps per second: 1, episode reward: 1.450, mean reward: 0.047 [0.000, 1.450], mean action: 0.097 [0.000, 1.000], mean observation: 164.128 [0.000, 255.000], loss: 0.012684, mae: 1.843097, mean_q: 2.498118\n",
      " 13373/20000: episode: 414, duration: 21.806s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 1.938 [0.000, 3.000], mean observation: 162.398 [0.000, 255.000], loss: 0.016166, mae: 1.805276, mean_q: 2.439314\n",
      " 13405/20000: episode: 415, duration: 21.491s, episode steps: 32, steps per second: 1, episode reward: 2.600, mean reward: 0.081 [0.000, 2.600], mean action: 1.469 [0.000, 3.000], mean observation: 162.699 [0.000, 255.000], loss: 0.019306, mae: 1.817290, mean_q: 2.450992\n",
      " 13437/20000: episode: 416, duration: 21.663s, episode steps: 32, steps per second: 1, episode reward: 1.350, mean reward: 0.042 [0.000, 1.350], mean action: 1.906 [0.000, 3.000], mean observation: 163.620 [0.000, 255.000], loss: 0.019682, mae: 1.820671, mean_q: 2.451375\n",
      " 13468/20000: episode: 417, duration: 21.585s, episode steps: 31, steps per second: 1, episode reward: 2.750, mean reward: 0.089 [0.000, 2.750], mean action: 0.968 [0.000, 3.000], mean observation: 161.776 [0.000, 255.000], loss: 0.082597, mae: 1.795523, mean_q: 2.418223\n",
      " 13498/20000: episode: 418, duration: 21.274s, episode steps: 30, steps per second: 1, episode reward: 2.450, mean reward: 0.082 [0.000, 2.450], mean action: 2.500 [0.000, 3.000], mean observation: 163.704 [0.000, 255.000], loss: 0.021654, mae: 1.832714, mean_q: 2.472120\n",
      " 13529/20000: episode: 419, duration: 21.666s, episode steps: 31, steps per second: 1, episode reward: 4.900, mean reward: 0.158 [0.000, 4.900], mean action: 1.419 [0.000, 3.000], mean observation: 165.484 [0.000, 255.000], loss: 0.016067, mae: 1.815629, mean_q: 2.442606\n",
      " 13560/20000: episode: 420, duration: 21.763s, episode steps: 31, steps per second: 1, episode reward: 2.500, mean reward: 0.081 [0.000, 2.500], mean action: 1.774 [0.000, 2.000], mean observation: 159.582 [0.000, 255.000], loss: 0.015034, mae: 1.813227, mean_q: 2.446860\n",
      " 13590/20000: episode: 421, duration: 21.376s, episode steps: 30, steps per second: 1, episode reward: 4.850, mean reward: 0.162 [0.000, 4.850], mean action: 1.200 [0.000, 3.000], mean observation: 164.499 [0.000, 255.000], loss: 0.081803, mae: 1.791098, mean_q: 2.409439\n",
      " 13621/20000: episode: 422, duration: 21.565s, episode steps: 31, steps per second: 1, episode reward: 4.850, mean reward: 0.156 [0.000, 4.850], mean action: 1.000 [0.000, 3.000], mean observation: 163.345 [0.000, 255.000], loss: 0.013065, mae: 1.797908, mean_q: 2.419526\n",
      " 13650/20000: episode: 423, duration: 21.987s, episode steps: 29, steps per second: 1, episode reward: 1.650, mean reward: 0.057 [0.000, 1.650], mean action: 0.621 [0.000, 3.000], mean observation: 161.712 [0.000, 255.000], loss: 0.091099, mae: 1.786018, mean_q: 2.406393\n",
      " 13681/20000: episode: 424, duration: 21.791s, episode steps: 31, steps per second: 1, episode reward: 4.900, mean reward: 0.158 [0.000, 4.900], mean action: 1.677 [0.000, 3.000], mean observation: 166.171 [0.000, 255.000], loss: 0.161436, mae: 1.825902, mean_q: 2.442781\n",
      " 13711/20000: episode: 425, duration: 21.806s, episode steps: 30, steps per second: 1, episode reward: 3.050, mean reward: 0.102 [0.000, 3.050], mean action: 1.200 [0.000, 3.000], mean observation: 161.968 [0.000, 255.000], loss: 0.016734, mae: 1.819984, mean_q: 2.437462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13737/20000: episode: 426, duration: 21.873s, episode steps: 26, steps per second: 1, episode reward: 1.450, mean reward: 0.056 [0.000, 1.450], mean action: 0.654 [0.000, 3.000], mean observation: 162.539 [0.000, 255.000], loss: 0.103379, mae: 1.750997, mean_q: 2.357804\n",
      " 13765/20000: episode: 427, duration: 21.977s, episode steps: 28, steps per second: 1, episode reward: 2.900, mean reward: 0.104 [0.000, 2.900], mean action: 1.679 [0.000, 3.000], mean observation: 161.291 [0.000, 255.000], loss: 0.087892, mae: 1.797143, mean_q: 2.428109\n",
      " 13793/20000: episode: 428, duration: 21.463s, episode steps: 28, steps per second: 1, episode reward: 1.250, mean reward: 0.045 [0.000, 1.250], mean action: 2.429 [0.000, 3.000], mean observation: 164.210 [0.000, 255.000], loss: 0.018744, mae: 1.813861, mean_q: 2.440982\n",
      " 13820/20000: episode: 429, duration: 21.487s, episode steps: 27, steps per second: 1, episode reward: 2.750, mean reward: 0.102 [0.000, 2.750], mean action: 0.741 [0.000, 3.000], mean observation: 162.080 [0.000, 255.000], loss: 0.166762, mae: 1.742574, mean_q: 2.340098\n",
      " 13849/20000: episode: 430, duration: 21.446s, episode steps: 29, steps per second: 1, episode reward: 2.700, mean reward: 0.093 [0.000, 2.700], mean action: 0.966 [0.000, 3.000], mean observation: 161.514 [0.000, 255.000], loss: 0.157261, mae: 1.778393, mean_q: 2.382187\n",
      " 13877/20000: episode: 431, duration: 23.297s, episode steps: 28, steps per second: 1, episode reward: 3.650, mean reward: 0.130 [0.000, 3.650], mean action: 1.464 [0.000, 3.000], mean observation: 161.904 [0.000, 255.000], loss: 0.087440, mae: 1.791448, mean_q: 2.402912\n",
      " 13907/20000: episode: 432, duration: 21.973s, episode steps: 30, steps per second: 1, episode reward: 4.900, mean reward: 0.163 [0.000, 4.900], mean action: 1.400 [0.000, 3.000], mean observation: 165.780 [0.000, 255.000], loss: 0.021365, mae: 1.789934, mean_q: 2.400224\n",
      " 13938/20000: episode: 433, duration: 21.879s, episode steps: 31, steps per second: 1, episode reward: 1.450, mean reward: 0.047 [0.000, 1.450], mean action: 0.097 [0.000, 1.000], mean observation: 164.128 [0.000, 255.000], loss: 0.082866, mae: 1.784990, mean_q: 2.396266\n",
      " 13969/20000: episode: 434, duration: 21.466s, episode steps: 31, steps per second: 1, episode reward: 3.600, mean reward: 0.116 [0.000, 3.600], mean action: 0.871 [0.000, 3.000], mean observation: 163.205 [0.000, 255.000], loss: 0.015804, mae: 1.786940, mean_q: 2.401143\n",
      " 13999/20000: episode: 435, duration: 21.617s, episode steps: 30, steps per second: 1, episode reward: 2.450, mean reward: 0.082 [0.000, 2.450], mean action: 1.667 [0.000, 3.000], mean observation: 163.885 [0.000, 255.000], loss: 0.016923, mae: 1.770774, mean_q: 2.377914\n",
      " 14030/20000: episode: 436, duration: 21.494s, episode steps: 31, steps per second: 1, episode reward: 1.150, mean reward: 0.037 [0.000, 1.150], mean action: 2.065 [0.000, 3.000], mean observation: 162.393 [0.000, 255.000], loss: 0.090385, mae: 1.790696, mean_q: 2.407730\n",
      " 14061/20000: episode: 437, duration: 21.582s, episode steps: 31, steps per second: 1, episode reward: 1.150, mean reward: 0.037 [0.000, 1.150], mean action: 2.000 [0.000, 3.000], mean observation: 161.778 [0.000, 255.000], loss: 0.013252, mae: 1.742850, mean_q: 2.340908\n",
      " 14092/20000: episode: 438, duration: 21.631s, episode steps: 31, steps per second: 1, episode reward: 2.350, mean reward: 0.076 [0.000, 2.350], mean action: 1.129 [0.000, 3.000], mean observation: 162.979 [0.000, 255.000], loss: 0.082962, mae: 1.767018, mean_q: 2.372077\n",
      " 14123/20000: episode: 439, duration: 21.585s, episode steps: 31, steps per second: 1, episode reward: 1.250, mean reward: 0.040 [0.000, 1.250], mean action: 2.226 [0.000, 3.000], mean observation: 162.194 [0.000, 255.000], loss: 0.017631, mae: 1.751657, mean_q: 2.357198\n",
      " 14154/20000: episode: 440, duration: 21.415s, episode steps: 31, steps per second: 1, episode reward: 1.150, mean reward: 0.037 [0.000, 1.150], mean action: 2.323 [1.000, 3.000], mean observation: 161.993 [0.000, 255.000], loss: 0.015380, mae: 1.762527, mean_q: 2.368192\n",
      " 14183/20000: episode: 441, duration: 21.335s, episode steps: 29, steps per second: 1, episode reward: 2.850, mean reward: 0.098 [0.000, 2.850], mean action: 1.655 [0.000, 3.000], mean observation: 161.805 [0.000, 255.000], loss: 0.011298, mae: 1.775430, mean_q: 2.394068\n",
      " 14213/20000: episode: 442, duration: 21.990s, episode steps: 30, steps per second: 1, episode reward: 2.350, mean reward: 0.078 [0.000, 2.350], mean action: 0.300 [0.000, 3.000], mean observation: 163.352 [0.000, 255.000], loss: 0.086331, mae: 1.782867, mean_q: 2.401371\n",
      " 14242/20000: episode: 443, duration: 21.602s, episode steps: 29, steps per second: 1, episode reward: 1.150, mean reward: 0.040 [0.000, 1.150], mean action: 2.069 [1.000, 3.000], mean observation: 159.897 [0.000, 255.000], loss: 0.014888, mae: 1.760126, mean_q: 2.373868\n",
      " 14267/20000: episode: 444, duration: 21.716s, episode steps: 25, steps per second: 1, episode reward: 1.150, mean reward: 0.046 [0.000, 1.150], mean action: 2.560 [1.000, 3.000], mean observation: 163.302 [0.000, 255.000], loss: 0.022150, mae: 1.757575, mean_q: 2.392510\n",
      " 14296/20000: episode: 445, duration: 22.059s, episode steps: 29, steps per second: 1, episode reward: 2.550, mean reward: 0.088 [0.000, 2.550], mean action: 1.759 [0.000, 3.000], mean observation: 161.060 [0.000, 255.000], loss: 0.014902, mae: 1.743941, mean_q: 2.357934\n",
      " 14325/20000: episode: 446, duration: 21.523s, episode steps: 29, steps per second: 1, episode reward: 1.250, mean reward: 0.043 [0.000, 1.250], mean action: 1.000 [0.000, 3.000], mean observation: 164.274 [0.000, 255.000], loss: 0.091666, mae: 1.751378, mean_q: 2.358217\n",
      " 14356/20000: episode: 447, duration: 21.329s, episode steps: 31, steps per second: 1, episode reward: 4.850, mean reward: 0.156 [0.000, 4.850], mean action: 1.194 [0.000, 3.000], mean observation: 164.162 [0.000, 255.000], loss: 0.141335, mae: 1.743049, mean_q: 2.362036\n",
      " 14386/20000: episode: 448, duration: 21.376s, episode steps: 30, steps per second: 1, episode reward: 1.250, mean reward: 0.042 [0.000, 1.250], mean action: 1.267 [0.000, 3.000], mean observation: 164.247 [0.000, 255.000], loss: 0.014397, mae: 1.764985, mean_q: 2.380810\n",
      " 14417/20000: episode: 449, duration: 21.583s, episode steps: 31, steps per second: 1, episode reward: 1.650, mean reward: 0.053 [0.000, 1.650], mean action: 0.935 [0.000, 3.000], mean observation: 163.561 [0.000, 255.000], loss: 0.078719, mae: 1.727023, mean_q: 2.325686\n",
      " 14448/20000: episode: 450, duration: 21.574s, episode steps: 31, steps per second: 1, episode reward: 3.150, mean reward: 0.102 [0.000, 3.150], mean action: 2.000 [0.000, 3.000], mean observation: 162.289 [0.000, 255.000], loss: 0.148212, mae: 1.723048, mean_q: 2.328155\n",
      " 14479/20000: episode: 451, duration: 21.522s, episode steps: 31, steps per second: 1, episode reward: 1.350, mean reward: 0.044 [0.000, 1.350], mean action: 0.516 [0.000, 3.000], mean observation: 164.148 [0.000, 255.000], loss: 0.078533, mae: 1.743152, mean_q: 2.361479\n",
      " 14510/20000: episode: 452, duration: 21.339s, episode steps: 31, steps per second: 1, episode reward: 1.150, mean reward: 0.037 [0.000, 1.150], mean action: 2.032 [0.000, 3.000], mean observation: 161.587 [0.000, 255.000], loss: 0.154834, mae: 1.704563, mean_q: 2.305468\n",
      " 14541/20000: episode: 453, duration: 21.618s, episode steps: 31, steps per second: 1, episode reward: 2.550, mean reward: 0.082 [0.000, 2.550], mean action: 0.452 [0.000, 3.000], mean observation: 163.933 [0.000, 255.000], loss: 0.017283, mae: 1.721291, mean_q: 2.316744\n",
      " 14572/20000: episode: 454, duration: 21.400s, episode steps: 31, steps per second: 1, episode reward: 5.000, mean reward: 0.161 [0.000, 5.000], mean action: 1.774 [0.000, 3.000], mean observation: 162.980 [0.000, 255.000], loss: 0.073470, mae: 1.729522, mean_q: 2.322166\n",
      " 14603/20000: episode: 455, duration: 21.375s, episode steps: 31, steps per second: 1, episode reward: 2.550, mean reward: 0.082 [0.000, 2.550], mean action: 0.742 [0.000, 3.000], mean observation: 164.033 [0.000, 255.000], loss: 0.079185, mae: 1.720774, mean_q: 2.319720\n",
      " 14634/20000: episode: 456, duration: 21.561s, episode steps: 31, steps per second: 1, episode reward: 1.250, mean reward: 0.040 [0.000, 1.250], mean action: 1.097 [0.000, 3.000], mean observation: 164.716 [0.000, 255.000], loss: 0.016196, mae: 1.737704, mean_q: 2.356069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14665/20000: episode: 457, duration: 21.310s, episode steps: 31, steps per second: 1, episode reward: 2.650, mean reward: 0.085 [0.000, 2.650], mean action: 2.355 [0.000, 3.000], mean observation: 161.829 [0.000, 255.000], loss: 0.071598, mae: 1.719283, mean_q: 2.301552\n",
      " 14696/20000: episode: 458, duration: 21.750s, episode steps: 31, steps per second: 1, episode reward: 1.250, mean reward: 0.040 [0.000, 1.250], mean action: 1.387 [0.000, 3.000], mean observation: 164.439 [0.000, 255.000], loss: 0.016863, mae: 1.722586, mean_q: 2.312104\n",
      " 14726/20000: episode: 459, duration: 21.571s, episode steps: 30, steps per second: 1, episode reward: 3.800, mean reward: 0.127 [0.000, 3.800], mean action: 0.467 [0.000, 3.000], mean observation: 164.547 [0.000, 255.000], loss: 0.081333, mae: 1.705655, mean_q: 2.293924\n",
      " 14757/20000: episode: 460, duration: 21.711s, episode steps: 31, steps per second: 1, episode reward: 4.950, mean reward: 0.160 [0.000, 4.950], mean action: 1.161 [0.000, 3.000], mean observation: 164.988 [0.000, 255.000], loss: 0.075424, mae: 1.696302, mean_q: 2.280934\n",
      " 14789/20000: episode: 461, duration: 21.610s, episode steps: 32, steps per second: 1, episode reward: 1.150, mean reward: 0.036 [0.000, 1.150], mean action: 1.406 [0.000, 2.000], mean observation: 159.174 [0.000, 255.000], loss: 0.078763, mae: 1.735174, mean_q: 2.336083\n",
      " 14821/20000: episode: 462, duration: 21.791s, episode steps: 32, steps per second: 1, episode reward: 1.150, mean reward: 0.036 [0.000, 1.150], mean action: 2.312 [0.000, 3.000], mean observation: 162.258 [0.000, 255.000], loss: 0.078976, mae: 1.697170, mean_q: 2.270672\n",
      " 14852/20000: episode: 463, duration: 21.406s, episode steps: 31, steps per second: 1, episode reward: 1.150, mean reward: 0.037 [0.000, 1.150], mean action: 2.645 [1.000, 3.000], mean observation: 163.465 [0.000, 255.000], loss: 0.012259, mae: 1.703628, mean_q: 2.290492\n",
      " 14882/20000: episode: 464, duration: 21.687s, episode steps: 30, steps per second: 1, episode reward: 1.250, mean reward: 0.042 [0.000, 1.250], mean action: 0.967 [0.000, 3.000], mean observation: 164.661 [0.000, 255.000], loss: 0.014343, mae: 1.713213, mean_q: 2.307702\n",
      " 14913/20000: episode: 465, duration: 21.263s, episode steps: 31, steps per second: 1, episode reward: 1.350, mean reward: 0.044 [0.000, 1.350], mean action: 1.323 [0.000, 2.000], mean observation: 161.118 [0.000, 255.000], loss: 0.086724, mae: 1.727872, mean_q: 2.328170\n",
      " 14944/20000: episode: 466, duration: 21.470s, episode steps: 31, steps per second: 1, episode reward: 1.350, mean reward: 0.044 [0.000, 1.350], mean action: 1.355 [0.000, 3.000], mean observation: 161.758 [0.000, 255.000], loss: 0.013004, mae: 1.702603, mean_q: 2.296305\n",
      " 14976/20000: episode: 467, duration: 21.904s, episode steps: 32, steps per second: 1, episode reward: 2.650, mean reward: 0.083 [0.000, 2.650], mean action: 0.594 [0.000, 3.000], mean observation: 163.170 [0.000, 255.000], loss: 0.215263, mae: 1.683631, mean_q: 2.249527\n",
      " 15007/20000: episode: 468, duration: 21.375s, episode steps: 31, steps per second: 1, episode reward: 1.250, mean reward: 0.040 [0.000, 1.250], mean action: 0.065 [0.000, 1.000], mean observation: 164.338 [0.000, 255.000], loss: 0.203774, mae: 1.694281, mean_q: 2.266256\n",
      " 15038/20000: episode: 469, duration: 21.283s, episode steps: 31, steps per second: 1, episode reward: 1.350, mean reward: 0.044 [0.000, 1.350], mean action: 0.065 [0.000, 1.000], mean observation: 164.331 [0.000, 255.000], loss: 0.012043, mae: 1.722273, mean_q: 2.325381\n",
      " 15069/20000: episode: 470, duration: 21.724s, episode steps: 31, steps per second: 1, episode reward: 2.800, mean reward: 0.090 [0.000, 2.800], mean action: 1.258 [0.000, 3.000], mean observation: 161.620 [0.000, 255.000], loss: 0.201809, mae: 1.685982, mean_q: 2.257594\n",
      " 15101/20000: episode: 471, duration: 21.705s, episode steps: 32, steps per second: 1, episode reward: 1.350, mean reward: 0.042 [0.000, 1.350], mean action: 0.812 [0.000, 3.000], mean observation: 163.310 [0.000, 255.000], loss: 0.136669, mae: 1.659942, mean_q: 2.246168\n",
      " 15133/20000: episode: 472, duration: 21.779s, episode steps: 32, steps per second: 1, episode reward: 2.600, mean reward: 0.081 [0.000, 2.600], mean action: 0.938 [0.000, 2.000], mean observation: 159.854 [0.000, 255.000], loss: 0.196061, mae: 1.650773, mean_q: 2.237768\n",
      " 15165/20000: episode: 473, duration: 21.658s, episode steps: 32, steps per second: 1, episode reward: 1.350, mean reward: 0.042 [0.000, 1.350], mean action: 0.375 [0.000, 3.000], mean observation: 163.904 [0.000, 255.000], loss: 0.081099, mae: 1.677770, mean_q: 2.268667\n",
      " 15197/20000: episode: 474, duration: 21.723s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 1.781 [0.000, 3.000], mean observation: 164.428 [0.000, 255.000], loss: 0.081817, mae: 1.663164, mean_q: 2.237862\n",
      " 15229/20000: episode: 475, duration: 21.520s, episode steps: 32, steps per second: 1, episode reward: 2.350, mean reward: 0.073 [0.000, 2.350], mean action: 0.312 [0.000, 3.000], mean observation: 163.736 [0.000, 255.000], loss: 0.077159, mae: 1.672070, mean_q: 2.264126\n",
      " 15261/20000: episode: 476, duration: 21.631s, episode steps: 32, steps per second: 1, episode reward: 2.750, mean reward: 0.086 [0.000, 2.750], mean action: 0.406 [0.000, 3.000], mean observation: 161.807 [0.000, 255.000], loss: 0.017998, mae: 1.674417, mean_q: 2.280555\n",
      " 15293/20000: episode: 477, duration: 21.590s, episode steps: 32, steps per second: 1, episode reward: 1.350, mean reward: 0.042 [0.000, 1.350], mean action: 0.812 [0.000, 3.000], mean observation: 164.285 [0.000, 255.000], loss: 0.016454, mae: 1.648152, mean_q: 2.228310\n",
      " 15325/20000: episode: 478, duration: 21.699s, episode steps: 32, steps per second: 1, episode reward: 1.250, mean reward: 0.039 [0.000, 1.250], mean action: 1.531 [0.000, 3.000], mean observation: 164.483 [0.000, 255.000], loss: 0.017228, mae: 1.670492, mean_q: 2.253742\n",
      " 15357/20000: episode: 479, duration: 21.732s, episode steps: 32, steps per second: 1, episode reward: 2.700, mean reward: 0.084 [0.000, 2.700], mean action: 1.156 [0.000, 3.000], mean observation: 163.083 [0.000, 255.000], loss: 0.136857, mae: 1.675031, mean_q: 2.261986\n",
      " 15389/20000: episode: 480, duration: 21.720s, episode steps: 32, steps per second: 1, episode reward: 2.550, mean reward: 0.080 [0.000, 2.550], mean action: 2.156 [0.000, 3.000], mean observation: 161.487 [0.000, 255.000], loss: 0.075143, mae: 1.680971, mean_q: 2.280665\n",
      " 15419/20000: episode: 481, duration: 21.462s, episode steps: 30, steps per second: 1, episode reward: 2.700, mean reward: 0.090 [0.000, 2.700], mean action: 1.100 [0.000, 2.000], mean observation: 160.827 [0.000, 255.000], loss: 0.079356, mae: 1.673165, mean_q: 2.258732\n",
      " 15455/20000: episode: 482, duration: 21.312s, episode steps: 36, steps per second: 2, episode reward: 2.650, mean reward: 0.074 [0.000, 2.650], mean action: 0.417 [0.000, 3.000], mean observation: 163.469 [0.000, 255.000], loss: 0.017715, mae: 1.673496, mean_q: 2.273063\n",
      " 15492/20000: episode: 483, duration: 21.211s, episode steps: 37, steps per second: 2, episode reward: 1.250, mean reward: 0.034 [0.000, 1.250], mean action: 1.486 [0.000, 3.000], mean observation: 164.027 [0.000, 255.000], loss: 0.118943, mae: 1.671759, mean_q: 2.254769\n",
      " 15529/20000: episode: 484, duration: 21.112s, episode steps: 37, steps per second: 2, episode reward: 4.000, mean reward: 0.108 [0.000, 4.000], mean action: 1.919 [0.000, 3.000], mean observation: 161.141 [0.000, 255.000], loss: 0.014884, mae: 1.660390, mean_q: 2.243113\n",
      " 15566/20000: episode: 485, duration: 21.135s, episode steps: 37, steps per second: 2, episode reward: 1.250, mean reward: 0.034 [0.000, 1.250], mean action: 2.378 [0.000, 3.000], mean observation: 161.804 [0.000, 255.000], loss: 0.017343, mae: 1.667746, mean_q: 2.257905\n",
      " 15603/20000: episode: 486, duration: 21.352s, episode steps: 37, steps per second: 2, episode reward: 2.700, mean reward: 0.073 [0.000, 2.700], mean action: 1.324 [0.000, 3.000], mean observation: 160.013 [0.000, 255.000], loss: 0.177187, mae: 1.648986, mean_q: 2.208220\n",
      " 15640/20000: episode: 487, duration: 21.255s, episode steps: 37, steps per second: 2, episode reward: 1.250, mean reward: 0.034 [0.000, 1.250], mean action: 1.919 [0.000, 3.000], mean observation: 164.397 [0.000, 255.000], loss: 0.016299, mae: 1.659047, mean_q: 2.235069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15677/20000: episode: 488, duration: 21.171s, episode steps: 37, steps per second: 2, episode reward: 1.250, mean reward: 0.034 [0.000, 1.250], mean action: 0.108 [0.000, 3.000], mean observation: 164.262 [0.000, 255.000], loss: 0.014430, mae: 1.660562, mean_q: 2.239698\n",
      " 15714/20000: episode: 489, duration: 21.440s, episode steps: 37, steps per second: 2, episode reward: 3.800, mean reward: 0.103 [0.000, 3.800], mean action: 1.541 [0.000, 3.000], mean observation: 160.453 [0.000, 255.000], loss: 0.176260, mae: 1.679229, mean_q: 2.263527\n",
      " 15751/20000: episode: 490, duration: 21.395s, episode steps: 37, steps per second: 2, episode reward: 1.150, mean reward: 0.031 [0.000, 1.150], mean action: 0.189 [0.000, 2.000], mean observation: 159.024 [0.000, 255.000], loss: 0.073692, mae: 1.665263, mean_q: 2.249958\n",
      " 15788/20000: episode: 491, duration: 21.258s, episode steps: 37, steps per second: 2, episode reward: 2.700, mean reward: 0.073 [0.000, 2.700], mean action: 1.486 [0.000, 3.000], mean observation: 160.386 [0.000, 255.000], loss: 0.015130, mae: 1.645372, mean_q: 2.224125\n",
      " 15825/20000: episode: 492, duration: 21.454s, episode steps: 37, steps per second: 2, episode reward: 5.000, mean reward: 0.135 [0.000, 5.000], mean action: 1.568 [0.000, 3.000], mean observation: 163.884 [0.000, 255.000], loss: 0.015077, mae: 1.640809, mean_q: 2.201390\n",
      " 15861/20000: episode: 493, duration: 21.158s, episode steps: 36, steps per second: 2, episode reward: 2.550, mean reward: 0.071 [0.000, 2.550], mean action: 2.333 [0.000, 3.000], mean observation: 163.431 [0.000, 255.000], loss: 0.015845, mae: 1.678059, mean_q: 2.255650\n",
      " 15898/20000: episode: 494, duration: 21.359s, episode steps: 37, steps per second: 2, episode reward: 1.350, mean reward: 0.036 [0.000, 1.350], mean action: 1.081 [0.000, 3.000], mean observation: 159.185 [0.000, 255.000], loss: 0.069211, mae: 1.663278, mean_q: 2.245923\n",
      " 15935/20000: episode: 495, duration: 21.283s, episode steps: 37, steps per second: 2, episode reward: 1.350, mean reward: 0.036 [0.000, 1.350], mean action: 1.919 [0.000, 3.000], mean observation: 161.578 [0.000, 255.000], loss: 0.118755, mae: 1.645315, mean_q: 2.208277\n",
      " 15972/20000: episode: 496, duration: 21.233s, episode steps: 37, steps per second: 2, episode reward: 3.100, mean reward: 0.084 [0.000, 3.100], mean action: 1.054 [0.000, 3.000], mean observation: 160.879 [0.000, 255.000], loss: 0.020716, mae: 1.651710, mean_q: 2.220172\n",
      " 16009/20000: episode: 497, duration: 21.299s, episode steps: 37, steps per second: 2, episode reward: 2.450, mean reward: 0.066 [0.000, 2.450], mean action: 1.270 [0.000, 3.000], mean observation: 160.905 [0.000, 255.000], loss: 0.123452, mae: 1.672573, mean_q: 2.254022\n",
      " 16046/20000: episode: 498, duration: 21.450s, episode steps: 37, steps per second: 2, episode reward: 4.950, mean reward: 0.134 [0.000, 4.950], mean action: 1.595 [0.000, 3.000], mean observation: 162.084 [0.000, 255.000], loss: 0.151949, mae: 1.655172, mean_q: 2.229902\n",
      " 16083/20000: episode: 499, duration: 21.106s, episode steps: 37, steps per second: 2, episode reward: 2.750, mean reward: 0.074 [0.000, 2.750], mean action: 1.108 [0.000, 3.000], mean observation: 161.491 [0.000, 255.000], loss: 0.017644, mae: 1.685339, mean_q: 2.281988\n",
      " 16121/20000: episode: 500, duration: 21.465s, episode steps: 38, steps per second: 2, episode reward: 3.700, mean reward: 0.097 [0.000, 3.700], mean action: 0.605 [0.000, 3.000], mean observation: 162.493 [0.000, 255.000], loss: 0.020767, mae: 1.699684, mean_q: 2.292367\n",
      " 16158/20000: episode: 501, duration: 21.531s, episode steps: 37, steps per second: 2, episode reward: 2.700, mean reward: 0.073 [0.000, 2.700], mean action: 1.595 [0.000, 3.000], mean observation: 162.748 [0.000, 255.000], loss: 0.020333, mae: 1.662917, mean_q: 2.240646\n",
      " 16195/20000: episode: 502, duration: 21.436s, episode steps: 37, steps per second: 2, episode reward: 2.550, mean reward: 0.069 [0.000, 2.550], mean action: 0.784 [0.000, 3.000], mean observation: 162.488 [0.000, 255.000], loss: 0.074189, mae: 1.678793, mean_q: 2.265846\n",
      " 16229/20000: episode: 503, duration: 21.523s, episode steps: 34, steps per second: 2, episode reward: 2.750, mean reward: 0.081 [0.000, 2.750], mean action: 1.735 [0.000, 3.000], mean observation: 160.870 [0.000, 255.000], loss: 0.019463, mae: 1.670501, mean_q: 2.254990\n",
      " 16265/20000: episode: 504, duration: 21.041s, episode steps: 36, steps per second: 2, episode reward: 2.450, mean reward: 0.068 [0.000, 2.450], mean action: 1.167 [0.000, 3.000], mean observation: 164.011 [0.000, 255.000], loss: 0.022863, mae: 1.684622, mean_q: 2.269548\n",
      " 16302/20000: episode: 505, duration: 21.300s, episode steps: 37, steps per second: 2, episode reward: 2.450, mean reward: 0.066 [0.000, 2.450], mean action: 2.378 [0.000, 3.000], mean observation: 161.623 [0.000, 255.000], loss: 0.015166, mae: 1.675809, mean_q: 2.257110\n",
      " 16340/20000: episode: 506, duration: 21.299s, episode steps: 38, steps per second: 2, episode reward: 2.450, mean reward: 0.064 [0.000, 2.450], mean action: 2.211 [0.000, 3.000], mean observation: 162.449 [0.000, 255.000], loss: 0.020579, mae: 1.676670, mean_q: 2.264735\n",
      " 16377/20000: episode: 507, duration: 21.448s, episode steps: 37, steps per second: 2, episode reward: 4.850, mean reward: 0.131 [0.000, 4.850], mean action: 1.108 [0.000, 3.000], mean observation: 164.125 [0.000, 255.000], loss: 0.126159, mae: 1.694995, mean_q: 2.273288\n",
      " 16414/20000: episode: 508, duration: 21.273s, episode steps: 37, steps per second: 2, episode reward: 1.550, mean reward: 0.042 [0.000, 1.550], mean action: 0.351 [0.000, 3.000], mean observation: 163.408 [0.000, 255.000], loss: 0.017305, mae: 1.673688, mean_q: 2.248742\n",
      " 16452/20000: episode: 509, duration: 21.530s, episode steps: 38, steps per second: 2, episode reward: 3.850, mean reward: 0.101 [0.000, 3.850], mean action: 1.105 [0.000, 3.000], mean observation: 161.254 [0.000, 255.000], loss: 0.015782, mae: 1.647993, mean_q: 2.221326\n",
      " 16489/20000: episode: 510, duration: 21.189s, episode steps: 37, steps per second: 2, episode reward: 1.250, mean reward: 0.034 [0.000, 1.250], mean action: 1.730 [0.000, 3.000], mean observation: 162.289 [0.000, 255.000], loss: 0.069843, mae: 1.669501, mean_q: 2.246780\n",
      " 16527/20000: episode: 511, duration: 21.139s, episode steps: 38, steps per second: 2, episode reward: 1.150, mean reward: 0.030 [0.000, 1.150], mean action: 2.263 [1.000, 3.000], mean observation: 163.046 [0.000, 255.000], loss: 0.168643, mae: 1.650895, mean_q: 2.238563\n",
      " 16565/20000: episode: 512, duration: 21.344s, episode steps: 38, steps per second: 2, episode reward: 2.650, mean reward: 0.070 [0.000, 2.650], mean action: 1.395 [0.000, 3.000], mean observation: 162.073 [0.000, 255.000], loss: 0.063463, mae: 1.686823, mean_q: 2.277732\n",
      " 16603/20000: episode: 513, duration: 21.144s, episode steps: 38, steps per second: 2, episode reward: 2.700, mean reward: 0.071 [0.000, 2.700], mean action: 1.289 [0.000, 2.000], mean observation: 159.250 [0.000, 255.000], loss: 0.014009, mae: 1.639637, mean_q: 2.220013\n",
      " 16641/20000: episode: 514, duration: 21.218s, episode steps: 38, steps per second: 2, episode reward: 5.000, mean reward: 0.132 [0.000, 5.000], mean action: 1.684 [0.000, 2.000], mean observation: 168.650 [0.000, 255.000], loss: 0.021598, mae: 1.655268, mean_q: 2.237234\n",
      " 16679/20000: episode: 515, duration: 21.375s, episode steps: 38, steps per second: 2, episode reward: 1.450, mean reward: 0.038 [0.000, 1.450], mean action: 0.605 [0.000, 3.000], mean observation: 163.759 [0.000, 255.000], loss: 0.062898, mae: 1.655738, mean_q: 2.228261\n",
      " 16717/20000: episode: 516, duration: 21.251s, episode steps: 38, steps per second: 2, episode reward: 2.650, mean reward: 0.070 [0.000, 2.650], mean action: 0.553 [0.000, 3.000], mean observation: 162.943 [0.000, 255.000], loss: 0.124485, mae: 1.633283, mean_q: 2.213357\n",
      " 16755/20000: episode: 517, duration: 21.285s, episode steps: 38, steps per second: 2, episode reward: 1.150, mean reward: 0.030 [0.000, 1.150], mean action: 2.789 [1.000, 3.000], mean observation: 163.561 [0.000, 255.000], loss: 0.020013, mae: 1.647316, mean_q: 2.208412\n",
      " 16793/20000: episode: 518, duration: 21.252s, episode steps: 38, steps per second: 2, episode reward: 2.650, mean reward: 0.070 [0.000, 2.650], mean action: 0.974 [0.000, 3.000], mean observation: 163.326 [0.000, 255.000], loss: 0.015864, mae: 1.671057, mean_q: 2.239877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16831/20000: episode: 519, duration: 21.288s, episode steps: 38, steps per second: 2, episode reward: 1.250, mean reward: 0.033 [0.000, 1.250], mean action: 2.553 [0.000, 3.000], mean observation: 162.872 [0.000, 255.000], loss: 0.065393, mae: 1.654434, mean_q: 2.220923\n",
      " 16869/20000: episode: 520, duration: 21.340s, episode steps: 38, steps per second: 2, episode reward: 3.250, mean reward: 0.086 [0.000, 3.250], mean action: 2.342 [0.000, 3.000], mean observation: 162.031 [0.000, 255.000], loss: 0.019289, mae: 1.654997, mean_q: 2.220751\n",
      " 16906/20000: episode: 521, duration: 21.092s, episode steps: 37, steps per second: 2, episode reward: 3.700, mean reward: 0.100 [0.000, 3.700], mean action: 0.757 [0.000, 3.000], mean observation: 162.644 [0.000, 255.000], loss: 0.068669, mae: 1.673127, mean_q: 2.246217\n",
      " 16944/20000: episode: 522, duration: 21.588s, episode steps: 38, steps per second: 2, episode reward: 1.350, mean reward: 0.036 [0.000, 1.350], mean action: 1.842 [0.000, 3.000], mean observation: 162.343 [0.000, 255.000], loss: 0.018492, mae: 1.638097, mean_q: 2.210253\n",
      " 16982/20000: episode: 523, duration: 21.518s, episode steps: 38, steps per second: 2, episode reward: 2.350, mean reward: 0.062 [0.000, 2.350], mean action: 1.579 [0.000, 3.000], mean observation: 161.530 [0.000, 255.000], loss: 0.063593, mae: 1.691195, mean_q: 2.268732\n",
      " 17020/20000: episode: 524, duration: 21.406s, episode steps: 38, steps per second: 2, episode reward: 1.350, mean reward: 0.036 [0.000, 1.350], mean action: 0.184 [0.000, 3.000], mean observation: 163.898 [0.000, 255.000], loss: 0.066373, mae: 1.664932, mean_q: 2.233494\n",
      " 17057/20000: episode: 525, duration: 21.028s, episode steps: 37, steps per second: 2, episode reward: 1.250, mean reward: 0.034 [0.000, 1.250], mean action: 0.486 [0.000, 3.000], mean observation: 164.329 [0.000, 255.000], loss: 0.063869, mae: 1.631953, mean_q: 2.196983\n",
      " 17096/20000: episode: 526, duration: 21.478s, episode steps: 39, steps per second: 2, episode reward: 1.450, mean reward: 0.037 [0.000, 1.450], mean action: 0.333 [0.000, 3.000], mean observation: 164.028 [0.000, 255.000], loss: 0.113955, mae: 1.651683, mean_q: 2.207251\n",
      " 17134/20000: episode: 527, duration: 21.034s, episode steps: 38, steps per second: 2, episode reward: 2.550, mean reward: 0.067 [0.000, 2.550], mean action: 1.211 [0.000, 3.000], mean observation: 162.643 [0.000, 255.000], loss: 0.014399, mae: 1.635682, mean_q: 2.214975\n",
      " 17172/20000: episode: 528, duration: 21.315s, episode steps: 38, steps per second: 2, episode reward: 2.350, mean reward: 0.062 [0.000, 2.350], mean action: 1.868 [0.000, 3.000], mean observation: 163.515 [0.000, 255.000], loss: 0.062745, mae: 1.647846, mean_q: 2.215578\n",
      " 17210/20000: episode: 529, duration: 21.392s, episode steps: 38, steps per second: 2, episode reward: 2.650, mean reward: 0.070 [0.000, 2.650], mean action: 0.395 [0.000, 3.000], mean observation: 162.819 [0.000, 255.000], loss: 0.013231, mae: 1.660927, mean_q: 2.250712\n",
      " 17248/20000: episode: 530, duration: 21.222s, episode steps: 38, steps per second: 2, episode reward: 2.600, mean reward: 0.068 [0.000, 2.600], mean action: 1.184 [0.000, 2.000], mean observation: 160.137 [0.000, 255.000], loss: 0.107145, mae: 1.638338, mean_q: 2.209658\n",
      " 17286/20000: episode: 531, duration: 21.258s, episode steps: 38, steps per second: 2, episode reward: 1.250, mean reward: 0.033 [0.000, 1.250], mean action: 0.921 [0.000, 3.000], mean observation: 160.292 [0.000, 255.000], loss: 0.073122, mae: 1.635757, mean_q: 2.214617\n",
      " 17324/20000: episode: 532, duration: 21.294s, episode steps: 38, steps per second: 2, episode reward: 4.100, mean reward: 0.108 [0.000, 4.100], mean action: 0.684 [0.000, 3.000], mean observation: 163.283 [0.000, 255.000], loss: 0.070519, mae: 1.670131, mean_q: 2.246778\n",
      " 17362/20000: episode: 533, duration: 21.371s, episode steps: 38, steps per second: 2, episode reward: 2.550, mean reward: 0.067 [0.000, 2.550], mean action: 1.895 [0.000, 3.000], mean observation: 162.721 [0.000, 255.000], loss: 0.070437, mae: 1.653610, mean_q: 2.209641\n",
      " 17400/20000: episode: 534, duration: 21.282s, episode steps: 38, steps per second: 2, episode reward: 1.450, mean reward: 0.038 [0.000, 1.450], mean action: 1.263 [0.000, 3.000], mean observation: 162.447 [0.000, 255.000], loss: 0.015250, mae: 1.652958, mean_q: 2.224449\n",
      " 17438/20000: episode: 535, duration: 21.340s, episode steps: 38, steps per second: 2, episode reward: 2.550, mean reward: 0.067 [0.000, 2.550], mean action: 1.605 [0.000, 3.000], mean observation: 162.266 [0.000, 255.000], loss: 0.014002, mae: 1.655795, mean_q: 2.232722\n",
      " 17476/20000: episode: 536, duration: 21.271s, episode steps: 38, steps per second: 2, episode reward: 2.550, mean reward: 0.067 [0.000, 2.550], mean action: 1.816 [0.000, 3.000], mean observation: 161.200 [0.000, 255.000], loss: 0.064083, mae: 1.652140, mean_q: 2.216794\n",
      " 17514/20000: episode: 537, duration: 21.246s, episode steps: 38, steps per second: 2, episode reward: 1.250, mean reward: 0.033 [0.000, 1.250], mean action: 1.974 [0.000, 3.000], mean observation: 164.018 [0.000, 255.000], loss: 0.116537, mae: 1.632179, mean_q: 2.205263\n",
      " 17552/20000: episode: 538, duration: 21.050s, episode steps: 38, steps per second: 2, episode reward: 5.100, mean reward: 0.134 [0.000, 5.100], mean action: 1.211 [0.000, 3.000], mean observation: 162.912 [0.000, 255.000], loss: 0.018300, mae: 1.648741, mean_q: 2.223296\n",
      " 17591/20000: episode: 539, duration: 21.519s, episode steps: 39, steps per second: 2, episode reward: 2.400, mean reward: 0.062 [0.000, 2.400], mean action: 0.615 [0.000, 3.000], mean observation: 164.147 [0.000, 255.000], loss: 0.072290, mae: 1.675328, mean_q: 2.253344\n",
      " 17629/20000: episode: 540, duration: 21.025s, episode steps: 38, steps per second: 2, episode reward: 3.700, mean reward: 0.097 [0.000, 3.700], mean action: 0.658 [0.000, 3.000], mean observation: 163.958 [0.000, 255.000], loss: 0.070452, mae: 1.629048, mean_q: 2.193016\n",
      " 17667/20000: episode: 541, duration: 21.181s, episode steps: 38, steps per second: 2, episode reward: 2.550, mean reward: 0.067 [0.000, 2.550], mean action: 1.711 [0.000, 3.000], mean observation: 161.330 [0.000, 255.000], loss: 0.118357, mae: 1.661731, mean_q: 2.231987\n",
      " 17705/20000: episode: 542, duration: 21.083s, episode steps: 38, steps per second: 2, episode reward: 2.750, mean reward: 0.072 [0.000, 2.750], mean action: 2.079 [0.000, 3.000], mean observation: 160.842 [0.000, 255.000], loss: 0.121175, mae: 1.691894, mean_q: 2.266919\n",
      " 17743/20000: episode: 543, duration: 21.198s, episode steps: 38, steps per second: 2, episode reward: 1.150, mean reward: 0.030 [0.000, 1.150], mean action: 2.553 [1.000, 3.000], mean observation: 163.046 [0.000, 255.000], loss: 0.064991, mae: 1.662068, mean_q: 2.235042\n",
      " 17781/20000: episode: 544, duration: 21.031s, episode steps: 38, steps per second: 2, episode reward: 4.000, mean reward: 0.105 [0.000, 4.000], mean action: 1.395 [0.000, 3.000], mean observation: 161.719 [0.000, 255.000], loss: 0.062669, mae: 1.694326, mean_q: 2.271489\n",
      " 17820/20000: episode: 545, duration: 21.463s, episode steps: 39, steps per second: 2, episode reward: 3.800, mean reward: 0.097 [0.000, 3.800], mean action: 1.872 [0.000, 3.000], mean observation: 163.760 [0.000, 255.000], loss: 0.019467, mae: 1.707289, mean_q: 2.293427\n",
      " 17859/20000: episode: 546, duration: 21.407s, episode steps: 39, steps per second: 2, episode reward: 4.950, mean reward: 0.127 [0.000, 4.950], mean action: 0.923 [0.000, 3.000], mean observation: 163.822 [0.000, 255.000], loss: 0.064098, mae: 1.723760, mean_q: 2.316668\n",
      " 17898/20000: episode: 547, duration: 21.556s, episode steps: 39, steps per second: 2, episode reward: 3.050, mean reward: 0.078 [0.000, 3.050], mean action: 1.026 [0.000, 3.000], mean observation: 161.986 [0.000, 255.000], loss: 0.022156, mae: 1.756895, mean_q: 2.376075\n",
      " 17936/20000: episode: 548, duration: 21.185s, episode steps: 38, steps per second: 2, episode reward: 2.450, mean reward: 0.064 [0.000, 2.450], mean action: 2.158 [0.000, 3.000], mean observation: 162.458 [0.000, 255.000], loss: 0.016408, mae: 1.720481, mean_q: 2.323735\n",
      " 17973/20000: episode: 549, duration: 21.497s, episode steps: 37, steps per second: 2, episode reward: 3.850, mean reward: 0.104 [0.000, 3.850], mean action: 0.919 [0.000, 3.000], mean observation: 163.282 [0.000, 255.000], loss: 0.020802, mae: 1.752137, mean_q: 2.360079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18011/20000: episode: 550, duration: 21.355s, episode steps: 38, steps per second: 2, episode reward: 2.800, mean reward: 0.074 [0.000, 2.800], mean action: 1.763 [0.000, 3.000], mean observation: 159.752 [0.000, 255.000], loss: 0.070645, mae: 1.735839, mean_q: 2.342985\n",
      " 18049/20000: episode: 551, duration: 21.405s, episode steps: 38, steps per second: 2, episode reward: 2.700, mean reward: 0.071 [0.000, 2.700], mean action: 0.868 [0.000, 2.000], mean observation: 160.115 [0.000, 255.000], loss: 0.131886, mae: 1.755275, mean_q: 2.375019\n",
      " 18086/20000: episode: 552, duration: 21.250s, episode steps: 37, steps per second: 2, episode reward: 2.650, mean reward: 0.072 [0.000, 2.650], mean action: 0.703 [0.000, 3.000], mean observation: 161.931 [0.000, 255.000], loss: 0.128153, mae: 1.734495, mean_q: 2.350236\n",
      " 18124/20000: episode: 553, duration: 21.315s, episode steps: 38, steps per second: 2, episode reward: 2.650, mean reward: 0.070 [0.000, 2.650], mean action: 1.684 [0.000, 3.000], mean observation: 161.105 [0.000, 255.000], loss: 0.021646, mae: 1.699631, mean_q: 2.282862\n",
      " 18162/20000: episode: 554, duration: 21.239s, episode steps: 38, steps per second: 2, episode reward: 2.450, mean reward: 0.064 [0.000, 2.450], mean action: 1.474 [0.000, 3.000], mean observation: 161.891 [0.000, 255.000], loss: 0.066037, mae: 1.707932, mean_q: 2.292446\n",
      " 18200/20000: episode: 555, duration: 21.018s, episode steps: 38, steps per second: 2, episode reward: 1.550, mean reward: 0.041 [0.000, 1.550], mean action: 0.605 [0.000, 3.000], mean observation: 163.752 [0.000, 255.000], loss: 0.019364, mae: 1.716775, mean_q: 2.322710\n",
      " 18238/20000: episode: 556, duration: 21.213s, episode steps: 38, steps per second: 2, episode reward: 4.950, mean reward: 0.130 [0.000, 4.950], mean action: 1.368 [0.000, 3.000], mean observation: 165.836 [0.000, 255.000], loss: 0.016762, mae: 1.733772, mean_q: 2.346707\n",
      " 18276/20000: episode: 557, duration: 21.352s, episode steps: 38, steps per second: 2, episode reward: 2.450, mean reward: 0.064 [0.000, 2.450], mean action: 1.079 [0.000, 3.000], mean observation: 160.870 [0.000, 255.000], loss: 0.017080, mae: 1.715861, mean_q: 2.312042\n",
      " 18314/20000: episode: 558, duration: 21.307s, episode steps: 38, steps per second: 2, episode reward: 2.550, mean reward: 0.067 [0.000, 2.550], mean action: 1.947 [0.000, 3.000], mean observation: 161.837 [0.000, 255.000], loss: 0.014714, mae: 1.716967, mean_q: 2.318875\n",
      " 18352/20000: episode: 559, duration: 21.173s, episode steps: 38, steps per second: 2, episode reward: 2.450, mean reward: 0.064 [0.000, 2.450], mean action: 1.947 [0.000, 3.000], mean observation: 162.329 [0.000, 255.000], loss: 0.121052, mae: 1.722796, mean_q: 2.321885\n",
      " 18389/20000: episode: 560, duration: 21.377s, episode steps: 37, steps per second: 2, episode reward: 4.850, mean reward: 0.131 [0.000, 4.850], mean action: 0.676 [0.000, 2.000], mean observation: 164.341 [0.000, 255.000], loss: 0.016136, mae: 1.710091, mean_q: 2.308129\n",
      " 18427/20000: episode: 561, duration: 21.097s, episode steps: 38, steps per second: 2, episode reward: 4.000, mean reward: 0.105 [0.000, 4.000], mean action: 0.868 [0.000, 3.000], mean observation: 160.762 [0.000, 255.000], loss: 0.022090, mae: 1.750751, mean_q: 2.354639\n",
      " 18465/20000: episode: 562, duration: 21.526s, episode steps: 38, steps per second: 2, episode reward: 2.700, mean reward: 0.071 [0.000, 2.700], mean action: 1.184 [0.000, 2.000], mean observation: 161.419 [0.000, 255.000], loss: 0.017810, mae: 1.735508, mean_q: 2.331950\n",
      " 18504/20000: episode: 563, duration: 21.298s, episode steps: 39, steps per second: 2, episode reward: 3.600, mean reward: 0.092 [0.000, 3.600], mean action: 0.667 [0.000, 3.000], mean observation: 161.824 [0.000, 255.000], loss: 0.017383, mae: 1.702930, mean_q: 2.303668\n",
      " 18542/20000: episode: 564, duration: 21.072s, episode steps: 38, steps per second: 2, episode reward: 1.350, mean reward: 0.036 [0.000, 1.350], mean action: 0.789 [0.000, 3.000], mean observation: 163.765 [0.000, 255.000], loss: 0.019898, mae: 1.720695, mean_q: 2.332620\n",
      " 18580/20000: episode: 565, duration: 21.108s, episode steps: 38, steps per second: 2, episode reward: 3.800, mean reward: 0.100 [0.000, 3.800], mean action: 1.132 [0.000, 3.000], mean observation: 163.282 [0.000, 255.000], loss: 0.011245, mae: 1.693366, mean_q: 2.301102\n",
      " 18618/20000: episode: 566, duration: 21.310s, episode steps: 38, steps per second: 2, episode reward: 2.550, mean reward: 0.067 [0.000, 2.550], mean action: 1.737 [0.000, 3.000], mean observation: 161.381 [0.000, 255.000], loss: 0.019228, mae: 1.727831, mean_q: 2.331908\n",
      " 18656/20000: episode: 567, duration: 21.088s, episode steps: 38, steps per second: 2, episode reward: 2.550, mean reward: 0.067 [0.000, 2.550], mean action: 0.737 [0.000, 2.000], mean observation: 162.379 [0.000, 255.000], loss: 0.016748, mae: 1.708112, mean_q: 2.299703\n",
      " 18694/20000: episode: 568, duration: 21.237s, episode steps: 38, steps per second: 2, episode reward: 1.650, mean reward: 0.043 [0.000, 1.650], mean action: 1.605 [0.000, 3.000], mean observation: 162.578 [0.000, 255.000], loss: 0.017455, mae: 1.719821, mean_q: 2.324235\n",
      " 18732/20000: episode: 569, duration: 21.285s, episode steps: 38, steps per second: 2, episode reward: 3.750, mean reward: 0.099 [0.000, 3.750], mean action: 1.079 [0.000, 3.000], mean observation: 161.032 [0.000, 255.000], loss: 0.122063, mae: 1.726801, mean_q: 2.339437\n",
      " 18770/20000: episode: 570, duration: 21.285s, episode steps: 38, steps per second: 2, episode reward: 2.500, mean reward: 0.066 [0.000, 2.500], mean action: 0.105 [0.000, 2.000], mean observation: 159.634 [0.000, 255.000], loss: 0.013307, mae: 1.707897, mean_q: 2.315646\n",
      " 18808/20000: episode: 571, duration: 21.263s, episode steps: 38, steps per second: 2, episode reward: 2.650, mean reward: 0.070 [0.000, 2.650], mean action: 1.316 [0.000, 3.000], mean observation: 161.933 [0.000, 255.000], loss: 0.075745, mae: 1.682924, mean_q: 2.253583\n",
      " 18846/20000: episode: 572, duration: 21.128s, episode steps: 38, steps per second: 2, episode reward: 2.900, mean reward: 0.076 [0.000, 2.900], mean action: 0.526 [0.000, 2.000], mean observation: 160.956 [0.000, 255.000], loss: 0.016670, mae: 1.749475, mean_q: 2.348402\n",
      " 18884/20000: episode: 573, duration: 21.045s, episode steps: 38, steps per second: 2, episode reward: 4.000, mean reward: 0.105 [0.000, 4.000], mean action: 0.947 [0.000, 3.000], mean observation: 163.103 [0.000, 255.000], loss: 0.016449, mae: 1.745378, mean_q: 2.349550\n",
      " 18922/20000: episode: 574, duration: 21.251s, episode steps: 38, steps per second: 2, episode reward: 1.250, mean reward: 0.033 [0.000, 1.250], mean action: 2.342 [0.000, 3.000], mean observation: 161.666 [0.000, 255.000], loss: 0.016197, mae: 1.723637, mean_q: 2.317632\n",
      " 18961/20000: episode: 575, duration: 21.406s, episode steps: 39, steps per second: 2, episode reward: 2.600, mean reward: 0.067 [0.000, 2.600], mean action: 1.051 [0.000, 3.000], mean observation: 160.167 [0.000, 255.000], loss: 0.014310, mae: 1.712290, mean_q: 2.302559\n",
      " 18999/20000: episode: 576, duration: 21.019s, episode steps: 38, steps per second: 2, episode reward: 2.950, mean reward: 0.078 [0.000, 2.950], mean action: 1.184 [0.000, 3.000], mean observation: 161.078 [0.000, 255.000], loss: 0.063981, mae: 1.728972, mean_q: 2.323278\n",
      " 19037/20000: episode: 577, duration: 21.185s, episode steps: 38, steps per second: 2, episode reward: 2.750, mean reward: 0.072 [0.000, 2.750], mean action: 1.763 [0.000, 3.000], mean observation: 160.680 [0.000, 255.000], loss: 0.015956, mae: 1.735712, mean_q: 2.337891\n",
      " 19075/20000: episode: 578, duration: 21.025s, episode steps: 38, steps per second: 2, episode reward: 3.050, mean reward: 0.080 [0.000, 3.050], mean action: 2.079 [0.000, 3.000], mean observation: 160.894 [0.000, 255.000], loss: 0.015348, mae: 1.740637, mean_q: 2.348510\n",
      " 19114/20000: episode: 579, duration: 21.499s, episode steps: 39, steps per second: 2, episode reward: 4.000, mean reward: 0.103 [0.000, 4.000], mean action: 1.538 [0.000, 3.000], mean observation: 162.425 [0.000, 255.000], loss: 0.014544, mae: 1.727685, mean_q: 2.318808\n",
      " 19152/20000: episode: 580, duration: 21.280s, episode steps: 38, steps per second: 2, episode reward: 1.450, mean reward: 0.038 [0.000, 1.450], mean action: 1.895 [0.000, 3.000], mean observation: 163.389 [0.000, 255.000], loss: 0.014434, mae: 1.739962, mean_q: 2.340761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19189/20000: episode: 581, duration: 21.021s, episode steps: 37, steps per second: 2, episode reward: 1.550, mean reward: 0.042 [0.000, 1.550], mean action: 1.676 [0.000, 3.000], mean observation: 162.285 [0.000, 255.000], loss: 0.014914, mae: 1.726305, mean_q: 2.317613\n",
      " 19227/20000: episode: 582, duration: 21.010s, episode steps: 38, steps per second: 2, episode reward: 2.900, mean reward: 0.076 [0.000, 2.900], mean action: 1.579 [0.000, 3.000], mean observation: 161.733 [0.000, 255.000], loss: 0.016296, mae: 1.770333, mean_q: 2.373301\n",
      " 19265/20000: episode: 583, duration: 21.101s, episode steps: 38, steps per second: 2, episode reward: 2.700, mean reward: 0.071 [0.000, 2.700], mean action: 1.132 [0.000, 3.000], mean observation: 163.425 [0.000, 255.000], loss: 0.015525, mae: 1.742630, mean_q: 2.336395\n",
      " 19303/20000: episode: 584, duration: 21.219s, episode steps: 38, steps per second: 2, episode reward: 1.250, mean reward: 0.033 [0.000, 1.250], mean action: 2.158 [0.000, 3.000], mean observation: 160.456 [0.000, 255.000], loss: 0.074195, mae: 1.729411, mean_q: 2.324689\n",
      " 19341/20000: episode: 585, duration: 21.128s, episode steps: 38, steps per second: 2, episode reward: 2.450, mean reward: 0.064 [0.000, 2.450], mean action: 1.553 [0.000, 3.000], mean observation: 161.857 [0.000, 255.000], loss: 0.018876, mae: 1.734217, mean_q: 2.326375\n",
      " 19379/20000: episode: 586, duration: 21.011s, episode steps: 38, steps per second: 2, episode reward: 2.550, mean reward: 0.067 [0.000, 2.550], mean action: 1.316 [0.000, 3.000], mean observation: 162.536 [0.000, 255.000], loss: 0.018203, mae: 1.732736, mean_q: 2.320087\n",
      " 19417/20000: episode: 587, duration: 21.135s, episode steps: 38, steps per second: 2, episode reward: 1.150, mean reward: 0.030 [0.000, 1.150], mean action: 2.184 [0.000, 3.000], mean observation: 162.906 [0.000, 255.000], loss: 0.075056, mae: 1.735214, mean_q: 2.326690\n",
      " 19455/20000: episode: 588, duration: 21.107s, episode steps: 38, steps per second: 2, episode reward: 1.350, mean reward: 0.036 [0.000, 1.350], mean action: 1.184 [0.000, 3.000], mean observation: 163.470 [0.000, 255.000], loss: 0.018980, mae: 1.743686, mean_q: 2.336368\n",
      " 19493/20000: episode: 589, duration: 20.983s, episode steps: 38, steps per second: 2, episode reward: 1.500, mean reward: 0.039 [0.000, 1.500], mean action: 1.842 [0.000, 3.000], mean observation: 165.511 [0.000, 255.000], loss: 0.015971, mae: 1.730634, mean_q: 2.321914\n",
      " 19531/20000: episode: 590, duration: 21.174s, episode steps: 38, steps per second: 2, episode reward: 2.450, mean reward: 0.064 [0.000, 2.450], mean action: 1.184 [0.000, 3.000], mean observation: 162.669 [0.000, 255.000], loss: 0.121737, mae: 1.741960, mean_q: 2.341613\n",
      " 19569/20000: episode: 591, duration: 21.500s, episode steps: 38, steps per second: 2, episode reward: 2.550, mean reward: 0.067 [0.000, 2.550], mean action: 0.763 [0.000, 2.000], mean observation: 161.675 [0.000, 255.000], loss: 0.124194, mae: 1.735182, mean_q: 2.332472\n",
      " 19607/20000: episode: 592, duration: 21.255s, episode steps: 38, steps per second: 2, episode reward: 1.150, mean reward: 0.030 [0.000, 1.150], mean action: 2.500 [0.000, 3.000], mean observation: 164.590 [0.000, 255.000], loss: 0.120957, mae: 1.722129, mean_q: 2.307518\n",
      " 19645/20000: episode: 593, duration: 21.420s, episode steps: 38, steps per second: 2, episode reward: 2.500, mean reward: 0.066 [0.000, 2.500], mean action: 1.053 [0.000, 3.000], mean observation: 160.023 [0.000, 255.000], loss: 0.068572, mae: 1.714454, mean_q: 2.312850\n",
      " 19683/20000: episode: 594, duration: 21.309s, episode steps: 38, steps per second: 2, episode reward: 4.900, mean reward: 0.129 [0.000, 4.900], mean action: 1.289 [0.000, 3.000], mean observation: 164.346 [0.000, 255.000], loss: 0.119191, mae: 1.711488, mean_q: 2.295079\n",
      " 19721/20000: episode: 595, duration: 21.308s, episode steps: 38, steps per second: 2, episode reward: 2.650, mean reward: 0.070 [0.000, 2.650], mean action: 1.789 [0.000, 3.000], mean observation: 162.241 [0.000, 255.000], loss: 0.016309, mae: 1.723546, mean_q: 2.314941\n",
      " 19759/20000: episode: 596, duration: 21.257s, episode steps: 38, steps per second: 2, episode reward: 1.150, mean reward: 0.030 [0.000, 1.150], mean action: 2.474 [1.000, 3.000], mean observation: 162.189 [0.000, 255.000], loss: 0.017033, mae: 1.708071, mean_q: 2.291130\n",
      " 19797/20000: episode: 597, duration: 21.015s, episode steps: 38, steps per second: 2, episode reward: 1.150, mean reward: 0.030 [0.000, 1.150], mean action: 1.132 [0.000, 3.000], mean observation: 163.731 [0.000, 255.000], loss: 0.016003, mae: 1.716692, mean_q: 2.308791\n",
      " 19835/20000: episode: 598, duration: 21.097s, episode steps: 38, steps per second: 2, episode reward: 2.700, mean reward: 0.071 [0.000, 2.700], mean action: 1.211 [0.000, 2.000], mean observation: 160.767 [0.000, 255.000], loss: 0.012280, mae: 1.702500, mean_q: 2.293409\n",
      " 19873/20000: episode: 599, duration: 21.296s, episode steps: 38, steps per second: 2, episode reward: 2.600, mean reward: 0.068 [0.000, 2.600], mean action: 0.737 [0.000, 3.000], mean observation: 163.539 [0.000, 255.000], loss: 0.012837, mae: 1.725645, mean_q: 2.317330\n",
      " 19911/20000: episode: 600, duration: 21.501s, episode steps: 38, steps per second: 2, episode reward: 2.800, mean reward: 0.074 [0.000, 2.800], mean action: 1.658 [0.000, 3.000], mean observation: 161.111 [0.000, 255.000], loss: 0.067001, mae: 1.718543, mean_q: 2.304543\n",
      " 19949/20000: episode: 601, duration: 21.213s, episode steps: 38, steps per second: 2, episode reward: 2.700, mean reward: 0.071 [0.000, 2.700], mean action: 1.026 [0.000, 2.000], mean observation: 160.510 [0.000, 255.000], loss: 0.015576, mae: 1.712171, mean_q: 2.307535\n",
      " 19987/20000: episode: 602, duration: 21.144s, episode steps: 38, steps per second: 2, episode reward: 1.250, mean reward: 0.033 [0.000, 1.250], mean action: 1.500 [0.000, 3.000], mean observation: 162.308 [0.000, 255.000], loss: 0.071856, mae: 1.730181, mean_q: 2.330474\n",
      "done, took 12996.177 seconds\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 1.350, steps: 170\n",
      "Episode 2: reward: 1.350, steps: 171\n",
      "Episode 3: reward: 1.350, steps: 133\n",
      "Episode 4: reward: 1.350, steps: 168\n",
      "Episode 5: reward: 1.350, steps: 168\n",
      "Episode 6: reward: 1.350, steps: 173\n",
      "Episode 7: reward: 1.350, steps: 175\n",
      "Episode 8: reward: 1.350, steps: 175\n",
      "Episode 9: reward: 1.350, steps: 173\n",
      "Episode 10: reward: 1.350, steps: 173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27ea8c0abc8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "import cupcake_game\n",
    "\n",
    "# ゲームクラスをロード\n",
    "env = cupcake_game.Game(step=True, image=True)\n",
    "# プレイヤーの行動種類数（上下左右の移動）\n",
    "nb_actions = 4\n",
    "\n",
    "# CNNで使うパラメータ\n",
    "hidden_size = 128\n",
    "n_filters = 8\n",
    "kernel = (13, 13)\n",
    "strides = (3, 3)\n",
    "\n",
    "# CNNモデル\n",
    "model = Sequential()\n",
    "model.add(Reshape((env.observation_space.shape), input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Conv2D(n_filters, kernel, strides=strides, activation='relu', padding='same'))\n",
    "model.add(Conv2D(n_filters, kernel, strides=strides, activation='relu', padding='same'))\n",
    "model.add(Conv2D(n_filters, kernel, strides=strides, activation='relu', padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_size, kernel_initializer='he_normal', activation='relu',\n",
    "                kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(hidden_size, kernel_initializer='he_normal', activation='relu',\n",
    "                kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(hidden_size, kernel_initializer='he_normal', activation='relu',\n",
    "                kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "print(model.summary())\n",
    "\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "policy = EpsGreedyQPolicy(eps=0.1)\n",
    "\n",
    "# DQNモデル\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, gamma=0.99, memory=memory, nb_steps_warmup=100,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# パラメータの重みを保存しているファイル\n",
    "fname = \"cupcake_dqn_weights.bin\"\n",
    "try:\n",
    "    dqn.load_weights(fname)\n",
    "    print(\"Weights are loaded.\")\n",
    "except:\n",
    "    print(\"Weights are NOT loaded.\")\n",
    "\n",
    "# 学習実行\n",
    "history = dqn.fit(env, nb_steps=20000, verbose=2)\n",
    "\n",
    "dqn.save_weights(fname, overwrite=True)\n",
    "\n",
    "# テスト実行\n",
    "dqn.test(env, nb_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
